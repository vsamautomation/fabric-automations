{"cells":[{"cell_type":"markdown","source":["# Enhanced Fabric Workspace Scanner v02 - Refactored\n","\n","## Features:\n","- **Lakehouse Storage**: Saves all analysis results to dedicated lakehouse tables\n","- **Enhanced Context**: Additional context columns for Reports, Tables, Relationships, Dataflows\n","- **Column Usage Analysis**: Detailed column usage analysis with context from measures, relationships, and dependencies\n","- **üÜï Optimized Code**: Eliminated repetitive loops and function-based approach for better maintainability\n","\n","## Tables Created in Lakehouse:\n","- `workspace_analysis` - Workspace information\n","- `dataset_analysis` - Datasets with Reports, Tables, Relationships, Dataflows context\n","- `table_analysis` - Tables with usage context from measures, relationships, dependencies\n","- `column_usage_analysis` - Columns with detailed usage analysis\n","- `usage_summary` - Summary of dataset usage patterns\n","\n","## Code Improvements:\n","- Single dataset processing loop with comprehensive data collection\n","- Reusable functions for dataset analysis\n","- Cached data structures to avoid redundant API calls\n","- Clear separation of concerns between data collection and analysis\n"],"metadata":{},"id":"header-cell"},{"cell_type":"code","source":["# Install semantic-link-labs for extended Fabric analytics\n","!pip install semantic-link-labs"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"d935d4da-73dd-43a0-8a3c-3e645a0118cc","normalized_state":"finished","queued_time":"2025-10-16T09:08:24.2984488Z","session_start_time":"2025-10-16T09:08:24.2994475Z","execution_start_time":"2025-10-16T09:08:43.1140925Z","execution_finish_time":"2025-10-16T09:09:11.1961267Z","parent_msg_id":"f1525020-c985-47c7-b4b8-10977e3500e7"},"text/plain":"StatementMeta(, d935d4da-73dd-43a0-8a3c-3e645a0118cc, 3, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Collecting semantic-link-labs\n  Downloading semantic_link_labs-0.12.4-py3-none-any.whl.metadata (27 kB)\nCollecting semantic-link-sempy>=0.12.1 (from semantic-link-labs)\n  Downloading semantic_link_sempy-0.12.1-py3-none-any.whl.metadata (11 kB)\nCollecting anytree (from semantic-link-labs)\n  Downloading anytree-2.13.0-py3-none-any.whl.metadata (8.0 kB)\nCollecting polib (from semantic-link-labs)\n  Downloading polib-1.2.0-py2.py3-none-any.whl.metadata (15 kB)\nCollecting jsonpath_ng (from semantic-link-labs)\n  Downloading jsonpath_ng-1.7.0-py3-none-any.whl.metadata (18 kB)\nRequirement already satisfied: clr-loader>=0.2.5 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from semantic-link-sempy>=0.12.1->semantic-link-labs) (0.2.5)\nCollecting fabric-analytics-sdk==0.0.1 (from fabric-analytics-sdk[online-notebook]==0.0.1->semantic-link-sempy>=0.12.1->semantic-link-labs)\n  Downloading fabric_analytics_sdk-0.0.1-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: graphviz>=0.20.1 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from semantic-link-sempy>=0.12.1->semantic-link-labs) (0.20.1)\nCollecting azure-keyvault-secrets>=4.7.0 (from semantic-link-sempy>=0.12.1->semantic-link-labs)\n  Downloading azure_keyvault_secrets-4.10.0-py3-none-any.whl.metadata (18 kB)\nRequirement already satisfied: pyarrow>=12.0.1 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from semantic-link-sempy>=0.12.1->semantic-link-labs) (14.0.2)\nRequirement already satisfied: pythonnet>=3.0.1 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from semantic-link-sempy>=0.12.1->semantic-link-labs) (3.0.1)\nRequirement already satisfied: scikit-learn>=1.2.2 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from semantic-link-sempy>=0.12.1->semantic-link-labs) (1.2.2)\nRequirement already satisfied: setuptools>=68.2.2 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from semantic-link-sempy>=0.12.1->semantic-link-labs) (68.2.2)\nRequirement already satisfied: tqdm>=4.65.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from semantic-link-sempy>=0.12.1->semantic-link-labs) (4.65.0)\nRequirement already satisfied: rich>=13.3.5 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from semantic-link-sempy>=0.12.1->semantic-link-labs) (13.3.5)\nRequirement already satisfied: regex>=2023.8.8 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from semantic-link-sempy>=0.12.1->semantic-link-labs) (2023.10.3)\nRequirement already satisfied: pandas>=1.5.3 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from semantic-link-sempy>=0.12.1->semantic-link-labs) (2.1.4)\nRequirement already satisfied: pyspark>=3.4.1 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from semantic-link-sempy>=0.12.1->semantic-link-labs) (3.5.1.5.4.20240407)\nRequirement already satisfied: requests>=2.31.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from semantic-link-sempy>=0.12.1->semantic-link-labs) (2.31.0)\nRequirement already satisfied: aiohttp>=3.8.6 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from semantic-link-sempy>=0.12.1->semantic-link-labs) (3.9.3)\nRequirement already satisfied: IPython>=8.14.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from semantic-link-sempy>=0.12.1->semantic-link-labs) (8.20.0)\nRequirement already satisfied: tenacity>=8.2.3 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from semantic-link-sempy>=0.12.1->semantic-link-labs) (8.2.3)\nRequirement already satisfied: azure-core in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from fabric-analytics-sdk==0.0.1->fabric-analytics-sdk[online-notebook]==0.0.1->semantic-link-sempy>=0.12.1->semantic-link-labs) (1.30.2)\nRequirement already satisfied: azure-identity in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from fabric-analytics-sdk==0.0.1->fabric-analytics-sdk[online-notebook]==0.0.1->semantic-link-sempy>=0.12.1->semantic-link-labs) (1.15.0)\nCollecting fabric-analytics-notebook-plugin (from fabric-analytics-sdk[online-notebook]==0.0.1->semantic-link-sempy>=0.12.1->semantic-link-labs)\n  Downloading fabric_analytics_notebook_plugin-0.0.1-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: ply in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from jsonpath_ng->semantic-link-labs) (3.11)\nRequirement already satisfied: aiosignal>=1.1.2 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from aiohttp>=3.8.6->semantic-link-sempy>=0.12.1->semantic-link-labs) (1.2.0)\nRequirement already satisfied: attrs>=17.3.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from aiohttp>=3.8.6->semantic-link-sempy>=0.12.1->semantic-link-labs) (23.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from aiohttp>=3.8.6->semantic-link-sempy>=0.12.1->semantic-link-labs) (1.4.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from aiohttp>=3.8.6->semantic-link-sempy>=0.12.1->semantic-link-labs) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from aiohttp>=3.8.6->semantic-link-sempy>=0.12.1->semantic-link-labs) (1.9.3)\nRequirement already satisfied: isodate>=0.6.1 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from azure-keyvault-secrets>=4.7.0->semantic-link-sempy>=0.12.1->semantic-link-labs) (0.6.1)\nCollecting azure-core (from fabric-analytics-sdk==0.0.1->fabric-analytics-sdk[online-notebook]==0.0.1->semantic-link-sempy>=0.12.1->semantic-link-labs)\n  Downloading azure_core-1.36.0-py3-none-any.whl.metadata (47 kB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m47.1/47.1 kB\u001b[0m \u001b[31m761.1 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:--:--\u001b[0m\n\u001b[?25hRequirement already satisfied: typing-extensions>=4.6.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from azure-keyvault-secrets>=4.7.0->semantic-link-sempy>=0.12.1->semantic-link-labs) (4.9.0)\nRequirement already satisfied: cffi>=1.13 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from clr-loader>=0.2.5->semantic-link-sempy>=0.12.1->semantic-link-labs) (1.16.0)\nRequirement already satisfied: decorator in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from IPython>=8.14.0->semantic-link-sempy>=0.12.1->semantic-link-labs) (5.1.1)\nRequirement already satisfied: jedi>=0.16 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from IPython>=8.14.0->semantic-link-sempy>=0.12.1->semantic-link-labs) (0.18.1)\nRequirement already satisfied: matplotlib-inline in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from IPython>=8.14.0->semantic-link-sempy>=0.12.1->semantic-link-labs) (0.1.6)\nRequirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from IPython>=8.14.0->semantic-link-sempy>=0.12.1->semantic-link-labs) (3.0.43)\nRequirement already satisfied: pygments>=2.4.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from IPython>=8.14.0->semantic-link-sempy>=0.12.1->semantic-link-labs) (2.15.1)\nRequirement already satisfied: stack-data in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from IPython>=8.14.0->semantic-link-sempy>=0.12.1->semantic-link-labs) (0.2.0)\nRequirement already satisfied: traitlets>=5 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from IPython>=8.14.0->semantic-link-sempy>=0.12.1->semantic-link-labs) (5.7.1)\nRequirement already satisfied: pexpect>4.3 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from IPython>=8.14.0->semantic-link-sempy>=0.12.1->semantic-link-labs) (4.8.0)\nRequirement already satisfied: numpy<2,>=1.23.2 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from pandas>=1.5.3->semantic-link-sempy>=0.12.1->semantic-link-labs) (1.26.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from pandas>=1.5.3->semantic-link-sempy>=0.12.1->semantic-link-labs) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from pandas>=1.5.3->semantic-link-sempy>=0.12.1->semantic-link-labs) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from pandas>=1.5.3->semantic-link-sempy>=0.12.1->semantic-link-labs) (2023.3)\nRequirement already satisfied: py4j==0.10.9.7 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from pyspark>=3.4.1->semantic-link-sempy>=0.12.1->semantic-link-labs) (0.10.9.7)\nRequirement already satisfied: charset-normalizer<4,>=2 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from requests>=2.31.0->semantic-link-sempy>=0.12.1->semantic-link-labs) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from requests>=2.31.0->semantic-link-sempy>=0.12.1->semantic-link-labs) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from requests>=2.31.0->semantic-link-sempy>=0.12.1->semantic-link-labs) (2.1.0)\nRequirement already satisfied: certifi>=2017.4.17 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from requests>=2.31.0->semantic-link-sempy>=0.12.1->semantic-link-labs) (2024.2.2)\nRequirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from rich>=13.3.5->semantic-link-sempy>=0.12.1->semantic-link-labs) (2.2.0)\nRequirement already satisfied: scipy>=1.3.2 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from scikit-learn>=1.2.2->semantic-link-sempy>=0.12.1->semantic-link-labs) (1.11.4)\nRequirement already satisfied: joblib>=1.1.1 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from scikit-learn>=1.2.2->semantic-link-sempy>=0.12.1->semantic-link-labs) (1.2.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from scikit-learn>=1.2.2->semantic-link-sempy>=0.12.1->semantic-link-labs) (2.2.0)\nRequirement already satisfied: pycparser in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from cffi>=1.13->clr-loader>=0.2.5->semantic-link-sempy>=0.12.1->semantic-link-labs) (2.21)\nRequirement already satisfied: six in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from isodate>=0.6.1->azure-keyvault-secrets>=4.7.0->semantic-link-sempy>=0.12.1->semantic-link-labs) (1.16.0)\nRequirement already satisfied: parso<0.9.0,>=0.8.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from jedi>=0.16->IPython>=8.14.0->semantic-link-sempy>=0.12.1->semantic-link-labs) (0.8.3)\nRequirement already satisfied: mdurl~=0.1 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=13.3.5->semantic-link-sempy>=0.12.1->semantic-link-labs) (0.1.0)\nRequirement already satisfied: ptyprocess>=0.5 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from pexpect>4.3->IPython>=8.14.0->semantic-link-sempy>=0.12.1->semantic-link-labs) (0.7.0)\nRequirement already satisfied: wcwidth in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->IPython>=8.14.0->semantic-link-sempy>=0.12.1->semantic-link-labs) (0.2.5)\nRequirement already satisfied: cryptography>=2.5 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from azure-identity->fabric-analytics-sdk==0.0.1->fabric-analytics-sdk[online-notebook]==0.0.1->semantic-link-sempy>=0.12.1->semantic-link-labs) (42.0.2)\nRequirement already satisfied: msal<2.0.0,>=1.24.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from azure-identity->fabric-analytics-sdk==0.0.1->fabric-analytics-sdk[online-notebook]==0.0.1->semantic-link-sempy>=0.12.1->semantic-link-labs) (1.25.0)\nRequirement already satisfied: msal-extensions<2.0.0,>=0.3.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from azure-identity->fabric-analytics-sdk==0.0.1->fabric-analytics-sdk[online-notebook]==0.0.1->semantic-link-sempy>=0.12.1->semantic-link-labs) (1.0.0)\nRequirement already satisfied: psutil in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from fabric-analytics-notebook-plugin->fabric-analytics-sdk[online-notebook]==0.0.1->semantic-link-sempy>=0.12.1->semantic-link-labs) (5.9.0)\nRequirement already satisfied: executing in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from stack-data->IPython>=8.14.0->semantic-link-sempy>=0.12.1->semantic-link-labs) (0.8.3)\nRequirement already satisfied: asttokens in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from stack-data->IPython>=8.14.0->semantic-link-sempy>=0.12.1->semantic-link-labs) (2.0.5)\nRequirement already satisfied: pure-eval in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from stack-data->IPython>=8.14.0->semantic-link-sempy>=0.12.1->semantic-link-labs) (0.2.2)\nRequirement already satisfied: PyJWT<3,>=1.0.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from PyJWT[crypto]<3,>=1.0.0->msal<2.0.0,>=1.24.0->azure-identity->fabric-analytics-sdk==0.0.1->fabric-analytics-sdk[online-notebook]==0.0.1->semantic-link-sempy>=0.12.1->semantic-link-labs) (2.4.0)\nRequirement already satisfied: portalocker<3,>=1.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from msal-extensions<2.0.0,>=0.3.0->azure-identity->fabric-analytics-sdk==0.0.1->fabric-analytics-sdk[online-notebook]==0.0.1->semantic-link-sempy>=0.12.1->semantic-link-labs) (2.3.0)\nDownloading semantic_link_labs-0.12.4-py3-none-any.whl (775 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m775.8/775.8 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n\u001b[?25hDownloading semantic_link_sempy-0.12.1-py3-none-any.whl (3.3 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0mm\n\u001b[?25hDownloading fabric_analytics_sdk-0.0.1-py3-none-any.whl (34 kB)\nDownloading anytree-2.13.0-py3-none-any.whl (45 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m45.1/45.1 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading jsonpath_ng-1.7.0-py3-none-any.whl (30 kB)\nDownloading polib-1.2.0-py2.py3-none-any.whl (20 kB)\nDownloading azure_keyvault_secrets-4.10.0-py3-none-any.whl (125 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m125.2/125.2 kB\u001b[0m \u001b[31m40.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading azure_core-1.36.0-py3-none-any.whl (213 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m213.3/213.3 kB\u001b[0m \u001b[31m49.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fabric_analytics_notebook_plugin-0.0.1-py3-none-any.whl (20 kB)\nInstalling collected packages: polib, jsonpath_ng, anytree, azure-core, azure-keyvault-secrets, fabric-analytics-sdk, fabric-analytics-notebook-plugin, semantic-link-sempy, semantic-link-labs\n  Attempting uninstall: azure-core\n    Found existing installation: azure-core 1.30.2\n    Uninstalling azure-core-1.30.2:\n      Successfully uninstalled azure-core-1.30.2\n  Attempting uninstall: semantic-link-sempy\n    Found existing installation: semantic-link-sempy 0.11.0\n    Uninstalling semantic-link-sempy-0.11.0:\n      Successfully uninstalled semantic-link-sempy-0.11.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nfsspec-wrapper 0.1.15 requires PyJWT>=2.6.0, but you have pyjwt 2.4.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed anytree-2.13.0 azure-core-2024.9.1 azure-keyvault-secrets-4.10.0 fabric-analytics-notebook-plugin-0.0.1 fabric-analytics-sdk-0.0.1 jsonpath_ng-1.7.0 polib-1.2.0 semantic-link-labs-0.12.4 semantic-link-sempy-0.12.1\n"]}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"jupyter":{"outputs_hidden":true}},"id":"install-cell"},{"cell_type":"code","source":["import pandas as pd\n","import sempy_labs\n","import sempy.fabric as fabric\n","from sempy_labs.report import ReportWrapper\n","import re\n","import sempy\n","from pyspark.sql import SparkSession, functions as F\n","from pyspark.sql.types import ArrayType, StringType, StructType, LongType, StructField, FloatType\n","from pyspark.sql.functions import col\n","from datetime import datetime\n","import time\n","from dataclasses import dataclass\n","from typing import Dict, List, Optional\n","\n","# Initialize Spark session\n","spark = SparkSession.builder.getOrCreate()\n","\n","print(\"‚úÖ All imports successful and Spark session initialized\")"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":4,"statement_ids":[4],"state":"finished","livy_statement_state":"available","session_id":"d935d4da-73dd-43a0-8a3c-3e645a0118cc","normalized_state":"finished","queued_time":"2025-10-16T09:08:31.0987941Z","session_start_time":null,"execution_start_time":"2025-10-16T09:09:11.1982156Z","execution_finish_time":"2025-10-16T09:09:15.9437794Z","parent_msg_id":"aa28dede-72d9-4694-86f6-acfaf36f1c7f"},"text/plain":"StatementMeta(, d935d4da-73dd-43a0-8a3c-3e645a0118cc, 4, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["‚úÖ All imports successful and Spark session initialized\n"]}],"execution_count":2,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"imports-cell"},{"cell_type":"code","source":["# ============================================================\n","# UTILITY FUNCTIONS AND DATA STRUCTURES\n","# ============================================================\n","\n","@dataclass\n","class DatasetInfo:\n","    \"\"\"Data structure to hold comprehensive dataset information\"\"\"\n","    ds_id: str\n","    ds_name: str\n","    ws_id: str\n","    ws_name: str\n","    dependencies_df: Optional[pd.DataFrame] = None\n","    tables_df: Optional[pd.DataFrame] = None\n","    relationships_df: Optional[pd.DataFrame] = None\n","    measures_df: Optional[pd.DataFrame] = None\n","    columns_df: Optional[pd.DataFrame] = None\n","\n","def sanitize_df_columns(df, extra_columns=False, ws_id=None, ds_id=None, ws_name=None, ds_name=None):\n","    \"\"\"\n","    Replaces spaces in column names with underscore to prevent errors during Spark Dataframe Creation\n","    \"\"\"\n","    if df.empty:\n","        return df\n","        \n","    df.columns = [\n","        re.sub(r'\\W+', \"_\", col.strip().lower())\n","        for col in df.columns\n","    ]\n","\n","    if extra_columns:\n","        df['workspace_id'] = ws_id\n","        df['dataset_id'] = ds_id\n","        df['workspace_name'] = ws_name\n","        df['dataset_name'] = ds_name\n","        \n","    return df\n","\n","def save_to_lakehouse(df, table_name, description=\"\"):\n","    \"\"\"\n","    Save DataFrame to lakehouse using Spark\n","    \"\"\"\n","    try:\n","        if df.empty:\n","            print(f\"  ‚ö†Ô∏è Skipping empty DataFrame for table: {table_name}\")\n","            return\n","            \n","        # Add analysis timestamp\n","        df_with_timestamp = df.copy()\n","        df_with_timestamp['analysis_date'] = datetime.now()\n","        \n","        # Convert to Spark DataFrame and save\n","        spark_df = spark.createDataFrame(df_with_timestamp)\n","        spark_df.write.mode(\"overwrite\").saveAsTable(table_name)\n","        \n","        print(f\"  ‚úÖ Saved {len(df)} records to '{table_name}' table\")\n","        if description:\n","            print(f\"     üìù {description}\")\n","            \n","    except Exception as e:\n","        print(f\"  ‚ùå Error saving to {table_name}: {str(e)}\")\n","\n","def collect_dataset_info(ds_id: str, ds_name: str, ws_id: str, ws_name: str) -> DatasetInfo:\n","    \"\"\"\n","    üÜï Centralized function to collect all dataset-related information in one go\n","    \"\"\"\n","    print(f\"üîπ Processing dataset: {ds_name} (Workspace: {ws_name})\")\n","    \n","    dataset_info = DatasetInfo(ds_id, ds_name, ws_id, ws_name)\n","    \n","    try:\n","        # Get model dependencies\n","        deps = fabric.get_model_calc_dependencies(dataset=ds_id, workspace=ws_id)\n","        with deps as calc_deps:\n","            dependencies_df = getattr(calc_deps, \"dependencies_df\", None)\n","        \n","        if dependencies_df is not None:\n","            dependencies_df = sanitize_df_columns(\n","                df = dependencies_df, \n","                extra_columns= True,\n","                ws_id = ws_id, \n","                ds_id= ds_id,\n","                ws_name= ws_name,\n","                ds_name= ds_name\n","            )\n","            dataset_info.dependencies_df = dependencies_df\n","\n","        # Get tables\n","        tables = fabric.list_tables(dataset=ds_id, workspace=ws_id)\n","        if not tables.empty:\n","            tables = sanitize_df_columns(\n","                df = tables, \n","                extra_columns = True,\n","                ws_id = ws_id, \n","                ds_id = ds_id,\n","                ws_name = ws_name,\n","                ds_name= ds_name\n","            )\n","            dataset_info.tables_df = tables\n","            print(f\"  Found {len(tables)} tables\")\n","        \n","        # Get relationships\n","        relationships = fabric.list_relationships(dataset=ds_id, workspace=ws_id, extended=True)\n","        if not relationships.empty:\n","            relationships = sanitize_df_columns(df = relationships)\n","            relationships['qualified_from'] = \"'\" + relationships['from_table'] + \"'[\" + relationships['from_column'] + \"]\"\n","            relationships['qualified_to'] = \"'\" + relationships['to_table'] + \"'[\" + relationships['to_column'] + \"]\"\n","            dataset_info.relationships_df = relationships\n","            print(f\"  Found {len(relationships)} relationships\")\n","\n","        # Get measures\n","        measures = fabric.list_measures(dataset=ds_id, workspace=ws_id)\n","        if not measures.empty:\n","            dataset_info.measures_df = measures\n","            print(f\"  Found {len(measures)} measures\")\n","\n","        # Get columns\n","        columns = fabric.list_columns(dataset=ds_id, workspace=ws_id, extended=True)\n","        if not columns.empty:\n","            columns = sanitize_df_columns(\n","                df = columns,\n","                extra_columns= True,\n","                ws_id = ws_id, \n","                ds_id= ds_id,\n","                ws_name= ws_name,\n","                ds_name= ds_name\n","            )\n","            columns['qualified_name'] = \"'\" + columns['table_name'] + \"'[\" + columns['column_name'] + ']'\n","            dataset_info.columns_df = columns\n","            print(f\"  Found {len(columns)} columns\")\n","\n","    except Exception as e:\n","        print(f\"  ‚ö†Ô∏è Error processing {ds_name}: {e}\")\n","    \n","    return dataset_info\n","\n","def analyze_table_usage(dataset_info: DatasetInfo) -> List[Dict]:\n","    \"\"\"\n","    üÜï Analyze table usage for a single dataset using pre-collected data\n","    \"\"\"\n","    table_usage = []\n","    \n","    if dataset_info.tables_df is None or dataset_info.tables_df.empty:\n","        return table_usage\n","    \n","    # Determine used tables from all sources\n","    used_tables = set()\n","    \n","    if dataset_info.dependencies_df is not None:\n","        used_tables.update(set(dataset_info.dependencies_df['referenced_table'].dropna()))\n","    \n","    if dataset_info.relationships_df is not None:\n","        used_tables.update(set(dataset_info.relationships_df['from_table'].dropna()))\n","        used_tables.update(set(dataset_info.relationships_df['to_table'].dropna()))\n","    \n","    if dataset_info.measures_df is not None:\n","        used_tables.update(set(dataset_info.measures_df['table_name'].dropna()))\n","    \n","    used_tables = {t for t in used_tables if pd.notna(t)}\n","    \n","    # Analyze each table\n","    for table_name in set(dataset_info.tables_df['name'].dropna()):\n","        measures_count = 0\n","        if dataset_info.measures_df is not None:\n","            measures_count = len(dataset_info.measures_df[dataset_info.measures_df['table_name'] == table_name])\n","        \n","        rel_count = 0\n","        if dataset_info.relationships_df is not None:\n","            rel_count = len(dataset_info.relationships_df[\n","                (dataset_info.relationships_df['from_table'] == table_name) | \n","                (dataset_info.relationships_df['to_table'] == table_name)\n","            ])\n","        \n","        dep_count = 0\n","        if dataset_info.dependencies_df is not None:\n","            dep_count = len(dataset_info.dependencies_df[dataset_info.dependencies_df['referenced_table'] == table_name])\n","        \n","        status = \"Unused\" if table_name not in used_tables else \"Used\"\n","        \n","        table_usage.append({\n","            'workspace': dataset_info.ws_name,\n","            'dataset': dataset_info.ds_name,\n","            'table': table_name,\n","            'measures': measures_count,\n","            'relationships': rel_count,\n","            'dependencies': dep_count,\n","            'usage': status,\n","            'workspace_id': dataset_info.ws_id,\n","            'dataset_id': dataset_info.ds_id\n","        })\n","    \n","    return table_usage\n","\n","def analyze_column_usage(dataset_info: DatasetInfo) -> List[Dict]:\n","    \"\"\"\n","    üÜï Analyze column usage for a single dataset using pre-collected data\n","    \"\"\"\n","    columns_usage = []\n","    \n","    if dataset_info.columns_df is None or dataset_info.columns_df.empty:\n","        return columns_usage\n","    \n","    # Prepare dependency analysis\n","    dep_columns_df = pd.DataFrame()\n","    if (dataset_info.dependencies_df is not None and \n","        not dataset_info.dependencies_df.empty and \n","        'Referenced Object Type' in dataset_info.dependencies_df.columns):\n","        dep_columns_df = dataset_info.dependencies_df[\n","            dataset_info.dependencies_df['Referenced Object Type'].isin(['Column', 'Calc Column'])\n","        ]\n","    \n","    # Extract subsets by object type\n","    measures_refs_df = pd.DataFrame()\n","    relationship_refs_df = pd.DataFrame()\n","    \n","    if not dep_columns_df.empty and 'Object Type' in dep_columns_df.columns:\n","        measures_refs_df = dep_columns_df[dep_columns_df['Object Type'] == 'Measure']\n","        relationship_refs_df = dep_columns_df[\n","            dep_columns_df['Object Type'].str.contains('Relationship', case=False, na=False)\n","        ]\n","    \n","    # Determine used columns\n","    dep_columns = set()\n","    if not dep_columns_df.empty and 'Referenced Full Object Name' in dep_columns_df.columns:\n","        dep_columns = set(dep_columns_df['Referenced Full Object Name'])\n","    rel_columns = set()\n","    \n","    if dataset_info.relationships_df is not None:\n","        rel_columns = set(dataset_info.relationships_df['qualified_from']).union(\n","            set(dataset_info.relationships_df['qualified_to'])\n","        )\n","    \n","    used_columns = dep_columns.union(rel_columns)\n","    used_columns = {c for c in used_columns if pd.notna(c)}\n","    \n","    # Analyze each column\n","    for _, row in dataset_info.columns_df.iterrows():\n","        table_name = row['table_name']\n","        column_name = row['column_name']\n","        qualified_name = row['qualified_name']\n","        \n","        if pd.isna(column_name):\n","            continue\n","        \n","        dep_count = 0\n","        if not dep_columns_df.empty and 'Referenced Full Object Name' in dep_columns_df.columns:\n","            dep_count = len(dep_columns_df[dep_columns_df['Referenced Full Object Name'] == qualified_name])\n","        \n","        # Safe column access with proper empty DataFrame handling\n","        measure_c = 0\n","        if not measures_refs_df.empty and 'Referenced Full Object Name' in measures_refs_df.columns:\n","            measure_c = len(measures_refs_df[measures_refs_df['Referenced Full Object Name'] == qualified_name])\n","        \n","        relationship_c = 0\n","        if not relationship_refs_df.empty and 'Referenced Full Object Name' in relationship_refs_df.columns:\n","            relationship_c = len(relationship_refs_df[relationship_refs_df['Referenced Full Object Name'] == qualified_name])\n","        \n","        # Build referenced-by list\n","        referenced_by = \"\"\n","        if not dep_columns_df.empty and all(col in dep_columns_df.columns for col in ['Referenced Full Object Name', 'Object Name']):\n","            referenced_by = \", \".join(\n","                dep_columns_df.loc[\n","                    dep_columns_df['Referenced Full Object Name'] == qualified_name, 'Object Name'\n","                ].unique().tolist()\n","            )\n","        \n","        usage_status = 'Used' if any([measure_c, relationship_c, dep_count]) else 'Unused'\n","        \n","        columns_usage.append({\n","            'workspace': dataset_info.ws_name,\n","            'dataset': dataset_info.ds_name,\n","            'table': table_name,\n","            'column': column_name,\n","            'measures': measure_c,\n","            'relationships': relationship_c,\n","            'dependencies': dep_count,\n","            'referenced_by': referenced_by,\n","            'usage': usage_status,\n","            'workspace_id': dataset_info.ws_id,\n","            'dataset_id': dataset_info.ds_id\n","        })\n","    \n","    return columns_usage\n","\n","print(\"‚úÖ Utility functions and data structures defined\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"utils-cell"},{"cell_type":"code","source":["# ------------------------------------------------------------\n","# STEP 1: Object Discovery\n","# ------------------------------------------------------------\n","\n","print(\"üîç Discovering workspaces...\")\n","\n","workspaces_df = fabric.list_workspaces()\n","workspaces_df = sanitize_df_columns(workspaces_df)\n","workspaces_df = workspaces_df[['id', 'name', 'type']]\n","display(workspaces_df)\n","\n","datasets_all, reports_all, paginated_all, dataflows_all = [], [], [], []\n","\n","for _, ws in workspaces_df.iterrows():\n","    ws_id = ws['id']\n","    ws_name = ws['name']\n","    ws_type = ws['type']\n","    if ws_type == \"AdminInsights\":\n","        continue\n","    print(f\"\\nüì¶ Scanning workspace: {ws_name}\")\n","\n","   # --- Datasets\n","    try:\n","        ds = fabric.list_datasets(workspace=ws_id)\n","        if not ds.empty:\n","            ds['workspace_id'] = ws_id\n","            ds['workspace_name'] = ws_name\n","            datasets_all.append(ds)\n","    except Exception as e:\n","        print(f\"  ‚ö†Ô∏è Datasets error in {ws_name}: {e}\")\n","\n","    # --- Reports (includes both Power BI and Paginated)\n","    try:\n","        rep = fabric.list_reports(workspace=ws_id)\n","        if not rep.empty:\n","            rep['workspace_id'] = ws_id\n","            rep['workspace_name'] = ws_name\n","            reports_all.append(rep)\n","    except Exception as e:\n","        print(f\"  ‚ö†Ô∏è Reports error in {ws_name}: {e}\")\n","\n","    # --- Dataflows\n","    try:\n","        dfs = fabric.list_items(type='Dataflow',workspace=ws_id)\n","        if not dfs.empty:\n","            dfs['workspace_id'] = ws_id\n","            dfs['workspace_name'] = ws_name\n","            dataflows_all.append(dfs)\n","    except Exception as e:\n","        print(f\"  ‚ö†Ô∏è Dataflows error in {ws_name}: {e}\")\n","\n","# Combine results\n","datasets_df  = sanitize_df_columns(pd.concat(datasets_all, ignore_index=True) if datasets_all else pd.DataFrame())\n","reports_df   = sanitize_df_columns(pd.concat(reports_all, ignore_index=True) if reports_all else pd.DataFrame())\n","dataflows_df = sanitize_df_columns(pd.concat(dataflows_all, ignore_index=True) if dataflows_all else pd.DataFrame())\n","\n","# Split report types for clarity\n","if not reports_df.empty and \"report_type\" in reports_df.columns:\n","    pbi_reports_df = reports_df[reports_df[\"report_type\"] == \"PowerBIReport\"].copy()\n","    paginated_reports_df = reports_df[reports_df[\"report_type\"] == \"PaginatedReport\"].copy()\n","else:\n","    pbi_reports_df = reports_df\n","    paginated_reports_df = pd.DataFrame()\n","\n","print(\"\\n‚úÖ Object discovery complete.\")\n","print(f\"  Workspaces: {len(workspaces_df)}\")\n","print(f\"  Datasets:   {len(datasets_df)}\")\n","print(f\"  Reports:    {len(reports_df)}\")\n","print(f\"  Paginated:  {len(paginated_reports_df)}\")\n","print(f\"  Dataflows:  {len(dataflows_df)}\")\n","\n","# Save to Lakehouse - Workspaces\n","print(\"\\nüíæ Saving workspace data to lakehouse...\")\n","save_to_lakehouse(workspaces_df, \"workspace_analysis\", \"Basic workspace information\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"discovery-cell"},{"cell_type":"code","source":["# ------------------------------------------------------------\n","# STEP 2: üÜï CENTRALIZED DATASET PROCESSING\n","# This replaces multiple separate loops with a single comprehensive processing step\n","# ------------------------------------------------------------\n","\n","print(\"\\nüîÑ Centralized Dataset Processing (Collecting all data in one pass)...\")\n","print(\"This eliminates the need for multiple API calls per dataset!\\n\")\n","\n","# Collection containers for all analysis results\n","all_dataset_info = []\n","table_usage_results = []\n","column_usage_results = []\n","all_dependencies = []\n","\n","# Single loop through all datasets - collect everything at once\n","for _, ds in datasets_df.iterrows():\n","    ds_id = ds['dataset_id']\n","    ds_name = ds['dataset_name']\n","    ws_id = ds['workspace_id']\n","    ws_name = ds['workspace_name']\n","    \n","    # üÜï Single comprehensive data collection per dataset\n","    dataset_info = collect_dataset_info(ds_id, ds_name, ws_id, ws_name)\n","    all_dataset_info.append(dataset_info)\n","    \n","    # Collect dependencies for later aggregation\n","    if dataset_info.dependencies_df is not None:\n","        all_dependencies.append(dataset_info.dependencies_df)\n","    \n","    # üÜï Perform table analysis using collected data\n","    table_analysis = analyze_table_usage(dataset_info)\n","    table_usage_results.extend(table_analysis)\n","    \n","    # üÜï Perform column analysis using collected data\n","    column_analysis = analyze_column_usage(dataset_info)\n","    column_usage_results.extend(column_analysis)\n","\n","print(f\"\\n‚úÖ Centralized processing complete!\")\n","print(f\"  üìä Processed {len(all_dataset_info)} datasets\")\n","print(f\"  üìä Analyzed {len(table_usage_results)} tables\")\n","print(f\"  üìä Analyzed {len(column_usage_results)} columns\")\n","print(f\"  üìä Collected {len(all_dependencies)} dependency sets\")\n"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"centralized-processing-cell"},{"cell_type":"code","source":["# ------------------------------------------------------------\n","# STEP 3: Usage Analysis and Enhanced Dataset Context\n","# ------------------------------------------------------------\n","\n","print(\"\\nüîé Analyzing dataset usage and creating enhanced context...\")\n","\n","# 1Ô∏è‚É£ Dataset IDs used by any report (Power BI or Paginated)\n","used_dataset_ids = set()\n","if not reports_df.empty:\n","    used_dataset_ids.update(reports_df['dataset_id'].dropna().unique())\n","\n","# 2Ô∏è‚É£ Dataset IDs used by dataflows (as sources)\n","dataflow_refs = []\n","\n","for _, row in dataflows_df.iterrows():\n","    try:\n","        refs = sempy_labs.get_dataflow_references(row['id'], row['workspace_id'])\n","        if refs is not None and not refs.empty:\n","            refs['dataflow_id'] = row['id']\n","            refs['dataflow_name'] = row['name']\n","            refs['workspace_id'] = row['workspace_id']\n","            dataflow_refs.append(refs)\n","    except Exception:\n","        pass\n","\n","dataflow_refs_df = pd.concat(dataflow_refs, ignore_index=True) if dataflow_refs else pd.DataFrame()\n","\n","if not dataflow_refs_df.empty:\n","    if 'source_dataset_id' in dataflow_refs_df.columns:\n","        used_dataset_ids.update(dataflow_refs_df['source_dataset_id'].dropna().unique())\n","\n","# 3Ô∏è‚É£ Determine unused datasets\n","unused_datasets_df = datasets_df[~datasets_df['dataset_id'].isin(used_dataset_ids)].copy()\n","\n","print(f\"‚úÖ Found {len(unused_datasets_df)} potentially unused datasets.\")\n","\n","# Enhanced Dataset Analysis with Context\n","print(\"\\nüìä Creating enhanced dataset analysis with context...\")\n","\n","# Add context columns for each dataset using pre-collected data\n","enhanced_datasets = datasets_df.copy()\n","if not enhanced_datasets.empty:\n","    enhanced_datasets['report_count'] = 0\n","    enhanced_datasets['dataflow_count'] = 0\n","    enhanced_datasets['table_count'] = 0\n","    enhanced_datasets['relationship_count'] = 0\n","    enhanced_datasets['is_used'] = enhanced_datasets['dataset_id'].isin(used_dataset_ids)\n","    \n","    # Count reports per dataset\n","    if not reports_df.empty:\n","        report_counts = reports_df.groupby('dataset_id').size().to_dict()\n","        enhanced_datasets['report_count'] = enhanced_datasets['dataset_id'].map(report_counts).fillna(0)\n","    \n","    # Count dataflow references per dataset\n","    if not dataflow_refs_df.empty and 'source_dataset_id' in dataflow_refs_df.columns:\n","        dataflow_counts = dataflow_refs_df.groupby('source_dataset_id').size().to_dict()\n","        enhanced_datasets['dataflow_count'] = enhanced_datasets['dataset_id'].map(dataflow_counts).fillna(0)\n","    \n","    # Add table and relationship counts using pre-collected data\n","    for dataset_info in all_dataset_info:\n","        mask = enhanced_datasets['dataset_id'] == dataset_info.ds_id\n","        \n","        if dataset_info.tables_df is not None:\n","            enhanced_datasets.loc[mask, 'table_count'] = len(dataset_info.tables_df)\n","        \n","        if dataset_info.relationships_df is not None:\n","            enhanced_datasets.loc[mask, 'relationship_count'] = len(dataset_info.relationships_df)\n","\n","# Save Enhanced Dataset Analysis to Lakehouse\n","print(\"\\nüíæ Saving enhanced dataset analysis to lakehouse...\")\n","save_to_lakehouse(enhanced_datasets, \"dataset_analysis\", \n","                 \"Datasets with Reports, Tables, Relationships, Dataflows context\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"usage-analysis-cell"},{"cell_type":"code","source":["# ------------------------------------------------------------\n","# STEP 4: Usage Summary Table Creation\n","# ------------------------------------------------------------\n","\n","print(\"\\nüìã Creating usage summary table...\")\n","\n","summary_records = []\n","\n","for _, ds in datasets_df.iterrows():\n","    ds_id = ds['dataset_id']\n","    ds_name = ds['dataset_name']\n","    ws_name = ds['workspace_name']\n","\n","    # Reports using this dataset\n","    rep_refs = pbi_reports_df[pbi_reports_df['dataset_id'] == ds_id]\n","    paginated_refs = rep_refs[rep_refs['report_type'] == 'PaginatedReport'] if 'report_type' in rep_refs.columns else pd.DataFrame()\n","    normal_refs = rep_refs[rep_refs['report_type'] != 'PaginatedReport'] if 'report_type' in rep_refs.columns else rep_refs\n","\n","    # Dataflows referencing this dataset (if any)\n","    dataflow_refs = []\n","    if not dataflow_refs_df.empty and 'source_dataset_id' in dataflow_refs_df.columns:\n","        dataflow_refs = dataflow_refs_df[dataflow_refs_df['source_dataset_id'] == ds_id]\n","\n","    # Determine usage\n","    total_refs = len(rep_refs) + len(dataflow_refs)\n","    usage_status = \"Unused\" if total_refs == 0 else \"Used\"\n","\n","    # Add records for all associated reports\n","    if not rep_refs.empty:\n","        for _, r in rep_refs.iterrows():\n","            summary_records.append({\n","                \"Dataset_Workspace\": ws_name,\n","                \"Dataset_Name\": ds_name,\n","                \"Report_Name\": r['name'],\n","                \"Report_Type\": r.get('report_type', 'PowerBIReport'),\n","                \"Report_Workspace\": r['workspace_name'],\n","                \"Usage_Status\": usage_status,\n","                \"Total_References\": total_refs\n","            })\n","    # Add records for datasets with no references\n","    elif total_refs == 0:\n","        summary_records.append({\n","            \"Dataset_Workspace\": ws_name,\n","            \"Dataset_Name\": ds_name,\n","            \"Report_Name\": None,\n","            \"Report_Type\": None,\n","            \"Report_Workspace\": None,\n","            \"Usage_Status\": usage_status,\n","            \"Total_References\": total_refs\n","        })\n","\n","usage_summary_df = pd.DataFrame(summary_records)\n","display(usage_summary_df)\n","\n","# Save Usage Summary to Lakehouse\n","print(\"\\nüíæ Saving usage summary to lakehouse...\")\n","save_to_lakehouse(usage_summary_df, \"usage_summary\", \"Summary of dataset usage patterns\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"summary-cell"},{"cell_type":"code","source":["# ------------------------------------------------------------\n","# STEP 5: üÜï RESULTS PROCESSING & LAKEHOUSE SAVING\n","# Process and save the pre-collected analysis results\n","# ------------------------------------------------------------\n","\n","print(\"\\nüíæ Processing and saving analysis results to lakehouse...\")\n","\n","# Convert table analysis results to DataFrame\n","if table_usage_results:\n","    table_usage_df = pd.DataFrame(table_usage_results)\n","    display(table_usage_df)\n","    \n","    print(\"\\nüíæ Saving table analysis to lakehouse...\")\n","    save_to_lakehouse(table_usage_df, \"table_analysis\", \n","                     \"Tables with usage context from measures, relationships, and dependencies\")\n","else:\n","    print(\"‚ö†Ô∏è No table usage data to save\")\n","\n","# Convert column analysis results to DataFrame\n","if column_usage_results:\n","    columns_usage_df = pd.DataFrame(column_usage_results)\n","    display(columns_usage_df)\n","    \n","    print(\"\\nüíæ Saving column usage analysis to lakehouse...\")\n","    save_to_lakehouse(columns_usage_df, \"column_usage_analysis\", \n","                     \"Detailed column usage analysis with context from measures, relationships, and dependencies\")\n","else:\n","    print(\"‚ö†Ô∏è No column usage data to save\")\n","\n","print(\"\\n‚úÖ All analysis results saved to lakehouse!\")"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"collapsed":false},"id":"results-processing-cell"},{"cell_type":"code","source":["# ------------------------------------------------------------\n","# STEP 6: Final Summary and Performance Metrics\n","# ------------------------------------------------------------\n","\n","print(\"\\n\" + \"=\"*80)\n","print(\"üéâ ENHANCED FABRIC WORKSPACE ANALYSIS COMPLETE - REFACTORED VERSION\")\n","print(\"=\"*80)\n","\n","# Summary statistics\n","print(f\"üìä Discovery Summary:\")\n","print(f\"  Workspaces: {len(workspaces_df)}\")\n","print(f\"  Datasets:   {len(datasets_df)}\")\n","print(f\"  Reports:    {len(reports_df)}\")\n","print(f\"  Dataflows:  {len(dataflows_df)}\")\n","\n","if table_usage_results:\n","    used_tables = sum(1 for t in table_usage_results if t['usage'] == 'Used')\n","    unused_tables = sum(1 for t in table_usage_results if t['usage'] == 'Unused')\n","    print(f\"  Tables:     {len(table_usage_results)} (Used: {used_tables}, Unused: {unused_tables})\")\n","\n","if column_usage_results:\n","    used_columns = sum(1 for c in column_usage_results if c['usage'] == 'Used')\n","    unused_columns = sum(1 for c in column_usage_results if c['usage'] == 'Unused')\n","    print(f\"  Columns:    {len(column_usage_results)} (Used: {used_columns}, Unused: {unused_columns})\")\n","\n","print(f\"\\nüöÄ Performance Improvements:\")\n","print(f\"  ‚úÖ Single dataset processing loop (instead of 2 separate loops)\")\n","print(f\"  ‚úÖ Eliminated redundant API calls for dependencies/relationships/measures\")\n","print(f\"  ‚úÖ Function-based analysis for better code reuse and maintainability\")\n","print(f\"  ‚úÖ Centralized data collection with DatasetInfo data structure\")\n","\n","print(f\"\\nüíæ Lakehouse Tables Created:\")\n","print(f\"  üìä workspace_analysis - Basic workspace information\")\n","print(f\"  üìä dataset_analysis - Datasets with context (Reports, Tables, Relationships, Dataflows)\")\n","print(f\"  üìä table_analysis - Tables with usage context from measures, relationships, dependencies\")\n","print(f\"  üìä column_usage_analysis - Detailed column usage analysis\")\n","print(f\"  üìä usage_summary - Summary of dataset usage patterns\")\n","\n","# Display final unused datasets\n","if not unused_datasets_df.empty:\n","    print(\"\\n‚ö†Ô∏è UNUSED DATASETS\")\n","    for _, row in unused_datasets_df.iterrows():\n","        print(f\" - {row['workspace_name']} ‚Üí {row['dataset_name']}\")\n","else:\n","    print(\"\\nüéâ No unused datasets found!\")\n","\n","print(\"\\n\" + \"=\"*80)\n","print(\"‚úÖ Refactored analysis complete! Better performance and maintainability.\")\n","print(\"‚úÖ Check your lakehouse for detailed results.\")\n","print(\"=\"*80)"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"summary-final-cell"}],"metadata":{"kernel_info":{"name":"synapse_pyspark"},"kernelspec":{"name":"synapse_pyspark","display_name":"synapse_pyspark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"b688d372-651f-4ab4-8a7a-9c8b0587b8a1"}],"default_lakehouse":"b688d372-651f-4ab4-8a7a-9c8b0587b8a1","default_lakehouse_name":"Scan_Results","default_lakehouse_workspace_id":"32bcf9c2-9d7d-4857-9188-ef29efac3a3b"}}},"nbformat":4,"nbformat_minor":5}