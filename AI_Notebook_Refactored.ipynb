{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "%pip install -q -U semantic-link-labs google-genai anthropic typing_extensions pydantic\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sempy\n",
        "import sempy_labs\n",
        "import sempy.fabric as fabric\n",
        "from pyspark.sql import SparkSession, functions as F\n",
        "from pyspark.sql.functions import col\n",
        "from google import genai\n",
        "from google.genai import types\n",
        "import pandas as pd\n",
        "import anthropic\n",
        "from dataclasses import dataclass, asdict\n",
        "from typing import Dict, List, Optional\n",
        "from datetime import datetime\n",
        "import time"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.getOrCreate()\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Power BI to T-SQL data type mapping\n",
        "DATATYPE_MAPPING = {\n",
        "    # Integer types\n",
        "    'Int64': 'BIGINT',\n",
        "    'Integer': 'INT',\n",
        "    'Whole Number': 'INT',\n",
        "    \n",
        "    # Decimal types\n",
        "    'Decimal': 'DECIMAL(18, 2)',\n",
        "    'Fixed Decimal Number': 'DECIMAL(18, 2)',\n",
        "    'Double': 'FLOAT',\n",
        "    'Percentage': 'DECIMAL(5, 2)',\n",
        "    \n",
        "    # String types\n",
        "    'String': 'NVARCHAR(255)',\n",
        "    'Text': 'NVARCHAR(MAX)',\n",
        "    \n",
        "    # Date/Time types\n",
        "    'DateTime': 'DATETIME2',\n",
        "    'Date': 'DATE',\n",
        "    'Time': 'TIME',\n",
        "    \n",
        "    # Boolean\n",
        "    'Boolean': 'BIT',\n",
        "    'True/False': 'BIT',\n",
        "    \n",
        "    # Other\n",
        "    'Binary': 'VARBINARY(MAX)',\n",
        "    'Unknown': 'NVARCHAR(255)'\n",
        "}"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class ColumnSpec:\n",
        "    \"\"\"Column specification for T-SQL generation\"\"\"\n",
        "    column_name: str\n",
        "    data_type: str\n",
        "    tsql_data_type: str\n",
        "    is_nullable: bool = True\n",
        "    is_primary_key: bool = False\n",
        "    is_foreign_key: bool = False\n",
        "    referenced_table: Optional[str] = None\n",
        "    referenced_column: Optional[str] = None\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class TableSpec:\n",
        "    \"\"\"Table specification for T-SQL generation\"\"\"\n",
        "    table_name: str\n",
        "    columns: List[ColumnSpec]\n",
        "    relationships_from: List[Dict]\n",
        "    relationships_to: List[Dict]\n",
        "    usage_metrics: Dict\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class DatasetMigrationSpec:\n",
        "    \"\"\"Complete dataset migration specification\"\"\"\n",
        "    dataset_id: str\n",
        "    dataset_name: str\n",
        "    workspace_id: str\n",
        "    workspace_name: str\n",
        "    tables: List[TableSpec]\n",
        "    excluded_tables: List[str]\n",
        "    excluded_columns: int\n",
        "    excluded_measures: int\n",
        "    total_relationships: int"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TSQLMigrationPrep:\n",
        "    \"\"\"Prepares Power BI datasets for T-SQL migration\"\"\"\n",
        "    def __init__(self, lakehouse: Optional[str] = None, api_key: Optional[str] = None, agent_mode: Optional[str] = None):\n",
        "        \"\"\"\n",
        "        Initialize the prep tool\n",
        "\n",
        "        Args:\n",
        "            lakehouse: Lakehouse id/path\n",
        "            api_key: AI agent API key\n",
        "            agent_mode: AI provider ('claude' or 'gemini')\n",
        "        \"\"\"\n",
        "\n",
        "        self.lakehouse = lakehouse\n",
        "        self.api_key = api_key\n",
        "        self.agent_mode = agent_mode\n",
        "        self.client = None\n",
        "\n",
        "        if api_key and agent_mode:\n",
        "            if agent_mode == \"claude\":\n",
        "                self.client = anthropic.Anthropic(api_key=api_key)\n",
        "            elif agent_mode == \"gemini\":\n",
        "                self.client = genai.Client(api_key=api_key)\n",
        "            else:\n",
        "                raise ValueError(\"Please specify the AI provider you are using. The script supports 'claude' or 'gemini' \")\n",
        "\n",
        "\n",
        "        #Data Containers\n",
        "        self.column_usage_df = pd.DataFrame()\n",
        "        self.table_analysis_df = pd.DataFrame()\n",
        "        self.dataset_analysis_df = pd.DataFrame()\n",
        "        self.relationships_df = pd.DataFrame()\n",
        "        \n",
        "        print(\"âœ… T-SQL Migration Prep initialized\")\n",
        "\n",
        "            \n",
        "    def load_lakehouse_data(self, \n",
        "                           column_usage_df: pd.DataFrame,\n",
        "                           table_analysis_df: pd.DataFrame,\n",
        "                           dataset_analysis_df: Optional[pd.DataFrame] = None,\n",
        "                           relationships_df: Optional[pd.DataFrame] = None):\n",
        "        \"\"\"\n",
        "        Load analysis data from lakehouse tables\n",
        "        \n",
        "        Args:\n",
        "            column_usage_df: Column usage analysis with data types\n",
        "            table_analysis_df: Table usage analysis\n",
        "            dataset_analysis_df: Dataset-level context\n",
        "            relationships_df: Table relationships\n",
        "        \"\"\"\n",
        "        print(\"\\nğŸ“¥ Loading lakehouse analysis data...\")\n",
        "        \n",
        "        self.column_usage_df = column_usage_df\n",
        "        self.table_analysis_df = table_analysis_df\n",
        "        self.dataset_analysis_df = dataset_analysis_df if dataset_analysis_df is not None else pd.DataFrame()\n",
        "        self.relationships_df = relationships_df if relationships_df is not None else pd.DataFrame()\n",
        "        \n",
        "        print(f\"  âœ… Loaded {len(self.column_usage_df)} column records\")\n",
        "        print(f\"  âœ… Loaded {len(self.table_analysis_df)} table records\")\n",
        "        print(f\"  âœ… Loaded {len(self.dataset_analysis_df)} dataset records\")\n",
        "        print(f\"  âœ… Loaded {len(self.relationships_df)} relationship records\")\n",
        "\n",
        "    \n",
        "    def map_datatype_to_tsql(self, pbi_datatype: str) -> str:\n",
        "        \"\"\"\n",
        "        Map Power BI data type to T-SQL data type\n",
        "        \n",
        "        Args:\n",
        "            pbi_datatype: Power BI data type\n",
        "            \n",
        "        Returns:\n",
        "            T-SQL data type\n",
        "        \"\"\"\n",
        "        # Clean the datatype string\n",
        "        pbi_datatype = str(pbi_datatype).strip()\n",
        "        \n",
        "        # Direct mapping\n",
        "        if pbi_datatype in DATATYPE_MAPPING:\n",
        "            return DATATYPE_MAPPING[pbi_datatype]\n",
        "        \n",
        "        # Partial matching for complex types\n",
        "        pbi_lower = pbi_datatype.lower()\n",
        "        \n",
        "        if 'int' in pbi_lower or 'whole' in pbi_lower:\n",
        "            return 'INT'\n",
        "        elif 'decimal' in pbi_lower or 'number' in pbi_lower or 'currency' in pbi_lower:\n",
        "            return 'DECIMAL(18, 2)'\n",
        "        elif 'double' in pbi_lower or 'float' in pbi_lower:\n",
        "            return 'FLOAT'\n",
        "        elif 'text' in pbi_lower or 'string' in pbi_lower:\n",
        "            return 'NVARCHAR(255)'\n",
        "        elif 'date' in pbi_lower:\n",
        "            if 'time' in pbi_lower:\n",
        "                return 'DATETIME2'\n",
        "            return 'DATE'\n",
        "        elif 'bool' in pbi_lower:\n",
        "            return 'BIT'\n",
        "        else:\n",
        "            return 'NVARCHAR(255)'  # Default fallback\n",
        "\n",
        "    \n",
        "    def prepare_dataset_migration(self, dataset_id: str) -> DatasetMigrationSpec:\n",
        "        \"\"\"\n",
        "        Prepare specification for a single dataset\n",
        "        \n",
        "        Args:\n",
        "            dataset_id: Dataset ID to prepare\n",
        "            \n",
        "        Returns:\n",
        "            DatasetSpec with all details\n",
        "        \"\"\"\n",
        "        print(f\"\\nğŸ”„ Preparing specs for dataset: {dataset_id}\")\n",
        "\n",
        "                \n",
        "        # Get dataset info\n",
        "        if not self.dataset_analysis_df.empty:\n",
        "            dataset_row = self.dataset_analysis_df[self.dataset_analysis_df['dataset_id'] == dataset_id]\n",
        "            if dataset_row.empty:\n",
        "                raise ValueError(f\"Dataset {dataset_id} not found in dataset_analysis\")\n",
        "            dataset_info = dataset_row.iloc[0]\n",
        "        else:\n",
        "            # Try to get from table_analysis_df\n",
        "            dataset_tables = self.table_analysis_df[self.table_analysis_df['dataset_id'] == dataset_id]\n",
        "            if dataset_tables.empty:\n",
        "                raise ValueError(f\"Dataset {dataset_id} not found\")\n",
        "            dataset_info = {\n",
        "                'dataset_name': dataset_tables.iloc[0]['dataset_name'],\n",
        "                'workspace_id': dataset_tables.iloc[0]['workspace_id'],\n",
        "                'workspace_name': dataset_tables.iloc[0]['workspace_name']\n",
        "            }\n",
        "\n",
        "                \n",
        "        dataset_name = dataset_info.get('dataset_name', dataset_info.get('dataset', 'Unknown'))\n",
        "        workspace_id = dataset_info.get('workspace_id', '')\n",
        "        workspace_name = dataset_info.get('workspace_name', dataset_info.get('workspace', ''))\n",
        "\n",
        "                \n",
        "        # Filter to used tables only for this dataset\n",
        "        used_tables_df = self.table_analysis_df[\n",
        "            (self.table_analysis_df['dataset_id'] == dataset_id) &\n",
        "            (self.table_analysis_df['is_used'] == True)\n",
        "        ]\n",
        "        \n",
        "        # Filter to used columns for this dataset\n",
        "        used_columns_df = self.column_usage_df[\n",
        "            (self.column_usage_df['dataset_id'] == dataset_id) &\n",
        "            (self.column_usage_df['is_used'] == True)\n",
        "        ]\n",
        "        \n",
        "        # Get excluded counts\n",
        "        unused_tables = self.table_analysis_df[\n",
        "            (self.table_analysis_df['dataset_id'] == dataset_id) &\n",
        "            (self.table_analysis_df['is_used'] == False)\n",
        "        ]\n",
        "        \n",
        "        unused_columns = self.column_usage_df[\n",
        "            (self.column_usage_df['dataset_id'] == dataset_id) &\n",
        "            (self.column_usage_df['is_used'] == False)\n",
        "        ]\n",
        "        \n",
        "        excluded_tables = unused_tables['table_name'].unique().tolist()\n",
        "        excluded_columns_count = len(unused_columns)\n",
        "        \n",
        "        print(f\"  ğŸ“Š Found {len(used_tables_df)} used tables\")\n",
        "        print(f\"  ğŸ“Š Found {len(used_columns_df)} used columns\")\n",
        "        print(f\"  âš ï¸  Excluding {len(excluded_tables)} unused tables\")\n",
        "        print(f\"  âš ï¸  Excluding {excluded_columns_count} unused columns\")\n",
        "\n",
        "                \n",
        "        # Build table specifications\n",
        "        table_specs = []\n",
        "\n",
        "        for _, table_row in used_tables_df.iterrows():\n",
        "            table_name = table_row['table_name']\n",
        "\n",
        "            # Get columns for this table\n",
        "            table_columns = used_columns_df[used_columns_df['table_name'] == table_name]\n",
        "\n",
        "                        \n",
        "            # Build column specs\n",
        "            column_specs = []\n",
        "            for _, col_row in table_columns.iterrows():\n",
        "                column_name = col_row['object_name']\n",
        "                pbi_datatype = col_row.get('data_type', 'Unknown')\n",
        "                tsql_datatype = self.map_datatype_to_tsql(pbi_datatype)\n",
        "\n",
        "                                \n",
        "                # Determine if column is in a relationship\n",
        "                is_fk = False\n",
        "                referenced_table = None\n",
        "                referenced_column = None\n",
        "\n",
        "                                \n",
        "                if not self.relationships_df.empty:\n",
        "                    # Check if this column is a foreign key\n",
        "                    fk_rels = self.relationships_df[\n",
        "                        (self.relationships_df['from_table'] == table_name) &\n",
        "                        (self.relationships_df['from_column'] == column_name)\n",
        "                    ]\n",
        "                    \n",
        "                    if not fk_rels.empty:\n",
        "                        is_fk = True\n",
        "                        rel = fk_rels.iloc[0]\n",
        "                        referenced_table = rel.get('to_table', '')\n",
        "                        referenced_column = rel.get('to_column', '')\n",
        "\n",
        "                                    \n",
        "                column_spec = ColumnSpec(\n",
        "                    column_name=column_name,\n",
        "                    data_type=pbi_datatype,\n",
        "                    tsql_data_type=tsql_datatype,\n",
        "                    is_nullable=True,  # Default to nullable\n",
        "                    is_primary_key=False,  # Would need additional logic\n",
        "                    is_foreign_key=is_fk,\n",
        "                    referenced_table=referenced_table,\n",
        "                    referenced_column=referenced_column\n",
        "                )\n",
        "\n",
        "                column_specs.append(column_spec)\n",
        "\n",
        "                            \n",
        "            # Get relationships for this table\n",
        "            relationships_from = []\n",
        "            relationships_to = []\n",
        "            \n",
        "            if not self.relationships_df.empty:\n",
        "                # Relationships where this table is the FROM side\n",
        "                rels_from = self.relationships_df[\n",
        "                    self.relationships_df['from_table'] == table_name\n",
        "                ]\n",
        "                relationships_from = rels_from.to_dict('records')\n",
        "                \n",
        "                # Relationships where this table is the TO side\n",
        "                rels_to = self.relationships_df[\n",
        "                    self.relationships_df['to_table'] == table_name\n",
        "                ]\n",
        "                relationships_to = rels_to.to_dict('records')\n",
        "            \n",
        "            # Usage metrics\n",
        "            usage_metrics = {\n",
        "                'measures_count': int(table_row.get('table_measure_count', 0)),\n",
        "                'relationships_count': int(table_row.get('table_relationship_count', 0)),\n",
        "                'dependencies_count': int(table_row.get('dependencies', 0))\n",
        "            }\n",
        "            \n",
        "            table_spec = TableSpec(\n",
        "                table_name=table_name,\n",
        "                columns=column_specs,\n",
        "                relationships_from=relationships_from,\n",
        "                relationships_to=relationships_to,\n",
        "                usage_metrics=usage_metrics\n",
        "            )\n",
        "            \n",
        "            table_specs.append(table_spec)\n",
        "\n",
        "        # Total relationship count for dataset\n",
        "        total_relationships = len(self.relationships_df[\n",
        "            self.relationships_df['dataset_id'] == dataset_id\n",
        "        ]) if not self.relationships_df.empty else 0\n",
        "\n",
        "                \n",
        "        print(f\"  âœ… Migration spec prepared with {len(table_specs)} tables\")\n",
        "\n",
        "                \n",
        "        migration_spec = DatasetMigrationSpec(\n",
        "            dataset_id=dataset_id,\n",
        "            dataset_name=dataset_name,\n",
        "            workspace_id=workspace_id,\n",
        "            workspace_name=workspace_name,\n",
        "            tables=table_specs,\n",
        "            excluded_tables=excluded_tables,\n",
        "            excluded_columns=excluded_columns_count,\n",
        "            excluded_measures=0,  # Would need measure data\n",
        "            total_relationships=total_relationships\n",
        "        )\n",
        "\n",
        "        return migration_spec\n",
        "\n",
        "            \n",
        "    def export_migration_spec_to_json(self, migration_spec: DatasetMigrationSpec, output_path: str = ''):\n",
        "        \"\"\"\n",
        "        Export migration spec to JSON file for AI consumption\n",
        "        \n",
        "        Args:\n",
        "            migration_spec: Migration specification\n",
        "            output_path: Path to save JSON file\n",
        "        \"\"\"\n",
        "        \n",
        "        # Convert to dict\n",
        "        spec_dict = {\n",
        "            'dataset_metadata': {\n",
        "                'dataset_id': migration_spec.dataset_id,\n",
        "                'dataset_name': migration_spec.dataset_name,\n",
        "                'workspace_id': migration_spec.workspace_id,\n",
        "                'workspace_name': migration_spec.workspace_name\n",
        "            },\n",
        "            'tables': [\n",
        "                {\n",
        "                    'table_name': table.table_name,\n",
        "                    'columns': [\n",
        "                        {\n",
        "                            'column_name': col.column_name,\n",
        "                            'original_data_type': col.data_type,\n",
        "                            'tsql_data_type': col.tsql_data_type,\n",
        "                            'is_nullable': col.is_nullable,\n",
        "                            'is_primary_key': col.is_primary_key,\n",
        "                            'is_foreign_key': col.is_foreign_key,\n",
        "                            'referenced_table': col.referenced_table,\n",
        "                            'referenced_column': col.referenced_column\n",
        "                        }\n",
        "                        for col in table.columns\n",
        "                    ],\n",
        "                    'relationships_from': table.relationships_from,\n",
        "                    'relationships_to': table.relationships_to,\n",
        "                    'usage_metrics': table.usage_metrics\n",
        "                }\n",
        "                for table in migration_spec.tables\n",
        "            ],\n",
        "            'exclusions': {\n",
        "                'excluded_tables': migration_spec.excluded_tables,\n",
        "                'excluded_columns_count': migration_spec.excluded_columns,\n",
        "                'excluded_measures_count': migration_spec.excluded_measures\n",
        "            },\n",
        "            'metadata': {\n",
        "                'total_relationships': migration_spec.total_relationships,\n",
        "                'exported_at': datetime.now().isoformat(),\n",
        "                'purpose': 'T-SQL CREATE TABLE generation for dataset migration'\n",
        "            }\n",
        "        }\n",
        "\n",
        "        return spec_dict\n",
        "    \n",
        "    def generate_tsql_with_ai(self, migration_spec: DatasetMigrationSpec) -> str:\n",
        "        \"\"\"\n",
        "        Generate T-SQL CREATE TABLE scripts using AI\n",
        "        \n",
        "        Args:\n",
        "            migration_spec: Dataset migration specification\n",
        "            \n",
        "        Returns:\n",
        "            Generated T-SQL scripts\n",
        "        \"\"\"\n",
        "        if not self.client:\n",
        "            raise ValueError(\"AI client not initialized. Provide api_key during initialization.\")\n",
        "\n",
        "        \n",
        "        # Build structured prompt\n",
        "        prompt = self._build_tsql_generation_prompt(migration_spec)\n",
        "        \n",
        "        if self.agent_mode == \"claude\":\n",
        "            # Call Claude API\n",
        "            response = self.client.messages.create(\n",
        "                model=\"claude-haiku-4-5\",\n",
        "                max_tokens=8000,\n",
        "                messages=[\n",
        "                    {\n",
        "                        \"role\": \"user\",\n",
        "                        \"content\": prompt\n",
        "                    }\n",
        "                ]\n",
        "            )\n",
        "            \n",
        "            tsql_scripts = response.content[0].text\n",
        "\n",
        "        elif self.agent_mode == \"gemini\":\n",
        "            #Call Gemini API\n",
        "            response = self.client.models.generate_content(\n",
        "                        model=\"gemini-2.0-flash-exp\", \n",
        "                        contents=prompt,\n",
        "                        config=types.GenerateContentConfig(\n",
        "                            temperature=0.1\n",
        "                        )\n",
        "                    )\n",
        "            tsql_scripts = response.text\n",
        "\n",
        "        return tsql_scripts\n",
        "            \n",
        "    def _build_tsql_generation_prompt(self, migration_spec: DatasetMigrationSpec) -> str:\n",
        "        \"\"\"Build the prompt for AI\"\"\"\n",
        "        \n",
        "        # Format tables and columns\n",
        "        tables_section = []\n",
        "        for table in migration_spec.tables:\n",
        "            columns_info = []\n",
        "            for col in table.columns:\n",
        "                fk_info = f\" (FK -> {col.referenced_table}.{col.referenced_column})\" if col.is_foreign_key else \"\"\n",
        "                columns_info.append(f\"  - {col.column_name}: {col.tsql_data_type}{fk_info}\")\n",
        "            \n",
        "            tables_section.append(f\"\"\"\n",
        "Table: {table.table_name}\n",
        "Columns:\n",
        "{chr(10).join(columns_info)}\n",
        "Usage: {table.usage_metrics['measures_count']} measures, {table.usage_metrics['relationships_count']} relationships\n",
        "\"\"\")\n",
        "        \n",
        "        # Format relationships\n",
        "        relationships_section = []\n",
        "        for table in migration_spec.tables:\n",
        "            for rel in table.relationships_from:\n",
        "                relationships_section.append(\n",
        "                    f\"  - {rel.get('from_table', '')}.{rel.get('from_column', '')} -> \"\n",
        "                    f\"{rel.get('to_table', '')}.{rel.get('to_column', '')} \"\n",
        "                    f\"[{'Active' if rel.get('active', True) else 'Inactive'}]\"\n",
        "                )\n",
        "        \n",
        "        prompt = f\"\"\"You are an expert SQL developer specializing in dimensional modeling. Your task is to generate T-SQL CREATE TABLE scripts for Power BI dataset migration.\n",
        "\n",
        "Dataset: {migration_spec.dataset_name}\n",
        "Workspace: {migration_spec.workspace_name}\n",
        "\n",
        "IMPORTANT CONSTRAINTS:\n",
        "1. Use EXACT column names as provided (case-sensitive)\n",
        "2. Use the EXACT T-SQL data types specified for each column\n",
        "3. Only include tables and columns listed below (unused objects already filtered out)\n",
        "4. Implement FOREIGN KEY constraints based on relationships provided\n",
        "5. Add indexes on foreign key columns for performance\n",
        "6. Include helpful comments documenting the original Power BI context\n",
        "\n",
        "EXCLUSIONS (already filtered - DO NOT include):\n",
        "- {len(migration_spec.excluded_tables)} unused tables excluded\n",
        "- {migration_spec.excluded_columns} unused columns excluded\n",
        "\n",
        "TABLES TO CREATE:\n",
        "{chr(10).join(tables_section)}\n",
        "\n",
        "RELATIONSHIPS (for FOREIGN KEY constraints):\n",
        "{chr(10).join(relationships_section) if relationships_section else '  - No relationships defined'}\n",
        "\n",
        "OUTPUT REQUIREMENTS:\n",
        "1. Generate complete CREATE TABLE statements for each table\n",
        "2. Include PRIMARY KEY constraints \n",
        "3. Include FOREIGN KEY constraints based on relationships\n",
        "4. Add CREATE INDEX statements for foreign key columns\n",
        "5. Use proper T-SQL syntax compatible with SQL Server 2019+\n",
        "6. Add comments explaining the table purpose\n",
        "7. Use a consistent naming convention\n",
        "\n",
        "Generate the complete T-SQL migration script now:\"\"\"\n",
        "        \n",
        "        return prompt\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def main(agent_mode: str = 'gemini', \n",
        "         api_key: Optional[str] = None,\n",
        "         dataset_ids: Optional[List[str]] = None,\n",
        "         process_all_datasets: bool = True,\n",
        "         export_json: bool = True,\n",
        "         generate_tsql: bool = True,\n",
        "         save_to_lakehouse: bool = True,\n",
        "         lakehouse_table_name: str = \"tsql_migration_results\"):\n",
        "    \"\"\"\n",
        "    Main function to execute T-SQL migration workflow for multiple datasets\n",
        "    \n",
        "    Args:\n",
        "        agent_mode: AI provider - 'claude' or 'gemini' (default: 'gemini')\n",
        "        api_key: API key for the selected AI provider\n",
        "        dataset_ids: List of specific dataset IDs to process (optional)\n",
        "        process_all_datasets: Process all datasets in data_context (default: True)\n",
        "        export_json: Whether to export migration spec to JSON (default: True)\n",
        "        generate_tsql: Whether to generate T-SQL scripts with AI (default: True)\n",
        "        save_to_lakehouse: Save results to lakehouse table (default: True)\n",
        "        lakehouse_table_name: Name of lakehouse table to save results\n",
        "    \n",
        "    Returns:\n",
        "        Dictionary with all_results and summary statistics\n",
        "    \"\"\"\n",
        "    \n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"ğŸš€ Starting Batch T-SQL Migration Workflow\")\n",
        "    print(f\"   AI Provider: {agent_mode.upper()}\")\n",
        "    print(f\"   Generate T-SQL: {'Yes' if generate_tsql and api_key else 'No'}\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "    \n",
        "    # Step 1: Load lakehouse tables (single read - efficient)\n",
        "    print(\"ğŸ“Š Step 1: Loading lakehouse tables...\")\n",
        "    data_context_spark = spark.table(\"ai_dataset_context\")\n",
        "    relationships_spark = spark.table(\"dataset_relationships\")\n",
        "    objects_spark = spark.read.table(\"ai_object_features\")\n",
        "    \n",
        "    # Convert to pandas once for all datasets\n",
        "    data_context_pd = data_context_spark.toPandas()\n",
        "    relationships_pd = relationships_spark.toPandas()\n",
        "    \n",
        "    print(f\"  âœ… Loaded {len(data_context_pd)} datasets from context\")\n",
        "    \n",
        "    # Step 2: Prepare table analysis DataFrame (single operation)\n",
        "    print(\"\\nğŸ”§ Step 2: Preparing table analysis...\")\n",
        "    tables = objects_spark.groupBy([\n",
        "        'workspace_id', \n",
        "        'workspace_name',\n",
        "        'dataset_id', \n",
        "        'dataset_name', \n",
        "        'table_name'\n",
        "    ]).agg(\n",
        "        F.mean('usage_score').alias('usage_score'),\n",
        "        F.first('table_measure_count').alias('table_measure_count'),\n",
        "        F.first('table_column_count').alias('table_column_count'),\n",
        "        F.first('table_relationship_count').alias('table_relationship_count'),\n",
        "        F.first('table_is_isolated').alias('table_is_isolated'),\n",
        "        F.first('dataset_total_tables').alias('dataset_total_tables'),\n",
        "        F.first('dataset_relationship_health').alias('dataset_relationship_health'),\n",
        "        F.first('dataset_usage_efficiency').alias('dataset_usage_efficiency'),\n",
        "        F.sum('used_by_dependencies').alias('dependencies')\n",
        "    ).withColumn(\n",
        "        'is_used', \n",
        "        F.when(F.col('usage_score') > 0, True).otherwise(False)\n",
        "    ).withColumn(\n",
        "        'usage_score',\n",
        "        F.round(F.col('usage_score'), 3)\n",
        "    )\n",
        "    \n",
        "    tables_pd = tables.toPandas()\n",
        "    \n",
        "    # Step 3: Filter columns\n",
        "    print(\"\\nğŸ”§ Step 3: Filtering column data...\")\n",
        "    columns = objects_spark[objects_spark['object_type'] == 'column']\n",
        "    columns_pd = columns.toPandas()\n",
        "    \n",
        "    print(f\"  âœ… Prepared {len(tables_pd)} table records\")\n",
        "    print(f\"  âœ… Prepared {len(columns_pd)} column records\")\n",
        "    \n",
        "    # Step 4: Determine which datasets to process\n",
        "    if process_all_datasets:\n",
        "        datasets_to_process = data_context_pd['dataset_id'].unique().tolist()\n",
        "    elif dataset_ids:\n",
        "        datasets_to_process = dataset_ids\n",
        "    else:\n",
        "        raise ValueError(\"Either set process_all_datasets=True or provide dataset_ids list\")\n",
        "    \n",
        "    print(f\"\\nğŸ“‹ Total datasets to process: {len(datasets_to_process)}\")\n",
        "    \n",
        "    # Step 5: Initialize TSQLMigrationPrep (single instance for all datasets)\n",
        "    print(\"\\nâš™ï¸  Step 4: Initializing T-SQL Migration Prep...\")\n",
        "    prep = TSQLMigrationPrep(api_key=api_key, agent_mode=agent_mode)\n",
        "    \n",
        "    # Load data once for all datasets\n",
        "    prep.load_lakehouse_data(\n",
        "        column_usage_df=columns_pd,\n",
        "        table_analysis_df=tables_pd,\n",
        "        dataset_analysis_df=data_context_pd,\n",
        "        relationships_df=relationships_pd\n",
        "    )\n",
        "    \n",
        "    # Step 6: Process each dataset\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"ğŸ”„ Processing {len(datasets_to_process)} datasets...\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "    \n",
        "    all_results = []\n",
        "    successful_count = 0\n",
        "    failed_count = 0\n",
        "    \n",
        "    for idx, dataset_id in enumerate(datasets_to_process, 1):\n",
        "        try:\n",
        "            print(f\"\\n{'â”€'*80}\")\n",
        "            print(f\"ğŸ“¦ [{idx}/{len(datasets_to_process)}] Processing Dataset: {dataset_id}\")\n",
        "            print(f\"{'â”€'*80}\")\n",
        "            \n",
        "            # Prepare migration spec\n",
        "            dataset_meta = prep.prepare_dataset_migration(dataset_id)\n",
        "            \n",
        "            result = {\n",
        "                'dataset_id': dataset_id,\n",
        "                'dataset_name': dataset_meta.dataset_name,\n",
        "                'workspace_name': dataset_meta.workspace_name,\n",
        "                'migration_spec': dataset_meta,\n",
        "                'json_spec': None,\n",
        "                'tsql_scripts': None,\n",
        "                'status': 'success',\n",
        "                'error': None,\n",
        "                'timestamp': datetime.now().isoformat(),\n",
        "                'tables_count': len(dataset_meta.tables),\n",
        "                'columns_count': sum(len(t.columns) for t in dataset_meta.tables)\n",
        "            }\n",
        "            \n",
        "            # Export to JSON\n",
        "            if export_json:\n",
        "                json_meta = prep.export_migration_spec_to_json(dataset_meta)\n",
        "                result['json_spec'] = json_meta\n",
        "            \n",
        "            # Generate T-SQL scripts (only if API key provided)\n",
        "            if generate_tsql and api_key:\n",
        "                print(f\"\\nğŸ¤– Generating T-SQL scripts using {agent_mode.upper()} AI...\")\n",
        "                tsql_scripts = prep.generate_tsql_with_ai(dataset_meta)\n",
        "                result['tsql_scripts'] = tsql_scripts\n",
        "                print(f\"  âœ… T-SQL scripts generated successfully\")\n",
        "                \n",
        "                # Add small delay to respect API rate limits\n",
        "                if idx < len(datasets_to_process):  # Don't delay after last dataset\n",
        "                    time.sleep(1)\n",
        "            \n",
        "            all_results.append(result)\n",
        "            successful_count += 1\n",
        "            \n",
        "            print(f\"\\nâœ… Dataset {dataset_meta.dataset_name} processed successfully\")\n",
        "            \n",
        "        except Exception as e:\n",
        "            failed_count += 1\n",
        "            error_result = {\n",
        "                'dataset_id': dataset_id,\n",
        "                'dataset_name': 'Unknown',\n",
        "                'workspace_name': 'Unknown',\n",
        "                'migration_spec': None,\n",
        "                'json_spec': None,\n",
        "                'tsql_scripts': None,\n",
        "                'status': 'failed',\n",
        "                'error': str(e),\n",
        "                'timestamp': datetime.now().isoformat(),\n",
        "                'tables_count': 0,\n",
        "                'columns_count': 0\n",
        "            }\n",
        "            all_results.append(error_result)\n",
        "            print(f\"\\nâŒ Error processing dataset {dataset_id}: {e}\")\n",
        "    \n",
        "    # Step 7: Save results to lakehouse (optional)\n",
        "    if save_to_lakehouse and all_results:\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"ğŸ’¾ Saving results to lakehouse table: {lakehouse_table_name}\")\n",
        "        print(f\"{'='*80}\\n\")\n",
        "        \n",
        "        # Prepare data for lakehouse\n",
        "        lakehouse_data = []\n",
        "        for result in all_results:\n",
        "            lakehouse_data.append({\n",
        "                'dataset_id': result['dataset_id'],\n",
        "                'dataset_name': result['dataset_name'],\n",
        "                'workspace_name': result['workspace_name'],\n",
        "                'status': result['status'],\n",
        "                'tables_count': result['tables_count'],\n",
        "                'columns_count': result['columns_count'],\n",
        "                'has_tsql': result['tsql_scripts'] is not None,\n",
        "                'tsql_scripts': result['tsql_scripts'] if result['tsql_scripts'] else '',\n",
        "                'error_message': result['error'] if result['error'] else '',\n",
        "                'timestamp': result['timestamp']\n",
        "            })\n",
        "        \n",
        "        # Save to lakehouse\n",
        "        lakehouse_df = pd.DataFrame(lakehouse_data)\n",
        "        spark_df = spark.createDataFrame(lakehouse_df)\n",
        "        spark_df.write.mode(\"overwrite\").saveAsTable(lakehouse_table_name)\n",
        "        \n",
        "        print(f\"  âœ… Results saved to {lakehouse_table_name}\")\n",
        "    \n",
        "    # Summary\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"ğŸ“Š BATCH PROCESSING SUMMARY\")\n",
        "    print(f\"{'='*80}\")\n",
        "    print(f\"  âœ… Total Datasets Processed: {len(datasets_to_process)}\")\n",
        "    print(f\"  âœ… Successful: {successful_count}\")\n",
        "    print(f\"  âŒ Failed: {failed_count}\")\n",
        "    print(f\"  ğŸ“Š Total Tables: {sum(r['tables_count'] for r in all_results)}\")\n",
        "    print(f\"  ğŸ“Š Total Columns: {sum(r['columns_count'] for r in all_results)}\")\n",
        "    if generate_tsql and api_key:\n",
        "        print(f\"  ğŸ¤– T-SQL Scripts Generated: {sum(1 for r in all_results if r['tsql_scripts'] is not None)}\")\n",
        "    print(f\"{'='*80}\\n\")\n",
        "    \n",
        "    return {\n",
        "        'all_results': all_results,\n",
        "        'summary': {\n",
        "            'total_datasets': len(datasets_to_process),\n",
        "            'successful': successful_count,\n",
        "            'failed': failed_count,\n",
        "            'total_tables': sum(r['tables_count'] for r in all_results),\n",
        "            'total_columns': sum(r['columns_count'] for r in all_results)\n",
        "        }\n",
        "    }"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Usage Examples\n",
        "\n",
        "### Example 1: Process ALL datasets with Gemini (Cost-Optimized)\n",
        "```python\n",
        "gemini_key = \"your-gemini-api-key\"\n",
        "\n",
        "results = main(\n",
        "    agent_mode='gemini',\n",
        "    api_key=gemini_key,\n",
        "    process_all_datasets=True,\n",
        "    generate_tsql=True,\n",
        "    save_to_lakehouse=True\n",
        ")\n",
        "```\n",
        "\n",
        "### Example 2: Process ALL datasets with Claude\n",
        "```python\n",
        "claude_key = \"your-claude-api-key\"\n",
        "\n",
        "results = main(\n",
        "    agent_mode='claude',\n",
        "    api_key=claude_key,\n",
        "    process_all_datasets=True,\n",
        "    generate_tsql=True\n",
        ")\n",
        "```\n",
        "\n",
        "### Example 3: Process SPECIFIC datasets only\n",
        "```python\n",
        "specific_datasets = [\n",
        "    \"5fef939e-8bd0-40e1-a0c5-a7a9a49094d1\",\n",
        "    \"another-dataset-id\"\n",
        "]\n",
        "\n",
        "results = main(\n",
        "    agent_mode='gemini',\n",
        "    api_key=gemini_key,\n",
        "    dataset_ids=specific_datasets,\n",
        "    process_all_datasets=False\n",
        ")\n",
        "```\n",
        "\n",
        "### Example 4: Only prepare specs WITHOUT AI generation (FREE)\n",
        "```python\n",
        "results = main(\n",
        "    process_all_datasets=True,\n",
        "    generate_tsql=False,  # No AI calls = No cost\n",
        "    save_to_lakehouse=True\n",
        ")\n",
        "```\n",
        "\n",
        "### Accessing Results\n",
        "```python\n",
        "# Get all results\n",
        "all_results = results['all_results']\n",
        "\n",
        "# Get specific dataset result\n",
        "dataset_result = all_results[0]\n",
        "print(dataset_result['tsql_scripts'])\n",
        "\n",
        "# Get summary statistics\n",
        "summary = results['summary']\n",
        "print(f\"Processed {summary['successful']} datasets successfully\")\n",
        "\n",
        "# Read results from lakehouse\n",
        "saved_results = spark.table(\"tsql_migration_results\")\n",
        "display(saved_results)\n",
        "```"
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuration - CHANGE THESE VALUES\n",
        "gemini_key = \"your-gemini-api-key-here\"\n",
        "claude_key = \"your-claude-api-key-here\"\n",
        "\n",
        "# Run for ALL datasets with Gemini (recommended for cost efficiency)\n",
        "results = main(\n",
        "    agent_mode='gemini',      # Change to 'claude' to use Claude\n",
        "    api_key=gemini_key,       # Change to claude_key for Claude\n",
        "    process_all_datasets=True,\n",
        "    export_json=True,\n",
        "    generate_tsql=True,\n",
        "    save_to_lakehouse=True,\n",
        "    lakehouse_table_name=\"tsql_migration_results\"\n",
        ")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# View summary\n",
        "print(\"\\nğŸ“Š Summary:\")\n",
        "print(results['summary'])"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# View specific dataset result\n",
        "if results['all_results']:\n",
        "    first_result = results['all_results'][0]\n",
        "    print(f\"\\nDataset: {first_result['dataset_name']}\")\n",
        "    print(f\"Status: {first_result['status']}\")\n",
        "    print(f\"Tables: {first_result['tables_count']}\")\n",
        "    print(f\"Columns: {first_result['columns_count']}\")\n",
        "    \n",
        "    if first_result['tsql_scripts']:\n",
        "        print(\"\\nğŸ“œ T-SQL Scripts:\")\n",
        "        print(first_result['tsql_scripts'][:500] + \"...\")  # First 500 chars"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Read saved results from lakehouse\n",
        "saved_results = spark.table(\"tsql_migration_results\")\n",
        "display(saved_results)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "collapsed": false
      }
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "Synapse PySpark"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
