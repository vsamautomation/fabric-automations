{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Enhanced Fabric Workspace Scanner v03 - With PowerBI Report Analysis\n",
        "\n",
        "## New Features:\n",
        "- **üÜï PowerBI Report Metadata Extraction**: Extracts used columns, tables, and measures from PowerBI reports\n",
        "- **üÜï Dual Analysis Approach**: Uses both sempy_labs.list_semantic_model_objects() and JSON parsing\n",
        "- **üÜï PBIR vs PBIR-Legacy Support**: Handles both modern and legacy PowerBI report formats\n",
        "- **Lakehouse Storage**: Saves all analysis results to dedicated lakehouse tables\n",
        "- **Enhanced Context**: Additional context columns for Reports, Tables, Relationships, Dataflows\n",
        "- **Column Usage Analysis**: Detailed column usage analysis with context from measures, relationships, and dependencies\n",
        "\n",
        "## Tables Created in Lakehouse:\n",
        "- `workspace_analysis` - Workspace information\n",
        "- `dataset_analysis` - Datasets with Reports, Tables, Relationships, Dataflows context\n",
        "- `table_analysis` - Tables with usage context from measures, relationships, dependencies\n",
        "- `column_usage_analysis` - Columns with detailed usage analysis\n",
        "- `usage_summary` - Summary of dataset usage patterns\n",
        "- **üÜï `report_metadata_analysis`** - PowerBI report metadata extraction results\n",
        "- **üÜï `report_objects_used`** - Objects (tables, columns, measures) used by each report\n",
        "\n",
        "## Workflow:\n",
        "1. **Object Discovery** - Find workspaces, datasets, reports, dataflows\n",
        "2. **Dataset Processing** - Analyze tables, columns, measures, relationships\n",
        "3. **üÜï Report Analysis** - Extract metadata from PowerBI reports\n",
        "4. **Usage Analysis** - Cross-reference usage patterns\n",
        "5. **Lakehouse Storage** - Save all results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install semantic-link-labs for extended Fabric analytics\n",
        "!pip install semantic-link-labs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import sempy_labs\n",
        "import sempy.fabric as fabric\n",
        "from sempy_labs.report import ReportWrapper\n",
        "import re\n",
        "import sempy\n",
        "import json\n",
        "from pyspark.sql import SparkSession, functions as F\n",
        "from pyspark.sql.types import ArrayType, StringType, StructType, LongType, StructField, FloatType\n",
        "from pyspark.sql.functions import col\n",
        "from datetime import datetime\n",
        "import time\n",
        "from dataclasses import dataclass\n",
        "from typing import Dict, List, Optional, Any\n",
        "\n",
        "# Import our PowerBI metadata extractor\n",
        "from pbi_metadata_extractor import PowerBIMetadataExtractor\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "print(\"‚úÖ All imports successful and Spark session initialized\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# UTILITY FUNCTIONS AND DATA STRUCTURES\n",
        "# ============================================================\n",
        "\n",
        "@dataclass\n",
        "class DatasetInfo:\n",
        "    \"\"\"Data structure to hold comprehensive dataset information\"\"\"\n",
        "    ds_id: str\n",
        "    ds_name: str\n",
        "    ws_id: str\n",
        "    ws_name: str\n",
        "    dependencies_df: Optional[pd.DataFrame] = None\n",
        "    tables_df: Optional[pd.DataFrame] = None\n",
        "    relationships_df: Optional[pd.DataFrame] = None\n",
        "    measures_df: Optional[pd.DataFrame] = None\n",
        "    columns_df: Optional[pd.DataFrame] = None\n",
        "\n",
        "@dataclass\n",
        "class ReportMetadata:\n",
        "    \"\"\"Data structure to hold Power BI report metadata analysis\"\"\"\n",
        "    report_id: str\n",
        "    report_name: str\n",
        "    workspace_id: str\n",
        "    workspace_name: str\n",
        "    dataset_id: str\n",
        "    report_format: str\n",
        "    extraction_method: str\n",
        "    tables: List[str]\n",
        "    columns: List[str]\n",
        "    measures: List[str]\n",
        "    visuals_count: int\n",
        "    filters_count: int\n",
        "    extraction_success: bool\n",
        "    error_message: str = \"\"\n",
        "\n",
        "def sanitize_df_columns(df, extra_columns=False, ws_id=None, ds_id=None, ws_name=None, ds_name=None):\n",
        "    \"\"\"\n",
        "    Replaces spaces in column names with underscore to prevent errors during Spark Dataframe Creation\n",
        "    \"\"\"\n",
        "    if df.empty:\n",
        "        return df\n",
        "        \n",
        "    df.columns = [\n",
        "        re.sub(r'\\W+', \"_\", col.strip().lower())\n",
        "        for col in df.columns\n",
        "    ]\n",
        "\n",
        "    if extra_columns:\n",
        "        df['workspace_id'] = ws_id\n",
        "        df['dataset_id'] = ds_id\n",
        "        df['workspace_name'] = ws_name\n",
        "        df['dataset_name'] = ds_name\n",
        "        \n",
        "    return df\n",
        "\n",
        "def save_to_lakehouse(df, table_name, description=\"\"):\n",
        "    \"\"\"\n",
        "    Save DataFrame to lakehouse using Spark\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if df.empty:\n",
        "            print(f\"  ‚ö†Ô∏è Skipping empty DataFrame for table: {table_name}\")\n",
        "            return\n",
        "            \n",
        "        # Add analysis timestamp\n",
        "        df_with_timestamp = df.copy()\n",
        "        df_with_timestamp['analysis_date'] = datetime.now()\n",
        "        \n",
        "        # Convert to Spark DataFrame and save\n",
        "        spark_df = spark.createDataFrame(df_with_timestamp)\n",
        "        spark_df.write.mode(\"overwrite\").saveAsTable(table_name)\n",
        "        \n",
        "        print(f\"  ‚úÖ Saved {len(df)} records to '{table_name}' table\")\n",
        "        if description:\n",
        "            print(f\"     üìù {description}\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"  ‚ùå Error saving to {table_name}: {str(e)}\")\n",
        "\n",
        "# ============================================================\n",
        "# POWERBI METADATA EXTRACTOR CLASS\n",
        "# ============================================================\n",
        "\n",
        "class PowerBIMetadataExtractor:\n",
        "    \"\"\"Extracts columns, tables, and measures from Power BI report metadata\"\"\"\n",
        "    \n",
        "    def __init__(self):\n",
        "        self.tables = set()\n",
        "        self.columns = set()\n",
        "        self.measures = set()\n",
        "        self.visual_details = []\n",
        "        self.filter_details = []\n",
        "        \n",
        "    def extract_from_json_data(self, data: Dict[str, Any]) -> Dict[str, Any]:\n",
        "        \"\"\"Extract metadata from JSON data\"\"\"\n",
        "        self._reset()\n",
        "        \n",
        "        # Extract from sections\n",
        "        sections = data.get('sections', [])\n",
        "        \n",
        "        for section_idx, section in enumerate(sections):\n",
        "            section_name = section.get('displayName', f'Section_{section_idx}')\n",
        "            \n",
        "            # Extract from section-level filters\n",
        "            self._extract_from_filters(section.get('filters', []), 'section', section_name)\n",
        "            \n",
        "            # Extract from visual containers\n",
        "            visual_containers = section.get('visualContainers', [])\n",
        "            self._extract_from_visual_containers(visual_containers, section_name)\n",
        "        \n",
        "        # Compile results\n",
        "        results = {\n",
        "            'tables': sorted(list(self.tables)),\n",
        "            'columns': sorted(list(self.columns)),\n",
        "            'measures': sorted(list(self.measures)),\n",
        "            'summary': {\n",
        "                'total_tables': len(self.tables),\n",
        "                'total_columns': len(self.columns),\n",
        "                'total_measures': len(self.measures)\n",
        "            },\n",
        "            'visual_details': self.visual_details,\n",
        "            'filter_details': self.filter_details\n",
        "        }\n",
        "        \n",
        "        return results\n",
        "    \n",
        "    def _reset(self):\n",
        "        \"\"\"Reset all collections for new extraction\"\"\"\n",
        "        self.tables.clear()\n",
        "        self.columns.clear()\n",
        "        self.measures.clear()\n",
        "        self.visual_details.clear()\n",
        "        self.filter_details.clear()\n",
        "    \n",
        "    def _extract_from_visual_containers(self, visual_containers: List[Dict], section_name: str):\n",
        "        \"\"\"Extract from visualContainers array\"\"\"\n",
        "        for visual_idx, visual_container in enumerate(visual_containers):\n",
        "            visual_config = visual_container.get('config', {})\n",
        "            visual_name = visual_config.get('name', f'Visual_{visual_idx}')\n",
        "            \n",
        "            # Extract from visual-level filters\n",
        "            self._extract_from_filters(\n",
        "                visual_container.get('filters', []), \n",
        "                'visual', \n",
        "                f\"{section_name}->{visual_name}\"\n",
        "            )\n",
        "            \n",
        "            # Extract from singleVisual\n",
        "            single_visual = visual_config.get('singleVisual', {})\n",
        "            if single_visual:\n",
        "                self._extract_from_single_visual(single_visual, section_name, visual_name)\n",
        "    \n",
        "    def _extract_from_single_visual(self, single_visual: Dict, section_name: str, visual_name: str):\n",
        "        \"\"\"Extract from singleVisual object\"\"\"\n",
        "        visual_type = single_visual.get('visualType', 'unknown')\n",
        "        \n",
        "        # Extract from projections\n",
        "        projections = single_visual.get('projections', {})\n",
        "        projection_refs = []\n",
        "        \n",
        "        for projection_type, projection_list in projections.items():\n",
        "            for proj in projection_list:\n",
        "                query_ref = proj.get('queryRef', '')\n",
        "                if query_ref:\n",
        "                    projection_refs.append(query_ref)\n",
        "                    self._parse_query_ref(query_ref)\n",
        "        \n",
        "        # Extract from prototypeQuery\n",
        "        prototype_query = single_visual.get('prototypeQuery', {})\n",
        "        self._extract_from_prototype_query(prototype_query)\n",
        "        \n",
        "        # Store visual details\n",
        "        self.visual_details.append({\n",
        "            'section': section_name,\n",
        "            'visual_name': visual_name,\n",
        "            'visual_type': visual_type,\n",
        "            'projection_refs': projection_refs,\n",
        "            'has_prototype_query': bool(prototype_query)\n",
        "        })\n",
        "    \n",
        "    def _extract_from_prototype_query(self, prototype_query: Dict):\n",
        "        \"\"\"Extract from prototypeQuery object\"\"\"\n",
        "        # Extract tables from 'From' clause\n",
        "        from_clause = prototype_query.get('From', [])\n",
        "        for from_item in from_clause:\n",
        "            entity = from_item.get('Entity', '')\n",
        "            if entity:\n",
        "                self.tables.add(entity)\n",
        "        \n",
        "        # Extract columns and measures from 'Select' clause\n",
        "        select_clause = prototype_query.get('Select', [])\n",
        "        for select_item in select_clause:\n",
        "            name = select_item.get('Name', '')\n",
        "            \n",
        "            # Check if it's a Column\n",
        "            if 'Column' in select_item:\n",
        "                column_property = select_item['Column'].get('Property', '')\n",
        "                if column_property and name:\n",
        "                    self.columns.add(name)  # Store full reference (table.column)\n",
        "                    # Also extract table name\n",
        "                    if '.' in name:\n",
        "                        table_name = name.split('.')[0]\n",
        "                        self.tables.add(table_name)\n",
        "            \n",
        "            # Check if it's a Measure\n",
        "            elif 'Measure' in select_item:\n",
        "                measure_property = select_item['Measure'].get('Property', '')\n",
        "                if measure_property and name:\n",
        "                    self.measures.add(name)  # Store full reference (table.measure)\n",
        "                    # Also extract table name\n",
        "                    if '.' in name:\n",
        "                        table_name = name.split('.')[0]\n",
        "                        self.tables.add(table_name)\n",
        "    \n",
        "    def _extract_from_filters(self, filters: List[Dict], filter_type: str, context: str):\n",
        "        \"\"\"Extract from filters array\"\"\"\n",
        "        for filter_idx, filter_obj in enumerate(filters):\n",
        "            filter_name = filter_obj.get('name', f'Filter_{filter_idx}')\n",
        "            \n",
        "            # Extract from expression\n",
        "            expression = filter_obj.get('expression', {})\n",
        "            self._extract_from_expression(expression)\n",
        "            \n",
        "            # Extract from filter object (nested structure)\n",
        "            filter_def = filter_obj.get('filter', {})\n",
        "            if filter_def:\n",
        "                # Extract tables from 'From' clause in filter\n",
        "                from_clause = filter_def.get('From', [])\n",
        "                for from_item in from_clause:\n",
        "                    entity = from_item.get('Entity', '')\n",
        "                    if entity:\n",
        "                        self.tables.add(entity)\n",
        "                \n",
        "                # Extract from 'Where' clause - might contain column references\n",
        "                where_clause = filter_def.get('Where', [])\n",
        "                for where_item in where_clause:\n",
        "                    self._extract_from_where_condition(where_item)\n",
        "            \n",
        "            # Store filter details\n",
        "            self.filter_details.append({\n",
        "                'filter_type': filter_type,\n",
        "                'context': context,\n",
        "                'filter_name': filter_name,\n",
        "                'has_expression': bool(expression),\n",
        "                'has_filter_def': bool(filter_def)\n",
        "            })\n",
        "    \n",
        "    def _extract_from_expression(self, expression: Dict):\n",
        "        \"\"\"Extract from expression object\"\"\"\n",
        "        if 'Column' in expression:\n",
        "            # Extract table from SourceRef\n",
        "            column_expr = expression['Column']\n",
        "            source_ref = column_expr.get('Expression', {}).get('SourceRef', {})\n",
        "            entity = source_ref.get('Entity', '')\n",
        "            if entity:\n",
        "                self.tables.add(entity)\n",
        "            \n",
        "            # Extract column property\n",
        "            property_name = column_expr.get('Property', '')\n",
        "            if property_name and entity:\n",
        "                self.columns.add(f\"{entity}.{property_name}\")\n",
        "    \n",
        "    def _extract_from_where_condition(self, where_item: Dict):\n",
        "        \"\"\"Extract from WHERE condition\"\"\"\n",
        "        condition = where_item.get('Condition', {})\n",
        "        if 'In' in condition:\n",
        "            expressions = condition['In'].get('Expressions', [])\n",
        "            for expr in expressions:\n",
        "                self._extract_from_expression(expr)\n",
        "    \n",
        "    def _parse_query_ref(self, query_ref: str):\n",
        "        \"\"\"Parse queryRef format (e.g., 'table.column' or 'table.measure')\"\"\"\n",
        "        if '.' in query_ref:\n",
        "            table_name, field_name = query_ref.split('.', 1)\n",
        "            self.tables.add(table_name)\n",
        "            # We'll determine if it's a column or measure from prototype query\n",
        "            # For now, just store the full reference\n",
        "\n",
        "def extract_report_metadata(report_id: str, report_name: str, workspace_id: str, workspace_name: str, dataset_id: str) -> ReportMetadata:\n",
        "    \"\"\"\n",
        "    üÜï Extract metadata from Power BI reports using dual approach:\n",
        "    1. Try sempy_labs.list_semantic_model_objects() for PBIR format\n",
        "    2. Fall back to JSON parsing for PBIR-Legacy format\n",
        "    \"\"\"\n",
        "    print(f\"  üîç Analyzing report: {report_name}\")\n",
        "    \n",
        "    # Initialize result object\n",
        "    result = ReportMetadata(\n",
        "        report_id=report_id,\n",
        "        report_name=report_name,\n",
        "        workspace_id=workspace_id,\n",
        "        workspace_name=workspace_name,\n",
        "        dataset_id=dataset_id,\n",
        "        report_format=\"Unknown\",\n",
        "        extraction_method=\"None\",\n",
        "        tables=[],\n",
        "        columns=[],\n",
        "        measures=[],\n",
        "        visuals_count=0,\n",
        "        filters_count=0,\n",
        "        extraction_success=False\n",
        "    )\n",
        "    \n",
        "    try:\n",
        "        # Step 1: Try to determine report format and use appropriate method\n",
        "        report = ReportWrapper(report=report_id, workspace=workspace_id)\n",
        "        rep_format = report.format\n",
        "        result.report_format = rep_format\n",
        "        \n",
        "        print(f\"    Report format: {rep_format}\")\n",
        "        \n",
        "        if rep_format == \"PBIR\":\n",
        "            # Method 1: Use sempy_labs.list_semantic_model_objects() for PBIR format\n",
        "            try:\n",
        "                print(f\"    üîç Trying sempy_labs.list_semantic_model_objects()...\")\n",
        "                objects = sempy_labs.list_semantic_model_objects(report=report_id, workspace=workspace_id)\n",
        "                \n",
        "                if objects is not None and not objects.empty:\n",
        "                    # Process the objects DataFrame\n",
        "                    tables = objects[objects['Object Type'] == 'Table']['Object Name'].unique().tolist()\n",
        "                    columns = objects[objects['Object Type'] == 'Column']['Object Name'].unique().tolist()\n",
        "                    measures = objects[objects['Object Type'] == 'Measure']['Object Name'].unique().tolist()\n",
        "                    \n",
        "                    result.tables = tables\n",
        "                    result.columns = columns\n",
        "                    result.measures = measures\n",
        "                    result.extraction_method = \"sempy_labs_objects\"\n",
        "                    result.extraction_success = True\n",
        "                    \n",
        "                    print(f\"    ‚úÖ Extracted via sempy_labs: {len(tables)} tables, {len(columns)} columns, {len(measures)} measures\")\n",
        "                    return result\n",
        "                    \n",
        "            except NotImplementedError as e:\n",
        "                print(f\"    ‚ö†Ô∏è sempy_labs method not supported: {str(e)}\")\n",
        "            except Exception as e:\n",
        "                print(f\"    ‚ö†Ô∏è sempy_labs method failed: {str(e)}\")\n",
        "        \n",
        "        # Method 2: Fall back to JSON parsing (works for PBIR-Legacy and as backup)\n",
        "        print(f\"    üîç Trying JSON metadata extraction...\")\n",
        "        \n",
        "        # Get report JSON\n",
        "        report_json = sempy_labs.get_report_json(report=report_id, workspace=workspace_id)\n",
        "        \n",
        "        if report_json:\n",
        "            # Use our custom extractor\n",
        "            extractor = PowerBIMetadataExtractor()\n",
        "            extraction_results = extractor.extract_from_json_data(report_json)\n",
        "            \n",
        "            result.tables = extraction_results.get('tables', [])\n",
        "            result.columns = extraction_results.get('columns', [])\n",
        "            result.measures = extraction_results.get('measures', [])\n",
        "            result.visuals_count = len(extraction_results.get('visual_details', []))\n",
        "            result.filters_count = len(extraction_results.get('filter_details', []))\n",
        "            result.extraction_method = \"json_parsing\"\n",
        "            result.extraction_success = True\n",
        "            \n",
        "            print(f\"    ‚úÖ Extracted via JSON: {len(result.tables)} tables, {len(result.columns)} columns, {len(result.measures)} measures\")\n",
        "            print(f\"    üé® Found {result.visuals_count} visuals, {result.filters_count} filters\")\n",
        "            return result\n",
        "        else:\n",
        "            result.error_message = \"Could not retrieve report JSON\"\n",
        "            \n",
        "    except Exception as e:\n",
        "        result.error_message = f\"Extraction failed: {str(e)}\"\n",
        "        print(f\"    ‚ùå Error extracting metadata: {str(e)}\")\n",
        "    \n",
        "    return result\n",
        "\n",
        "print(\"‚úÖ Enhanced utility functions, PowerBI metadata extractor, and data structures defined\")
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def collect_dataset_info(ds_id: str, ds_name: str, ws_id: str, ws_name: str) -> DatasetInfo:\n",
        "    \"\"\"\n",
        "    üÜï Centralized function to collect all dataset-related information in one go\n",
        "    üÜï Improved: Individual error handling for each API call to prevent blocking\n",
        "    \"\"\"\n",
        "    print(f\"üîπ Processing dataset: {ds_name} (Workspace: {ws_name})\")\n",
        "    \n",
        "    dataset_info = DatasetInfo(ds_id, ds_name, ws_id, ws_name)\n",
        "    \n",
        "    # Get model dependencies - separate try-catch to not block other operations\n",
        "    try:\n",
        "        deps = fabric.get_model_calc_dependencies(dataset=ds_id, workspace=ws_id)\n",
        "        with deps as calc_deps:\n",
        "            dependencies_df = getattr(calc_deps, \"dependencies_df\", None)\n",
        "        \n",
        "        if dependencies_df is not None and not dependencies_df.empty:\n",
        "            dependencies_df = sanitize_df_columns(\n",
        "                df = dependencies_df, \n",
        "                extra_columns= True,\n",
        "                ws_id = ws_id, \n",
        "                ds_id= ds_id,\n",
        "                ws_name= ws_name,\n",
        "                ds_name= ds_name\n",
        "            )\n",
        "            dataset_info.dependencies_df = dependencies_df\n",
        "            print(f\"  Found {len(dependencies_df)} dependencies\")\n",
        "        else:\n",
        "            dataset_info.dependencies_df = pd.DataFrame()\n",
        "            print(f\"  No dependencies found for {ds_name}\")\n",
        "    except Exception as e:\n",
        "        print(f\"  ‚ö†Ô∏è Dependencies unavailable for {ds_name}: {e}\")\n",
        "        dataset_info.dependencies_df = pd.DataFrame()\n",
        "\n",
        "    # Get tables\n",
        "    try:\n",
        "        tables = fabric.list_tables(dataset=ds_id, workspace=ws_id)\n",
        "        if not tables.empty:\n",
        "            tables = sanitize_df_columns(\n",
        "                df = tables, \n",
        "                extra_columns = True,\n",
        "                ws_id = ws_id, \n",
        "                ds_id = ds_id,\n",
        "                ws_name = ws_name,\n",
        "                ds_name= ds_name\n",
        "            )\n",
        "            dataset_info.tables_df = tables\n",
        "            print(f\"  Found {len(tables)} tables\")\n",
        "    except Exception as e:\n",
        "        print(f\"  ‚ö†Ô∏è Tables unavailable for {ds_name}: {e}\")\n",
        "        \n",
        "    # Get relationships\n",
        "    try:\n",
        "        relationships = fabric.list_relationships(dataset=ds_id, workspace=ws_id, extended=True)\n",
        "        if not relationships.empty:\n",
        "            relationships = sanitize_df_columns(df = relationships)\n",
        "            relationships['qualified_from'] = \"'\" + relationships['from_table'] + \"'[\" + relationships['from_column'] + \"]\"\n",
        "            relationships['qualified_to'] = \"'\" + relationships['to_table'] + \"'[\" + relationships['to_column'] + \"]\"\n",
        "            dataset_info.relationships_df = relationships\n",
        "            print(f\"  Found {len(relationships)} relationships\")\n",
        "    except Exception as e:\n",
        "        print(f\"  ‚ö†Ô∏è Relationships unavailable for {ds_name}: {e}\")\n",
        "\n",
        "    # Get measures\n",
        "    try:\n",
        "        measures = fabric.list_measures(dataset=ds_id, workspace=ws_id)\n",
        "        if not measures.empty:\n",
        "            measures = sanitize_df_columns(df = measures)\n",
        "            dataset_info.measures_df = measures\n",
        "            print(f\"  Found {len(measures)} measures\")\n",
        "    except Exception as e:\n",
        "        print(f\"  ‚ö†Ô∏è Measures unavailable for {ds_name}: {e}\")\n",
        "\n",
        "    # Get columns\n",
        "    try:\n",
        "        columns = fabric.list_columns(dataset=ds_id, workspace=ws_id, extended=True)\n",
        "        if not columns.empty:\n",
        "            columns = sanitize_df_columns(\n",
        "                df = columns,\n",
        "                extra_columns= True,\n",
        "                ws_id = ws_id, \n",
        "                ds_id= ds_id,\n",
        "                ws_name= ws_name,\n",
        "                ds_name= ds_name\n",
        "            )\n",
        "            columns['qualified_name'] = \"'\" + columns['table_name'] + \"'[\" + columns['column_name'] + ']'\n",
        "            dataset_info.columns_df = columns\n",
        "            print(f\"  Found {len(columns)} columns\")\n",
        "    except Exception as e:\n",
        "        print(f\"  ‚ö†Ô∏è Columns unavailable for {ds_name}: {e}\")\n",
        "    \n",
        "    return dataset_info\n",
        "\n",
        "def analyze_table_usage(dataset_info: DatasetInfo) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    üÜï Analyze table usage for a single dataset using pre-collected data\n",
        "    \"\"\"\n",
        "    table_usage = []\n",
        "    \n",
        "    if dataset_info.tables_df is None or dataset_info.tables_df.empty:\n",
        "        return table_usage\n",
        "\n",
        "    # Determine used tables from all sources\n",
        "    used_tables = set()\n",
        "    \n",
        "    if dataset_info.dependencies_df is not None and not dataset_info.dependencies_df.empty:\n",
        "        used_tables.update(set(dataset_info.dependencies_df['referenced_table'].dropna()))\n",
        "    \n",
        "    if dataset_info.relationships_df is not None:\n",
        "        used_tables.update(set(dataset_info.relationships_df['from_table'].dropna()))\n",
        "        used_tables.update(set(dataset_info.relationships_df['to_table'].dropna()))\n",
        "    \n",
        "    if dataset_info.measures_df is not None:\n",
        "        used_tables.update(set(dataset_info.measures_df['table_name'].dropna()))\n",
        "    \n",
        "    used_tables = {t for t in used_tables if pd.notna(t)}\n",
        "    \n",
        "    # Analyze each table\n",
        "    for table_name in set(dataset_info.tables_df['name'].dropna()):\n",
        "        measures_count = 0\n",
        "        if dataset_info.measures_df is not None:\n",
        "            measures_count = len(dataset_info.measures_df[dataset_info.measures_df['table_name'] == table_name])\n",
        "        \n",
        "        rel_count = 0\n",
        "        if dataset_info.relationships_df is not None:\n",
        "            rel_count = len(dataset_info.relationships_df[\n",
        "                (dataset_info.relationships_df['from_table'] == table_name) | \n",
        "                (dataset_info.relationships_df['to_table'] == table_name)\n",
        "            ])\n",
        "        \n",
        "        dep_count = 0\n",
        "        if (dataset_info.dependencies_df is not None and \n",
        "            not dataset_info.dependencies_df.empty and \n",
        "            'referenced_table' in dataset_info.dependencies_df.columns):\n",
        "            dep_count = len(dataset_info.dependencies_df[dataset_info.dependencies_df['referenced_table'] == table_name])\n",
        "        \n",
        "        status = \"Unused\" if table_name not in used_tables else \"Used\"\n",
        "        \n",
        "        table_usage.append({\n",
        "            'workspace': dataset_info.ws_name,\n",
        "            'dataset': dataset_info.ds_name,\n",
        "            'table': table_name,\n",
        "            'measures': measures_count,\n",
        "            'relationships': rel_count,\n",
        "            'dependencies': dep_count,\n",
        "            'usage': status,\n",
        "            'workspace_id': dataset_info.ws_id,\n",
        "            'dataset_id': dataset_info.ds_id\n",
        "        })\n",
        "    \n",
        "    return table_usage\n",
        "\n",
        "def analyze_column_usage(dataset_info: DatasetInfo) -> List[Dict]:\n",
        "    \"\"\"\n",
        "    üÜï Analyze column usage for a single dataset using pre-collected data\n",
        "    \"\"\"\n",
        "    columns_usage = []\n",
        "    \n",
        "    if dataset_info.columns_df is None or dataset_info.columns_df.empty:\n",
        "        return columns_usage\n",
        "    \n",
        "    # Prepare dependency analysis\n",
        "    dep_columns_df = pd.DataFrame()\n",
        "    if (dataset_info.dependencies_df is not None and \n",
        "        not dataset_info.dependencies_df.empty and \n",
        "        'referenced_object_type' in dataset_info.dependencies_df.columns):\n",
        "        dep_columns_df = dataset_info.dependencies_df[\n",
        "            dataset_info.dependencies_df['referenced_object_type'].isin(['Column', 'Calc Column'])\n",
        "        ]\n",
        "    \n",
        "    # Extract subsets by object type\n",
        "    measures_refs_df = pd.DataFrame()\n",
        "    relationship_refs_df = pd.DataFrame()\n",
        "    \n",
        "    if not dep_columns_df.empty and 'object_type' in dep_columns_df.columns:\n",
        "        measures_refs_df = dep_columns_df[dep_columns_df['object_type'] == 'Measure']\n",
        "        relationship_refs_df = dep_columns_df[\n",
        "            dep_columns_df['object_type'].str.contains('Relationship', case=False, na=False)\n",
        "        ]\n",
        "    \n",
        "    # Determine used columns\n",
        "    dep_columns = set()\n",
        "    if not dep_columns_df.empty and 'referenced_full_object_name' in dep_columns_df.columns:\n",
        "        dep_columns = set(dep_columns_df['referenced_full_object_name'])\n",
        "    rel_columns = set()\n",
        "    \n",
        "    if dataset_info.relationships_df is not None:\n",
        "        rel_columns = set(dataset_info.relationships_df['qualified_from']).union(\n",
        "            set(dataset_info.relationships_df['qualified_to'])\n",
        "        )\n",
        "    \n",
        "    used_columns = dep_columns.union(rel_columns)\n",
        "    used_columns = {c for c in used_columns if pd.notna(c)}\n",
        "    \n",
        "    # Analyze each column\n",
        "    for _, row in dataset_info.columns_df.iterrows():\n",
        "        table_name = row['table_name']\n",
        "        column_name = row['column_name']\n",
        "        qualified_name = row['qualified_name']\n",
        "        \n",
        "        if pd.isna(column_name):\n",
        "            continue\n",
        "        \n",
        "        dep_count = 0\n",
        "        if not dep_columns_df.empty and 'referenced_full_object_name' in dep_columns_df.columns:\n",
        "            dep_count = len(dep_columns_df[dep_columns_df['referenced_full_object_name'] == qualified_name])\n",
        "        \n",
        "        # Safe column access with proper empty DataFrame handling\n",
        "        measure_c = 0\n",
        "        if not measures_refs_df.empty and 'referenced_full_object_name' in measures_refs_df.columns:\n",
        "            measure_c = len(measures_refs_df[measures_refs_df['referenced_full_object_name'] == qualified_name])\n",
        "        \n",
        "        relationship_c = 0\n",
        "        if not relationship_refs_df.empty and 'referenced_full_object_name' in relationship_refs_df.columns:\n",
        "            relationship_c = len(relationship_refs_df[relationship_refs_df['referenced_full_object_name'] == qualified_name])\n",
        "        \n",
        "        # Build referenced-by list\n",
        "        referenced_by = \"\"\n",
        "        if not dep_columns_df.empty and all(col in dep_columns_df.columns for col in ['referenced_full_object_name', 'object_name']):\n",
        "            referenced_by = \", \".join(\n",
        "                dep_columns_df.loc[\n",
        "                    dep_columns_df['referenced_full_object_name'] == qualified_name, 'object_name'\n",
        "                ].unique().tolist()\n",
        "            )\n",
        "        \n",
        "        usage_status = 'Used' if any([measure_c, relationship_c, dep_count]) else 'Unused'\n",
        "        \n",
        "        columns_usage.append({\n",
        "            'workspace': dataset_info.ws_name,\n",
        "            'dataset': dataset_info.ds_name,\n",
        "            'table': table_name,\n",
        "            'column': column_name,\n",
        "            'measures': measure_c,\n",
        "            'relationships': relationship_c,\n",
        "            'dependencies': dep_count,\n",
        "            'referenced_by': referenced_by,\n",
        "            'usage': usage_status,\n",
        "            'workspace_id': dataset_info.ws_id,\n",
        "            'dataset_id': dataset_info.ds_id\n",
        "        })\n",
        "    \n",
        "    return columns_usage\n",
        "\n",
        "print(\"‚úÖ Dataset analysis functions defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ------------------------------------------------------------\n",
        "# STEP 1: Object Discovery\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "print(\"üîç Discovering workspaces...\")\n",
        "\n",
        "workspaces_df = fabric.list_workspaces()\n",
        "workspaces_df = sanitize_df_columns(workspaces_df)\n",
        "workspaces_df = workspaces_df[['id', 'name', 'type']]\n",
        "display(workspaces_df)\n",
        "\n",
        "datasets_all, reports_all, paginated_all, dataflows_all = [], [], [], []\n",
        "\n",
        "for _, ws in workspaces_df.iterrows():\n",
        "    ws_id = ws['id']\n",
        "    ws_name = ws['name']\n",
        "    ws_type = ws['type']\n",
        "    if ws_type == \"AdminInsights\":\n",
        "        continue\n",
        "    print(f\"\\nüì¶ Scanning workspace: {ws_name}\")\n",
        "\n",
        "    # --- Datasets\n",
        "    try:\n",
        "        ds = fabric.list_datasets(workspace=ws_id)\n",
        "        if not ds.empty:\n",
        "            ds['workspace_id'] = ws_id\n",
        "            ds['workspace_name'] = ws_name\n",
        "            datasets_all.append(ds)\n",
        "    except Exception as e:\n",
        "        print(f\"  ‚ö†Ô∏è Datasets error in {ws_name}: {e}\")\n",
        "\n",
        "    # --- Reports (includes both Power BI and Paginated)\n",
        "    try:\n",
        "        rep = fabric.list_reports(workspace=ws_id)\n",
        "        if not rep.empty:\n",
        "            rep['workspace_id'] = ws_id\n",
        "            rep['workspace_name'] = ws_name\n",
        "            reports_all.append(rep)\n",
        "    except Exception as e:\n",
        "        print(f\"  ‚ö†Ô∏è Reports error in {ws_name}: {e}\")\n",
        "\n",
        "    # --- Dataflows\n",
        "    try:\n",
        "        dfs = fabric.list_items(type='Dataflow',workspace=ws_id)\n",
        "        if not dfs.empty:\n",
        "            dataflows_all.append(dfs)\n",
        "    except Exception as e:\n",
        "        print(f\"  ‚ö†Ô∏è Dataflows error in {ws_name}: {e}\")\n",
        "\n",
        "# Combine results\n",
        "datasets_df  = sanitize_df_columns(pd.concat(datasets_all, ignore_index=True) if datasets_all else pd.DataFrame())\n",
        "reports_df   = sanitize_df_columns(pd.concat(reports_all, ignore_index=True) if reports_all else pd.DataFrame())\n",
        "dataflows_df = sanitize_df_columns(pd.concat(dataflows_all, ignore_index=True) if dataflows_all else pd.DataFrame())\n",
        "\n",
        "# Split report types for clarity\n",
        "if not reports_df.empty and \"report_type\" in reports_df.columns:\n",
        "    pbi_reports_df = reports_df[reports_df[\"report_type\"] == \"PowerBIReport\"].copy()\n",
        "    paginated_reports_df = reports_df[reports_df[\"report_type\"] == \"PaginatedReport\"].copy()\n",
        "else:\n",
        "    pbi_reports_df = reports_df\n",
        "    paginated_reports_df = pd.DataFrame()\n",
        "\n",
        "# üÜï ADD OBJECT COUNTS TO WORKSPACE DATAFRAME\n",
        "print(\"\\nüìä Adding object counts to workspace dataframe...\")\n",
        "\n",
        "# Initialize count columns\n",
        "workspaces_df['dataset_count'] = 0\n",
        "workspaces_df['total_reports'] = 0\n",
        "workspaces_df['pbi_reports'] = 0\n",
        "workspaces_df['paginated_reports'] = 0\n",
        "workspaces_df['dataflows'] = 0\n",
        "\n",
        "# Count objects per workspace\n",
        "if not datasets_df.empty:\n",
        "    dataset_counts = datasets_df['workspace_id'].value_counts().to_dict()\n",
        "    workspaces_df['dataset_count'] = workspaces_df['id'].map(dataset_counts).fillna(0).astype(int)\n",
        "\n",
        "if not reports_df.empty:\n",
        "    # Total reports count\n",
        "    total_report_counts = reports_df['workspace_id'].value_counts().to_dict()\n",
        "    workspaces_df['total_reports'] = workspaces_df['id'].map(total_report_counts).fillna(0).astype(int)\n",
        "    \n",
        "    # PBI reports count\n",
        "    if not pbi_reports_df.empty:\n",
        "        pbi_counts = pbi_reports_df['workspace_id'].value_counts().to_dict()\n",
        "        workspaces_df['pbi_reports'] = workspaces_df['id'].map(pbi_counts).fillna(0).astype(int)\n",
        "    \n",
        "    # Paginated reports count\n",
        "    if not paginated_reports_df.empty:\n",
        "        paginated_counts = paginated_reports_df['workspace_id'].value_counts().to_dict()\n",
        "        workspaces_df['paginated_reports'] = workspaces_df['id'].map(paginated_counts).fillna(0).astype(int)\n",
        "\n",
        "if not dataflows_df.empty:\n",
        "    dataflow_counts = dataflows_df['workspace_id'].value_counts().to_dict()\n",
        "    workspaces_df['dataflows'] = workspaces_df['id'].map(dataflow_counts).fillna(0).astype(int)\n",
        "\n",
        "print(\"\\n‚úÖ Object discovery complete with enhanced workspace context.\")\n",
        "print(f\"  Workspaces: {len(workspaces_df)}\")\n",
        "print(f\"  Datasets:   {len(datasets_df)}\")\n",
        "print(f\"  Reports:    {len(reports_df)} (PBI: {len(pbi_reports_df)}, Paginated: {len(paginated_reports_df)})\")\n",
        "print(f\"  Dataflows:  {len(dataflows_df)}\")\n",
        "\n",
        "# Display enhanced workspace summary\n",
        "print(\"\\nüìã Workspace Object Summary:\")\n",
        "workspace_summary = workspaces_df[['name', 'dataset_count', 'total_reports', 'pbi_reports', 'paginated_reports', 'dataflows']]\n",
        "display(workspace_summary)\n",
        "\n",
        "# Save to Lakehouse - Enhanced Workspaces\n",
        "print(\"\\nüíæ Saving enhanced workspace data to lakehouse...\")\n",
        "save_to_lakehouse(workspaces_df, \"workspace_analysis\", \"Workspace information with object counts\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ------------------------------------------------------------\n",
        "# STEP 2: üÜï CENTRALIZED DATASET PROCESSING\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "# Collection containers for all analysis results\n",
        "all_dataset_info = []\n",
        "table_usage_results = []\n",
        "column_usage_results = []\n",
        "all_dependencies = []\n",
        "\n",
        "# Single loop through all datasets - collect everything at once\n",
        "for _, ds in datasets_df.iterrows():\n",
        "    ds_id = ds['dataset_id']\n",
        "    ds_name = ds['dataset_name']\n",
        "    ws_id = ds['workspace_id']\n",
        "    ws_name = ds['workspace_name']\n",
        "    \n",
        "    # üÜï Single comprehensive data collection per dataset\n",
        "    dataset_info = collect_dataset_info(ds_id, ds_name, ws_id, ws_name)\n",
        "    all_dataset_info.append(dataset_info)\n",
        "    \n",
        "    # Collect dependencies for later aggregation\n",
        "    if dataset_info.dependencies_df is not None and not dataset_info.dependencies_df.empty:\n",
        "        all_dependencies.append(dataset_info.dependencies_df)\n",
        "    \n",
        "    # üÜï Perform table analysis using collected data\n",
        "    table_analysis = analyze_table_usage(dataset_info)\n",
        "    table_usage_results.extend(table_analysis)\n",
        "    \n",
        "    # üÜï Perform column analysis using collected data\n",
        "    column_analysis = analyze_column_usage(dataset_info)\n",
        "    column_usage_results.extend(column_analysis)\n",
        "\n",
        "print(f\"\\n‚úÖ Centralized processing complete!\")\n",
        "print(f\"  üìä Processed {len(all_dataset_info)} datasets\")\n",
        "print(f\"  üìä Analyzed {len(table_usage_results)} tables\")\n",
        "print(f\"  üìä Analyzed {len(column_usage_results)} columns\")\n",
        "print(f\"  üìä Collected {len(all_dependencies)} dependency sets\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ------------------------------------------------------------\n",
        "# STEP 3: üÜï POWERBI REPORT METADATA EXTRACTION\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üìä POWER BI REPORT METADATA EXTRACTION\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Container for report metadata\n",
        "all_report_metadata = []\n",
        "report_objects_used = []\n",
        "\n",
        "# Process only PowerBI reports (not paginated reports)\n",
        "if not pbi_reports_df.empty:\n",
        "    print(f\"\\nüñºÔ∏è Processing {len(pbi_reports_df)} PowerBI reports...\")\n",
        "    \n",
        "    for idx, report_row in pbi_reports_df.iterrows():\n",
        "        report_id = report_row.get('id', '')\n",
        "        report_name = report_row.get('name', f'Report_{idx}')\n",
        "        workspace_id = report_row.get('workspace_id', '')\n",
        "        workspace_name = report_row.get('workspace_name', '')\n",
        "        dataset_id = report_row.get('dataset_id', '')\n",
        "        \n",
        "        print(f\"\\nüìä Processing report {idx+1}/{len(pbi_reports_df)}: {report_name}\")\n",
        "        \n",
        "        # Extract metadata using our dual approach function\n",
        "        report_metadata = extract_report_metadata(\n",
        "            report_id, report_name, workspace_id, workspace_name, dataset_id\n",
        "        )\n",
        "        \n",
        "        all_report_metadata.append(report_metadata)\n",
        "        \n",
        "        # Create detailed records for each object used by this report\n",
        "        if report_metadata.extraction_success:\n",
        "            # Add table records\n",
        "            for table in report_metadata.tables:\n",
        "                report_objects_used.append({\n",
        "                    'report_id': report_id,\n",
        "                    'report_name': report_name,\n",
        "                    'workspace_id': workspace_id,\n",
        "                    'workspace_name': workspace_name,\n",
        "                    'dataset_id': dataset_id,\n",
        "                    'object_type': 'Table',\n",
        "                    'object_name': table,\n",
        "                    'full_reference': table,\n",
        "                    'extraction_method': report_metadata.extraction_method\n",
        "                })\n",
        "            \n",
        "            # Add column records\n",
        "            for column in report_metadata.columns:\n",
        "                table_name = column.split('.')[0] if '.' in column else ''\n",
        "                column_name = column.split('.', 1)[1] if '.' in column else column\n",
        "                report_objects_used.append({\n",
        "                    'report_id': report_id,\n",
        "                    'report_name': report_name,\n",
        "                    'workspace_id': workspace_id,\n",
        "                    'workspace_name': workspace_name,\n",
        "                    'dataset_id': dataset_id,\n",
        "                    'object_type': 'Column',\n",
        "                    'object_name': column_name,\n",
        "                    'full_reference': column,\n",
        "                    'table_name': table_name,\n",
        "                    'extraction_method': report_metadata.extraction_method\n",
        "                })\n",
        "            \n",
        "            # Add measure records\n",
        "            for measure in report_metadata.measures:\n",
        "                table_name = measure.split('.')[0] if '.' in measure else ''\n",
        "                measure_name = measure.split('.', 1)[1] if '.' in measure else measure\n",
        "                report_objects_used.append({\n",
        "                    'report_id': report_id,\n",
        "                    'report_name': report_name,\n",
        "                    'workspace_id': workspace_id,\n",
        "                    'workspace_name': workspace_name,\n",
        "                    'dataset_id': dataset_id,\n",
        "                    'object_type': 'Measure',\n",
        "                    'object_name': measure_name,\n",
        "                    'full_reference': measure,\n",
        "                    'table_name': table_name,\n",
        "                    'extraction_method': report_metadata.extraction_method\n",
        "                })\n",
        "\n",
        "    print(f\"\\n‚úÖ Report metadata extraction complete!\")\n",
        "    print(f\"  üìä Processed {len(all_report_metadata)} reports\")\n",
        "    print(f\"  üìä Extracted {len(report_objects_used)} object references\")\n",
        "    \n",
        "    # Show success/failure summary\n",
        "    successful_extractions = sum(1 for r in all_report_metadata if r.extraction_success)\n",
        "    failed_extractions = len(all_report_metadata) - successful_extractions\n",
        "    print(f\"  ‚úÖ Successful extractions: {successful_extractions}\")\n",
        "    print(f\"  ‚ùå Failed extractions: {failed_extractions}\")\n",
        "    \n",
        "    # Show extraction methods used\n",
        "    methods = {}\n",
        "    for r in all_report_metadata:\n",
        "        if r.extraction_success:\n",
        "            methods[r.extraction_method] = methods.get(r.extraction_method, 0) + 1\n",
        "    \n",
        "    print(f\"  üîß Extraction methods used:\")\n",
        "    for method, count in methods.items():\n",
        "        print(f\"    - {method}: {count} reports\")\n",
        "\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è No PowerBI reports found for metadata extraction.\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ------------------------------------------------------------\n",
        "# STEP 4: Usage Analysis and Enhanced Dataset Context\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "print(\"\\nüîé Analyzing dataset usage and creating enhanced context...\")\n",
        "\n",
        "# 1Ô∏è‚É£ Dataset IDs used by any report (Power BI or Paginated)\n",
        "used_dataset_ids = set()\n",
        "if not reports_df.empty:\n",
        "    used_dataset_ids.update(reports_df['dataset_id'].dropna().unique())\n",
        "\n",
        "# 2Ô∏è‚É£ Dataset IDs used by dataflows (as sources)\n",
        "dataflow_refs = []\n",
        "\n",
        "for _, row in dataflows_df.iterrows():\n",
        "    try:\n",
        "        refs = sempy_labs.get_dataflow_references(row['id'], row['workspace_id'])\n",
        "        if refs is not None and not refs.empty:\n",
        "            refs['dataflow_id'] = row['id']\n",
        "            refs['dataflow_name'] = row['name']\n",
        "            refs['workspace_id'] = row['workspace_id']\n",
        "            dataflow_refs.append(refs)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "dataflow_refs_df = pd.concat(dataflow_refs, ignore_index=True) if dataflow_refs else pd.DataFrame()\n",
        "\n",
        "if not dataflow_refs_df.empty:\n",
        "    if 'source_dataset_id' in dataflow_refs_df.columns:\n",
        "        used_dataset_ids.update(dataflow_refs_df['source_dataset_id'].dropna().unique())\n",
        "\n",
        "# 3Ô∏è‚É£ Determine unused datasets\n",
        "unused_datasets_df = datasets_df[~datasets_df['dataset_id'].isin(used_dataset_ids)].copy()\n",
        "\n",
        "print(f\"‚úÖ Found {len(unused_datasets_df)} potentially unused datasets.\")\n",
        "\n",
        "# Enhanced Dataset Analysis with Context\n",
        "print(\"\\nüìä Creating enhanced dataset analysis with context...\")\n",
        "\n",
        "# Add context columns for each dataset using pre-collected data\n",
        "enhanced_datasets = datasets_df.copy()\n",
        "if not enhanced_datasets.empty:\n",
        "    enhanced_datasets['report_count'] = 0\n",
        "    enhanced_datasets['dataflow_count'] = 0\n",
        "    enhanced_datasets['table_count'] = 0\n",
        "    enhanced_datasets['relationship_count'] = 0\n",
        "    enhanced_datasets['is_used'] = enhanced_datasets['dataset_id'].isin(used_dataset_ids)\n",
        "    \n",
        "    # Count reports per dataset\n",
        "    if not reports_df.empty:\n",
        "        report_counts = reports_df.groupby('dataset_id').size().to_dict()\n",
        "        enhanced_datasets['report_count'] = enhanced_datasets['dataset_id'].map(report_counts).fillna(0)\n",
        "    \n",
        "    # Count dataflow references per dataset\n",
        "    if not dataflow_refs_df.empty and 'source_dataset_id' in dataflow_refs_df.columns:\n",
        "        dataflow_counts = dataflow_refs_df.groupby('source_dataset_id').size().to_dict()\n",
        "        enhanced_datasets['dataflow_count'] = enhanced_datasets['dataset_id'].map(dataflow_counts).fillna(0)\n",
        "    \n",
        "    # Add table and relationship counts using pre-collected data\n",
        "    for dataset_info in all_dataset_info:\n",
        "        mask = enhanced_datasets['dataset_id'] == dataset_info.ds_id\n",
        "        \n",
        "        if dataset_info.tables_df is not None:\n",
        "            enhanced_datasets.loc[mask, 'table_count'] = len(dataset_info.tables_df)\n",
        "        \n",
        "        if dataset_info.relationships_df is not None:\n",
        "            enhanced_datasets.loc[mask, 'relationship_count'] = len(dataset_info.relationships_df)\n",
        "\n",
        "# Save Enhanced Dataset Analysis to Lakehouse\n",
        "print(\"\\nüíæ Saving enhanced dataset analysis to lakehouse...\")\n",
        "save_to_lakehouse(enhanced_datasets, \"dataset_analysis\", \n",
        "                 \"Datasets with Reports, Tables, Relationships, Dataflows context\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ------------------------------------------------------------\n",
        "# STEP 5: üÜï RESULTS PROCESSING & LAKEHOUSE SAVING\n",
        "# Process and save all analysis results to lakehouse\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "print(\"\\nüíæ Processing and saving analysis results to lakehouse...\")\n",
        "\n",
        "# Convert table analysis results to DataFrame\n",
        "if table_usage_results:\n",
        "    table_usage_df = pd.DataFrame(table_usage_results)\n",
        "    print(\"\\nüíæ Saving table analysis to lakehouse...\")\n",
        "    save_to_lakehouse(table_usage_df, \"table_analysis\", \n",
        "                     \"Tables with usage context from measures, relationships, and dependencies\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No table usage data to save\")\n",
        "\n",
        "# Convert column analysis results to DataFrame\n",
        "if column_usage_results:\n",
        "    columns_usage_df = pd.DataFrame(column_usage_results)\n",
        "    print(\"\\nüíæ Saving column usage analysis to lakehouse...\")\n",
        "    save_to_lakehouse(columns_usage_df, \"column_usage_analysis\", \n",
        "                     \"Detailed column usage analysis with context from measures, relationships, and dependencies\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No column usage data to save\")\n",
        "\n",
        "# üÜï Save PowerBI report metadata analysis\n",
        "if all_report_metadata:\n",
        "    # Convert ReportMetadata objects to dictionaries\n",
        "    report_metadata_records = []\n",
        "    for metadata in all_report_metadata:\n",
        "        record = {\n",
        "            'report_id': metadata.report_id,\n",
        "            'report_name': metadata.report_name,\n",
        "            'workspace_id': metadata.workspace_id,\n",
        "            'workspace_name': metadata.workspace_name,\n",
        "            'dataset_id': metadata.dataset_id,\n",
        "            'report_format': metadata.report_format,\n",
        "            'extraction_method': metadata.extraction_method,\n",
        "            'tables_count': len(metadata.tables),\n",
        "            'columns_count': len(metadata.columns),\n",
        "            'measures_count': len(metadata.measures),\n",
        "            'visuals_count': metadata.visuals_count,\n",
        "            'filters_count': metadata.filters_count,\n",
        "            'extraction_success': metadata.extraction_success,\n",
        "            'error_message': metadata.error_message,\n",
        "            'tables_list': ','.join(metadata.tables) if metadata.tables else '',\n",
        "            'columns_list': ','.join(metadata.columns) if metadata.columns else '',\n",
        "            'measures_list': ','.join(metadata.measures) if metadata.measures else ''\n",
        "        }\n",
        "        report_metadata_records.append(record)\n",
        "    \n",
        "    report_metadata_df = pd.DataFrame(report_metadata_records)\n",
        "    print(\"\\nüíæ Saving PowerBI report metadata analysis to lakehouse...\")\n",
        "    save_to_lakehouse(report_metadata_df, \"report_metadata_analysis\", \n",
        "                     \"PowerBI report metadata extraction results with dual approach\")\n",
        "    \n",
        "    # Display summary of report metadata\n",
        "    print(\"\\nüìä PowerBI Report Metadata Summary:\")\n",
        "    display(report_metadata_df[['report_name', 'report_format', 'extraction_method', \n",
        "                               'tables_count', 'columns_count', 'measures_count', \n",
        "                               'extraction_success']].head(10))\n",
        "    \n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No report metadata to save\")\n",
        "\n",
        "# üÜï Save detailed report objects usage\n",
        "if report_objects_used:\n",
        "    report_objects_df = pd.DataFrame(report_objects_used)\n",
        "    print(\"\\nüíæ Saving detailed report objects usage to lakehouse...\")\n",
        "    save_to_lakehouse(report_objects_df, \"report_objects_used\", \n",
        "                     \"Detailed breakdown of objects (tables, columns, measures) used by each PowerBI report\")\n",
        "    \n",
        "    # Display summary of objects used\n",
        "    print(\"\\nüìä Report Objects Usage Summary:\")\n",
        "    objects_summary = report_objects_df.groupby(['object_type']).size().reset_index(name='count')\n",
        "    display(objects_summary)\n",
        "    \n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No report objects usage data to save\")\n",
        "\n",
        "print(\"\\n‚úÖ All analysis results saved to lakehouse!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ------------------------------------------------------------\n",
        "# STEP 6: Usage Summary Table Creation\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "print(\"\\nüìã Creating usage summary table...\")\n",
        "\n",
        "summary_records = []\n",
        "\n",
        "for _, ds in datasets_df.iterrows():\n",
        "    ds_id = ds['dataset_id']\n",
        "    ds_name = ds['dataset_name']\n",
        "    ws_name = ds['workspace_name']\n",
        "\n",
        "    # Reports using this dataset\n",
        "    rep_refs = pbi_reports_df[pbi_reports_df['dataset_id'] == ds_id]\n",
        "    paginated_refs = rep_refs[rep_refs['report_type'] == 'PaginatedReport'] if 'report_type' in rep_refs.columns else pd.DataFrame()\n",
        "    normal_refs = rep_refs[rep_refs['report_type'] != 'PaginatedReport'] if 'report_type' in rep_refs.columns else rep_refs\n",
        "\n",
        "    # Dataflows referencing this dataset (if any)\n",
        "    dataflow_refs = []\n",
        "    if not dataflow_refs_df.empty and 'source_dataset_id' in dataflow_refs_df.columns:\n",
        "        dataflow_refs = dataflow_refs_df[dataflow_refs_df['source_dataset_id'] == ds_id]\n",
        "\n",
        "    # Determine usage\n",
        "    total_refs = len(rep_refs) + len(dataflow_refs)\n",
        "    usage_status = \"Unused\" if total_refs == 0 else \"Used\"\n",
        "\n",
        "    # Add records for all associated reports\n",
        "    if not rep_refs.empty:\n",
        "        for _, r in rep_refs.iterrows():\n",
        "            summary_records.append({\n",
        "                \"Dataset_Workspace\": ws_name,\n",
        "                \"Dataset_Name\": ds_name,\n",
        "                \"Report_Name\": r['name'],\n",
        "                \"Report_Type\": r.get('report_type', 'PowerBIReport'),\n",
        "                \"Report_Workspace\": r['workspace_name'],\n",
        "                \"Usage_Status\": usage_status,\n",
        "                \"Total_References\": total_refs\n",
        "            })\n",
        "    # Add records for datasets with no references\n",
        "    elif total_refs == 0:\n",
        "        summary_records.append({\n",
        "            \"Dataset_Workspace\": ws_name,\n",
        "            \"Dataset_Name\": ds_name,\n",
        "            \"Report_Name\": None,\n",
        "            \"Report_Type\": None,\n",
        "            \"Report_Workspace\": None,\n",
        "            \"Usage_Status\": usage_status,\n",
        "            \"Total_References\": total_refs\n",
        "        })\n",
        "\n",
        "usage_summary_df = pd.DataFrame(summary_records)\n",
        "display(usage_summary_df)\n",
        "\n",
        "# Save Usage Summary to Lakehouse\n",
        "print(\"\\nüíæ Saving usage summary to lakehouse...\")\n",
        "save_to_lakehouse(usage_summary_df, \"usage_summary\", \"Summary of dataset usage patterns\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ------------------------------------------------------------\n",
        "# STEP 7: Final Summary and Performance Metrics\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üéâ ENHANCED FABRIC WORKSPACE ANALYSIS COMPLETE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Summary statistics\n",
        "print(f\"üìä Discovery Summary:\")\n",
        "print(f\"  Workspaces: {len(workspaces_df)}\")\n",
        "print(f\"  Datasets:   {len(datasets_df)}\")\n",
        "print(f\"  Reports:    {len(reports_df)}\")\n",
        "print(f\"  Dataflows:  {len(dataflows_df)}\")\n",
        "\n",
        "if table_usage_results:\n",
        "    used_tables = sum(1 for t in table_usage_results if t['usage'] == 'Used')\n",
        "    unused_tables = sum(1 for t in table_usage_results if t['usage'] == 'Unused')\n",
        "    print(f\"  Tables:     {len(table_usage_results)} (Used: {used_tables}, Unused: {unused_tables})\")\n",
        "\n",
        "if column_usage_results:\n",
        "    used_columns = sum(1 for c in column_usage_results if c['usage'] == 'Used')\n",
        "    unused_columns = sum(1 for c in column_usage_results if c['usage'] == 'Unused')\n",
        "    print(f\"  Columns:    {len(column_usage_results)} (Used: {used_columns}, Unused: {unused_columns})\")\n",
        "\n",
        "# üÜï PowerBI Report Analysis Summary\n",
        "if all_report_metadata:\n",
        "    successful_reports = sum(1 for r in all_report_metadata if r.extraction_success)\n",
        "    failed_reports = len(all_report_metadata) - successful_reports\n",
        "    total_objects_extracted = sum(len(r.tables) + len(r.columns) + len(r.measures) for r in all_report_metadata if r.extraction_success)\n",
        "    \n",
        "    print(f\"\\nüñºÔ∏è PowerBI Report Analysis:\")\n",
        "    print(f\"  Reports Analyzed: {len(all_report_metadata)}\")\n",
        "    print(f\"  Successful Extractions: {successful_reports}\")\n",
        "    print(f\"  Failed Extractions: {failed_reports}\")\n",
        "    print(f\"  Total Objects Extracted: {total_objects_extracted}\")\n",
        "    \n",
        "    # Show extraction method breakdown\n",
        "    methods_count = {}\n",
        "    for r in all_report_metadata:\n",
        "        if r.extraction_success:\n",
        "            methods_count[r.extraction_method] = methods_count.get(r.extraction_method, 0) + 1\n",
        "    \n",
        "    print(f\"  Extraction Methods Used:\")\n",
        "    for method, count in methods_count.items():\n",
        "        print(f\"    - {method}: {count} reports\")\n",
        "\n",
        "print(f\"\\nüíæ Lakehouse Tables Created:\")\n",
        "print(f\"  üìä workspace_analysis - Basic workspace information\")\n",
        "print(f\"  üìä dataset_analysis - Datasets with context (Reports, Tables, Relationships, Dataflows)\")\n",
        "print(f\"  üìä table_analysis - Tables with usage context from measures, relationships, dependencies\")\n",
        "print(f\"  üìä column_usage_analysis - Detailed column usage analysis\")\n",
        "print(f\"  üìä usage_summary - Summary of dataset usage patterns\")\n",
        "print(f\"  üÜï report_metadata_analysis - PowerBI report metadata extraction results\")\n",
        "print(f\"  üÜï report_objects_used - Objects (tables, columns, measures) used by each report\")\n",
        "\n",
        "# Display final unused datasets\n",
        "if not unused_datasets_df.empty:\n",
        "    print(\"\\n‚ö†Ô∏è UNUSED DATASETS\")\n",
        "    for _, row in unused_datasets_df.iterrows():\n",
        "        print(f\" - {row['workspace_name']} ‚Üí {row['dataset_name']}\")\n",
        "else:\n",
        "    print(\"\\nüéâ No unused datasets found!\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"‚úÖ Check your lakehouse for detailed results.\")\n",
        "print(\"üÜï NEW: Report metadata analysis provides insights into PowerBI report object usage!\")\n",
        "print(\"=\"*80)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
