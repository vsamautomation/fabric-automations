{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Enhanced Fabric Workspace Scanner v02\n",
        "\n",
        "## Features Added from Version 1:\n",
        "- **Lakehouse Storage**: Saves all analysis results to dedicated lakehouse tables\n",
        "- **Enhanced Context**: Additional context columns for Reports, Tables, Relationships, Dataflows\n",
        "- **Column Usage Analysis**: Detailed column usage analysis with context from measures, relationships, and dependencies\n",
        "- **Spark Integration**: Uses Spark DataFrames for efficient storage\n",
        "\n",
        "## Tables Created in Lakehouse:\n",
        "- `workspace_analysis` - Workspace information\n",
        "- `dataset_analysis` - Datasets with Reports, Tables, Relationships, Dataflows context\n",
        "- `table_analysis` - Tables with usage context from measures, relationships, dependencies\n",
        "- `column_usage_analysis` - Columns with detailed usage analysis\n",
        "- `usage_summary` - Summary of dataset usage patterns\n"
      ],
      "metadata": {},
      "id": "header-cell"
    },
    {
      "cell_type": "code",
      "source": [
        "# Install semantic-link-labs for extended Fabric analytics\n",
        "!pip install semantic-link-labs"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "jupyter": {
          "outputs_hidden": true
        }
      },
      "id": "install-cell"
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import sempy_labs\n",
        "import sempy.fabric as fabric\n",
        "from sempy_labs.report import ReportWrapper\n",
        "import re\n",
        "import sempy\n",
        "from pyspark.sql import SparkSession, functions as F\n",
        "from pyspark.sql.types import ArrayType, StringType, StructType, LongType, StructField, FloatType\n",
        "from pyspark.sql.functions import col\n",
        "from datetime import datetime\n",
        "import time\n",
        "\n",
        "# Initialize Spark session\n",
        "spark = SparkSession.builder.getOrCreate()\n",
        "\n",
        "print(\"‚úÖ All imports successful and Spark session initialized\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "id": "imports-cell"
    },
    {
      "cell_type": "code",
      "source": [
        "def sanitize_df_columns(df, extra_columns=False, ws_id=None, ds_id=None):\n",
        "    \"\"\"\n",
        "    Replaces spaces in column names with underscore to prevent errors during Spark Dataframe Creation\n",
        "    \"\"\"\n",
        "    if df.empty:\n",
        "        return df\n",
        "        \n",
        "    df.columns = [\n",
        "        re.sub(r'\\W+', \"_\", col.strip().lower())\n",
        "        for col in df.columns\n",
        "    ]\n",
        "\n",
        "    if extra_columns:\n",
        "        df['workspace_id'] = ws_id\n",
        "        df['dataset_id'] = ds_id\n",
        "        \n",
        "    return df\n",
        "\n",
        "def save_to_lakehouse(df, table_name, description=\"\"):\n",
        "    \"\"\"\n",
        "    Save DataFrame to lakehouse using Spark - extracted from Version 1\n",
        "    \"\"\"\n",
        "    try:\n",
        "        if df.empty:\n",
        "            print(f\"  ‚ö†Ô∏è Skipping empty DataFrame for table: {table_name}\")\n",
        "            return\n",
        "            \n",
        "        # Add analysis timestamp\n",
        "        df_with_timestamp = df.copy()\n",
        "        df_with_timestamp['analysis_date'] = datetime.now()\n",
        "        \n",
        "        # Convert to Spark DataFrame and save\n",
        "        spark_df = spark.createDataFrame(df_with_timestamp)\n",
        "        spark_df.write.mode(\"overwrite\").saveAsTable(table_name)\n",
        "        \n",
        "        print(f\"  ‚úÖ Saved {len(df)} records to '{table_name}' table\")\n",
        "        if description:\n",
        "            print(f\"     üìù {description}\")\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"  ‚ùå Error saving to {table_name}: {str(e)}\")\n",
        "\n",
        "print(\"‚úÖ Utility functions defined\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "id": "utils-cell"
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------\n",
        "# STEP 1: Object Discovery\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "print(\"üîç Discovering workspaces...\")\n",
        "\n",
        "workspaces_df = fabric.list_workspaces()\n",
        "workspaces_df = sanitize_df_columns(workspaces_df)\n",
        "workspaces_df = workspaces_df[['id', 'name', 'type']]\n",
        "display(workspaces_df)\n",
        "\n",
        "datasets_all, reports_all, paginated_all, dataflows_all = [], [], [], []\n",
        "\n",
        "for _, ws in workspaces_df.iterrows():\n",
        "    ws_id = ws['id']\n",
        "    ws_name = ws['name']\n",
        "    ws_type = ws['type']\n",
        "    if ws_type == \"AdminInsights\":\n",
        "        continue\n",
        "    print(f\"\\nüì¶ Scanning workspace: {ws_name}\")\n",
        "\n",
        "   # --- Datasets\n",
        "    try:\n",
        "        ds = fabric.list_datasets(workspace=ws_id)\n",
        "        if not ds.empty:\n",
        "            ds['workspace_id'] = ws_id\n",
        "            ds['workspace_name'] = ws_name\n",
        "            datasets_all.append(ds)\n",
        "    except Exception as e:\n",
        "        print(f\"  ‚ö†Ô∏è Datasets error in {ws_name}: {e}\")\n",
        "\n",
        "    # --- Reports (includes both Power BI and Paginated)\n",
        "    try:\n",
        "        rep = fabric.list_reports(workspace=ws_id)\n",
        "        if not rep.empty:\n",
        "            rep['workspace_id'] = ws_id\n",
        "            rep['workspace_name'] = ws_name\n",
        "            reports_all.append(rep)\n",
        "    except Exception as e:\n",
        "        print(f\"  ‚ö†Ô∏è Reports error in {ws_name}: {e}\")\n",
        "\n",
        "    # --- Dataflows\n",
        "    try:\n",
        "        dfs = fabric.list_items(type='Dataflow',workspace=ws_id)\n",
        "        if not dfs.empty:\n",
        "            dfs['workspace_id'] = ws_id\n",
        "            dfs['workspace_name'] = ws_name\n",
        "            dataflows_all.append(dfs)\n",
        "    except Exception as e:\n",
        "        print(f\"  ‚ö†Ô∏è Dataflows error in {ws_name}: {e}\")\n",
        "\n",
        "# Combine results\n",
        "datasets_df  = sanitize_df_columns(pd.concat(datasets_all, ignore_index=True) if datasets_all else pd.DataFrame())\n",
        "reports_df   = sanitize_df_columns(pd.concat(reports_all, ignore_index=True) if reports_all else pd.DataFrame())\n",
        "dataflows_df = sanitize_df_columns(pd.concat(dataflows_all, ignore_index=True) if dataflows_all else pd.DataFrame())\n",
        "\n",
        "# Split report types for clarity\n",
        "if not reports_df.empty and \"report_type\" in reports_df.columns:\n",
        "    pbi_reports_df = reports_df[reports_df[\"report_type\"] == \"PowerBIReport\"].copy()\n",
        "    paginated_reports_df = reports_df[reports_df[\"report_type\"] == \"PaginatedReport\"].copy()\n",
        "else:\n",
        "    pbi_reports_df = reports_df\n",
        "    paginated_reports_df = pd.DataFrame()\n",
        "\n",
        "print(\"\\n‚úÖ Object discovery complete.\")\n",
        "print(f\"  Workspaces: {len(workspaces_df)}\")\n",
        "print(f\"  Datasets:   {len(datasets_df)}\")\n",
        "print(f\"  Reports:    {len(reports_df)}\")\n",
        "print(f\"  Paginated:  {len(paginated_reports_df)}\")\n",
        "print(f\"  Dataflows:  {len(dataflows_df)}\")\n",
        "\n",
        "# üÜï Save to Lakehouse - Workspaces\n",
        "print(\"\\nüíæ Saving workspace data to lakehouse...\")\n",
        "save_to_lakehouse(workspaces_df, \"workspace_analysis\", \"Basic workspace information\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "collapsed": false
      },
      "id": "discovery-cell"
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------\n",
        "# STEP 2: Usage Analysis\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "print(\"\\nüîé Analyzing dataset usage...\")\n",
        "\n",
        "# 1Ô∏è‚É£ Dataset IDs used by any report (Power BI or Paginated)\n",
        "used_dataset_ids = set()\n",
        "if not reports_df.empty:\n",
        "    used_dataset_ids.update(reports_df['dataset_id'].dropna().unique())\n",
        "\n",
        "# 2Ô∏è‚É£ Dataset IDs used by dataflows (as sources)\n",
        "dataflow_refs = []\n",
        "\n",
        "for _, row in dataflows_df.iterrows():\n",
        "    try:\n",
        "        refs = sempy_labs.get_dataflow_references(row['id'], row['workspace_id'])\n",
        "        if refs is not None and not refs.empty:\n",
        "            refs['dataflow_id'] = row['id']\n",
        "            refs['dataflow_name'] = row['name']\n",
        "            refs['workspace_id'] = row['workspace_id']\n",
        "            dataflow_refs.append(refs)\n",
        "    except Exception:\n",
        "        pass\n",
        "\n",
        "dataflow_refs_df = pd.concat(dataflow_refs, ignore_index=True) if dataflow_refs else pd.DataFrame()\n",
        "\n",
        "if not dataflow_refs_df.empty:\n",
        "    if 'source_dataset_id' in dataflow_refs_df.columns:\n",
        "        used_dataset_ids.update(dataflow_refs_df['source_dataset_id'].dropna().unique())\n",
        "\n",
        "# 3Ô∏è‚É£ Determine unused datasets\n",
        "unused_datasets_df = datasets_df[~datasets_df['dataset_id'].isin(used_dataset_ids)].copy()\n",
        "\n",
        "print(f\"‚úÖ Found {len(unused_datasets_df)} potentially unused datasets.\")\n",
        "\n",
        "# üÜï Enhanced Dataset Analysis with Context from Version 1\n",
        "print(\"\\nüìä Creating enhanced dataset analysis with context...\")\n",
        "\n",
        "# Add context columns for each dataset\n",
        "enhanced_datasets = datasets_df.copy()\n",
        "if not enhanced_datasets.empty:\n",
        "    enhanced_datasets['report_count'] = 0\n",
        "    enhanced_datasets['dataflow_count'] = 0\n",
        "    enhanced_datasets['table_count'] = 0\n",
        "    enhanced_datasets['relationship_count'] = 0\n",
        "    enhanced_datasets['is_used'] = enhanced_datasets['dataset_id'].isin(used_dataset_ids)\n",
        "    \n",
        "    # Count reports per dataset\n",
        "    if not reports_df.empty:\n",
        "        report_counts = reports_df.groupby('dataset_id').size().to_dict()\n",
        "        enhanced_datasets['report_count'] = enhanced_datasets['dataset_id'].map(report_counts).fillna(0)\n",
        "    \n",
        "    # Count dataflow references per dataset\n",
        "    if not dataflow_refs_df.empty and 'source_dataset_id' in dataflow_refs_df.columns:\n",
        "        dataflow_counts = dataflow_refs_df.groupby('source_dataset_id').size().to_dict()\n",
        "        enhanced_datasets['dataflow_count'] = enhanced_datasets['dataset_id'].map(dataflow_counts).fillna(0)\n",
        "\n",
        "# üÜï Save Enhanced Dataset Analysis to Lakehouse\n",
        "save_to_lakehouse(enhanced_datasets, \"dataset_analysis\", \n",
        "                 \"Datasets with Reports, Tables, Relationships, Dataflows context\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "collapsed": false
      },
      "id": "usage-analysis-cell"
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------\n",
        "# STEP 3: Usage Summary Table (Enhanced)\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "summary_records = []\n",
        "\n",
        "for _, ds in datasets_df.iterrows():\n",
        "    ds_id = ds['dataset_id']\n",
        "    ds_name = ds['dataset_name']\n",
        "    ws_name = ds['workspace_name']\n",
        "\n",
        "    # Reports using this dataset\n",
        "    rep_refs = pbi_reports_df[pbi_reports_df['dataset_id'] == ds_id]\n",
        "    paginated_refs = rep_refs[rep_refs['report_type'] == 'PaginatedReport'] if 'report_type' in rep_refs.columns else pd.DataFrame()\n",
        "    normal_refs = rep_refs[rep_refs['report_type'] != 'PaginatedReport'] if 'report_type' in rep_refs.columns else rep_refs\n",
        "\n",
        "    # Dataflows referencing this dataset (if any)\n",
        "    dataflow_refs = []\n",
        "    if not dataflow_refs_df.empty and 'source_dataset_id' in dataflow_refs_df.columns:\n",
        "        dataflow_refs = dataflow_refs_df[dataflow_refs_df['source_dataset_id'] == ds_id]\n",
        "\n",
        "    # Determine usage\n",
        "    total_refs = len(rep_refs) + len(dataflow_refs)\n",
        "    usage_status = \"Unused\" if total_refs == 0 else \"Used\"\n",
        "\n",
        "    # Add records for all associated reports\n",
        "    if not rep_refs.empty:\n",
        "        for _, r in rep_refs.iterrows():\n",
        "            summary_records.append({\n",
        "                \"Dataset_Workspace\": ws_name,\n",
        "                \"Dataset_Name\": ds_name,\n",
        "                \"Report_Name\": r['name'],\n",
        "                \"Report_Type\": r.get('report_type', 'PowerBIReport'),\n",
        "                \"Report_Workspace\": r['workspace_name'],\n",
        "                \"Usage_Status\": usage_status,\n",
        "                \"Total_References\": total_refs\n",
        "            })\n",
        "    # Add records for datasets with no references\n",
        "    elif total_refs == 0:\n",
        "        summary_records.append({\n",
        "            \"Dataset_Workspace\": ws_name,\n",
        "            \"Dataset_Name\": ds_name,\n",
        "            \"Report_Name\": None,\n",
        "            \"Report_Type\": None,\n",
        "            \"Report_Workspace\": None,\n",
        "            \"Usage_Status\": usage_status,\n",
        "            \"Total_References\": total_refs\n",
        "        })\n",
        "\n",
        "usage_summary_df = pd.DataFrame(summary_records)\n",
        "\n",
        "display(usage_summary_df)\n",
        "\n",
        "# üÜï Save Usage Summary to Lakehouse\n",
        "print(\"\\nüíæ Saving usage summary to lakehouse...\")\n",
        "save_to_lakehouse(usage_summary_df, \"usage_summary\", \"Summary of dataset usage patterns\")\n"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "collapsed": false
      },
      "id": "summary-cell"
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------\n",
        "# STEP 4: Enhanced Table Analysis (From Version 1)\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "print(\"\\nüîç Enhanced Table Analysis with Usage Context...\")\n",
        "\n",
        "table_usage = []\n",
        "all_dependencies = []\n",
        "\n",
        "for _, ds in datasets_df.iterrows():\n",
        "    ds_id = ds['dataset_id']\n",
        "    ds_name = ds['dataset_name']\n",
        "    ws_id = ds['workspace_id']\n",
        "    ws_name = ds['workspace_name']\n",
        "    print(f\"\\nüîπ Dataset: {ds_name} (Workspace: {ws_name})\")    \n",
        "    \n",
        "    try:\n",
        "        # Get model dependencies\n",
        "        deps = fabric.get_model_calc_dependencies(dataset=ds_id, workspace=ws_id)\n",
        "        with deps as calc_deps:\n",
        "            dependencies_df = getattr(calc_deps, \"dependencies_df\", None)\n",
        "        \n",
        "        if dependencies_df is not None:\n",
        "            dependencies_df['dataset_id'] = ds_id\n",
        "            dependencies_df['dataset_name'] = ds_name\n",
        "            dependencies_df['workspace_id'] = ws_id\n",
        "            dependencies_df['workspace_name'] = ws_name\n",
        "            all_dependencies.append(dependencies_df)\n",
        "\n",
        "        # Get tables\n",
        "        tables = fabric.list_tables(dataset=ds_id,workspace=ws_id)\n",
        "        tables = sanitize_df_columns(tables)\n",
        "        tables['workspace_id'] = ws_id\n",
        "        tables['dataset_id'] = ds_id\n",
        "        tables['workspace_name'] = ws_name\n",
        "        tables['dataset_name'] = ds_name\n",
        "\n",
        "        print(f\" Found {len(tables)} total tables\")\n",
        "        \n",
        "        # Get relationships\n",
        "        relationships = fabric.list_relationships(dataset=ds_id, workspace=ws_id, extended=True)\n",
        "        relationships['qualified_from'] = \"'\" + relationships['From Table'] + \"'[\" + relationships['From Column'] + \"]\"\n",
        "        relationships['qualified_to'] = \"'\" + relationships['To Table'] + \"'[\" + relationships['To Column'] + \"]\"\n",
        "\n",
        "        # Get measures\n",
        "        measures = fabric.list_measures(dataset=ds_id, workspace=ws_id)\n",
        "\n",
        "        # Determine used tables\n",
        "        used_tables = set()\n",
        "        if dependencies_df is not None:\n",
        "            used_tables.update(set(dependencies_df['Referenced Table'].dropna()))\n",
        "        used_tables.update(set(relationships['From Table'].dropna()))\n",
        "        used_tables.update(set(relationships['To Table'].dropna()))\n",
        "        used_tables.update(set(measures['Table Name'].dropna()))\n",
        "        \n",
        "        used_tables = {t for t in used_tables if pd.notna(t)}\n",
        "        \n",
        "        print(f\" Found {len(used_tables)} used tables\")\n",
        "\n",
        "        # Analyze each table\n",
        "        for t in set(tables['name'].dropna()):\n",
        "            measures_count = len(measures[measures['Table Name'] == t])\n",
        "            rel_count = len(relationships[(relationships['From Table'] == t) | (relationships['To Table'] == t)])\n",
        "            dep_count = len(dependencies_df[dependencies_df['Referenced Table'] == t]) if dependencies_df is not None else 0\n",
        "            status = \"Unused\" if t not in used_tables else \"Used\"\n",
        "            \n",
        "            table_usage.append({\n",
        "                'workspace': ws_name,\n",
        "                'dataset': ds_name,\n",
        "                'table': t,\n",
        "                'measures': measures_count,\n",
        "                'relationships': rel_count,\n",
        "                'dependencies': dep_count,\n",
        "                \"usage\": status,\n",
        "                'workspace_id': ws_id,\n",
        "                'dataset_id': ds_id\n",
        "            })\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"  ‚ö†Ô∏è Error analyzing {ds_name}: {e}\")\n",
        "\n",
        "# Convert to DataFrame and display\n",
        "table_usage_df = pd.DataFrame(table_usage)\n",
        "display(table_usage_df)\n",
        "\n",
        "# üÜï Save Table Analysis to Lakehouse\n",
        "print(\"\\nüíæ Saving table analysis to lakehouse...\")\n",
        "save_to_lakehouse(table_usage_df, \"table_analysis\", \n",
        "                 \"Tables with usage context from measures, relationships, and dependencies\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "collapsed": false
      },
      "id": "table-analysis-cell"
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------\n",
        "# STEP 5: Enhanced Column Usage Analysis (From Version 1)\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "print(\"\\nüîç Enhanced Column Usage Analysis...\")\n",
        "\n",
        "columns_usage = []\n",
        "\n",
        "for _, ds in datasets_df.iterrows():\n",
        "    ds_id = ds['dataset_id']\n",
        "    ds_name = ds['dataset_name']\n",
        "    ws_id = ds['workspace_id']\n",
        "    ws_name = ds['workspace_name']\n",
        "\n",
        "    print(f\"\\nüîπ Dataset: {ds_name} (Workspace: {ws_name})\")    \n",
        "\n",
        "    try:\n",
        "        # Get model dependencies (reuse from previous step if available)\n",
        "        deps = fabric.get_model_calc_dependencies(dataset=ds_id, workspace=ws_id)\n",
        "        with deps as calc_deps:\n",
        "            dependencies_df = getattr(calc_deps, \"dependencies_df\", None)\n",
        "    \n",
        "        # Get relationships\n",
        "        relationships = fabric.list_relationships(dataset=ds_id, workspace=ws_id, extended=True)\n",
        "        relationships['qualified_from'] = \"'\" + relationships['From Table'] + \"'[\" + relationships['From Column'] + \"]\"\n",
        "        relationships['qualified_to'] = \"'\" + relationships['To Table'] + \"'[\" + relationships['To Column'] + \"]\"\n",
        "\n",
        "        # --- Get all columns in the dataset ---\n",
        "        all_columns = fabric.list_columns(dataset=ds_id, workspace=ws_id, extended=True)\n",
        "        all_columns = sanitize_df_columns(all_columns)\n",
        "        all_columns['workspace_id'] = ws_id\n",
        "        all_columns['dataset_id'] = ds_id\n",
        "        all_columns['workspace_name'] = ws_name\n",
        "        all_columns['dataset_name'] = ds_name\n",
        "        all_columns['qualified_name'] = \"'\" + all_columns['table_name'] + \"'[\" + all_columns['column_name'] + ']'\n",
        "\n",
        "        print(f\" Found {len(all_columns)} total columns\")\n",
        "\n",
        "        # --- Filtered dependencies to only Columns and Calc Columns ---\n",
        "        dep_columns_df = (\n",
        "            dependencies_df[\n",
        "                dependencies_df['Referenced Object Type'].isin(['Column', 'Calc Column'])\n",
        "            ]\n",
        "            if dependencies_df is not None else pd.DataFrame()\n",
        "        )\n",
        "\n",
        "        # --- Extract subsets by object type ---\n",
        "        if not dep_columns_df.empty:\n",
        "            measures_refs_df = dep_columns_df[dep_columns_df['Object Type'] == 'Measure']\n",
        "            relationship_refs_df = dep_columns_df[\n",
        "                dep_columns_df['Object Type'].str.contains('Relationship', case=False, na=False)\n",
        "            ]\n",
        "        else:\n",
        "            measures_refs_df = pd.DataFrame()\n",
        "            relationship_refs_df = pd.DataFrame()\n",
        "\n",
        "        # --- Used columns (in dependencies or relationships) ---\n",
        "        dep_columns = set(dep_columns_df['Referenced Full Object Name']) if not dep_columns_df.empty else set()\n",
        "        rel_columns = set(relationships['qualified_from']).union(set(relationships['qualified_to']))\n",
        "        used_columns = dep_columns.union(rel_columns)\n",
        "        used_columns = {c for c in used_columns if pd.notna(c)}\n",
        "\n",
        "        print(f\" Found {len(used_columns)} used columns\")\n",
        "\n",
        "        # --- Determine usage per column ---\n",
        "        for _, row in all_columns.iterrows():\n",
        "            table_name = row['table_name']\n",
        "            column_name = row['column_name']                           \n",
        "            qualified_name = row['qualified_name']\n",
        "            \n",
        "            if pd.isna(column_name):\n",
        "                continue\n",
        "\n",
        "            dep_count = len(dep_columns_df[\n",
        "                dep_columns_df['Referenced Full Object Name'] == qualified_name\n",
        "            ]) if not dep_columns_df.empty else 0\n",
        "\n",
        "            measure_c = len(measures_refs_df[measures_refs_df['Referenced Full Object Name'] == qualified_name])\n",
        "            relationship_c = len(relationship_refs_df[relationship_refs_df['Referenced Full Object Name'] == qualified_name])\n",
        "\n",
        "            # Build a referenced-by list (measures, relationships, etc.)\n",
        "            referenced_by = \", \".join(\n",
        "                dep_columns_df.loc[\n",
        "                    dep_columns_df['Referenced Full Object Name'] == qualified_name, 'Object Name'\n",
        "                ].unique().tolist()\n",
        "            ) if not dep_columns_df.empty else \"\"\n",
        "\n",
        "            # Determine usage\n",
        "            usage_status = 'Used' if any([measure_c, relationship_c, dep_count]) else 'Unused'\n",
        "            \n",
        "            # Append result\n",
        "            columns_usage.append({\n",
        "                'workspace': ws_name,\n",
        "                'dataset': ds_name,\n",
        "                'table': table_name,\n",
        "                'column': column_name,\n",
        "                'measures': measure_c,\n",
        "                'relationships': relationship_c,\n",
        "                'dependencies': dep_count,\n",
        "                'referenced_by': referenced_by,\n",
        "                'usage': usage_status,\n",
        "                'workspace_id': ws_id,\n",
        "                'dataset_id': ds_id\n",
        "            })\n",
        "            \n",
        "    except Exception as e:\n",
        "        print(f\"  ‚ö†Ô∏è Error analyzing columns for {ds_name}: {e}\")\n",
        "\n",
        "# Convert to DataFrame and display\n",
        "columns_usage_df = pd.DataFrame(columns_usage)\n",
        "display(columns_usage_df)\n",
        "\n",
        "# üÜï Save Column Usage Analysis to Lakehouse\n",
        "print(\"\\nüíæ Saving column usage analysis to lakehouse...\")\n",
        "save_to_lakehouse(columns_usage_df, \"column_usage_analysis\", \n",
        "                 \"Detailed column usage analysis with context from measures, relationships, and dependencies\")"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "collapsed": false
      },
      "id": "column-analysis-cell"
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------------------------------------------------\n",
        "# STEP 6: Final Summary and Lakehouse Overview\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"üéâ ENHANCED FABRIC WORKSPACE ANALYSIS COMPLETE\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Summary statistics\n",
        "print(f\"üìä Discovery Summary:\")\n",
        "print(f\"  Workspaces: {len(workspaces_df)}\")\n",
        "print(f\"  Datasets:   {len(datasets_df)}\")\n",
        "print(f\"  Reports:    {len(reports_df)}\")\n",
        "print(f\"  Dataflows:  {len(dataflows_df)}\")\n",
        "\n",
        "if table_usage:\n",
        "    table_usage_summary = pd.DataFrame(table_usage)\n",
        "    used_tables = len(table_usage_summary[table_usage_summary['usage'] == 'Used'])\n",
        "    unused_tables = len(table_usage_summary[table_usage_summary['usage'] == 'Unused'])\n",
        "    print(f\"  Tables:     {len(table_usage)} (Used: {used_tables}, Unused: {unused_tables})\")\n",
        "\n",
        "if columns_usage:\n",
        "    columns_usage_summary = pd.DataFrame(columns_usage)\n",
        "    used_columns = len(columns_usage_summary[columns_usage_summary['usage'] == 'Used'])\n",
        "    unused_columns = len(columns_usage_summary[columns_usage_summary['usage'] == 'Unused'])\n",
        "    print(f\"  Columns:    {len(columns_usage)} (Used: {used_columns}, Unused: {unused_columns})\")\n",
        "\n",
        "print(f\"\\nüíæ Lakehouse Tables Created:\")\n",
        "print(f\"  üìä workspace_analysis - Basic workspace information\")\n",
        "print(f\"  üìä dataset_analysis - Datasets with context (Reports, Tables, Relationships, Dataflows)\")\n",
        "print(f\"  üìä table_analysis - Tables with usage context from measures, relationships, dependencies\")\n",
        "print(f\"  üìä column_usage_analysis - Detailed column usage analysis\")\n",
        "print(f\"  üìä usage_summary - Summary of dataset usage patterns\")\n",
        "\n",
        "# Display final unused datasets\n",
        "if not unused_datasets_df.empty:\n",
        "    print(\"\\n‚ö†Ô∏è UNUSED DATASETS\")\n",
        "    for _, row in unused_datasets_df.iterrows():\n",
        "        print(f\" - {row['workspace_name']} ‚Üí {row['dataset_name']}\")\n",
        "else:\n",
        "    print(\"\\nüéâ No unused datasets found!\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"‚úÖ Analysis complete! Check your lakehouse for detailed results.\")\n",
        "print(\"=\"*80)"
      ],
      "outputs": [],
      "execution_count": null,
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        }
      },
      "id": "summary-final-cell"
    }
  ],
  "metadata": {
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "kernelspec": {
      "name": "synapse_pyspark",
      "display_name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "microsoft": {
      "language": "python",
      "language_group": "synapse_pyspark",
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "dependencies": {
      "lakehouse": {
        "known_lakehouses": [],
        "default_lakehouse": null,
        "default_lakehouse_name": null,
        "default_lakehouse_workspace_id": null
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}