{"cells":[{"cell_type":"markdown","source":["# Fabric Model Analysis & Optimization Toolkit\n","\n","## Features\n","- **Integrated Unused Objects Detection**: Automatically detects unused measures, columns, and tables after collecting workspace objects\n","- **Enhanced Visualization**: Comprehensive 4-panel visualization showing usage metrics for all object types\n","- **Lakehouse Storage**: Saves unused objects analysis results to dedicated lakehouse tables\n","- **Real-time Progress Tracking**: Enhanced progress tracker showing unused objects analysis status\n","\n","## Required Libraries\n","- `SemPy` which is part of the `semantic-link` feature with Core Fabric semantic model operations\n","\n","---"],"metadata":{},"id":"5cadac4c-498f-4893-8e17-81e451817b93"},{"cell_type":"markdown","source":["## Installation & Dependencies\n","\n","Install the required semantic-link-labs package for extended Fabric analytics capabilities."],"metadata":{},"id":"aceddf8c-8606-4538-a134-d50dee26d786"},{"cell_type":"code","source":["# Install semantic-link-labs for extended Fabric analytics\n","!pip install semantic-link-labs"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":3,"statement_ids":[3],"state":"finished","livy_statement_state":"available","session_id":"7030184d-2af6-454b-a1a9-716434176701","normalized_state":"finished","queued_time":"2025-10-13T21:53:04.3557707Z","session_start_time":"2025-10-13T21:53:04.3567058Z","execution_start_time":"2025-10-13T21:53:17.128618Z","execution_finish_time":"2025-10-13T21:53:44.003904Z","parent_msg_id":"6a07a3cc-3a27-40cf-9a97-8a924cf65577"},"text/plain":"StatementMeta(, 7030184d-2af6-454b-a1a9-716434176701, 3, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["Collecting semantic-link-labs\n  Downloading semantic_link_labs-0.12.3-py3-none-any.whl.metadata (27 kB)\nCollecting semantic-link-sempy>=0.12.0 (from semantic-link-labs)\n  Downloading semantic_link_sempy-0.12.1-py3-none-any.whl.metadata (11 kB)\nCollecting anytree (from semantic-link-labs)\n  Downloading anytree-2.13.0-py3-none-any.whl.metadata (8.0 kB)\nCollecting polib (from semantic-link-labs)\n  Downloading polib-1.2.0-py2.py3-none-any.whl.metadata (15 kB)\nCollecting jsonpath_ng (from semantic-link-labs)\n  Downloading jsonpath_ng-1.7.0-py3-none-any.whl.metadata (18 kB)\nRequirement already satisfied: clr-loader>=0.2.5 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from semantic-link-sempy>=0.12.0->semantic-link-labs) (0.2.5)\nCollecting fabric-analytics-sdk==0.0.1 (from fabric-analytics-sdk[online-notebook]==0.0.1->semantic-link-sempy>=0.12.0->semantic-link-labs)\n  Downloading fabric_analytics_sdk-0.0.1-py3-none-any.whl.metadata (14 kB)\nRequirement already satisfied: graphviz>=0.20.1 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from semantic-link-sempy>=0.12.0->semantic-link-labs) (0.20.1)\nCollecting azure-keyvault-secrets>=4.7.0 (from semantic-link-sempy>=0.12.0->semantic-link-labs)\n  Downloading azure_keyvault_secrets-4.10.0-py3-none-any.whl.metadata (18 kB)\nRequirement already satisfied: pyarrow>=12.0.1 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from semantic-link-sempy>=0.12.0->semantic-link-labs) (14.0.2)\nRequirement already satisfied: pythonnet>=3.0.1 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from semantic-link-sempy>=0.12.0->semantic-link-labs) (3.0.1)\nRequirement already satisfied: scikit-learn>=1.2.2 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from semantic-link-sempy>=0.12.0->semantic-link-labs) (1.2.2)\nRequirement already satisfied: setuptools>=68.2.2 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from semantic-link-sempy>=0.12.0->semantic-link-labs) (68.2.2)\nRequirement already satisfied: tqdm>=4.65.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from semantic-link-sempy>=0.12.0->semantic-link-labs) (4.65.0)\nRequirement already satisfied: rich>=13.3.5 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from semantic-link-sempy>=0.12.0->semantic-link-labs) (13.3.5)\nRequirement already satisfied: regex>=2023.8.8 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from semantic-link-sempy>=0.12.0->semantic-link-labs) (2023.10.3)\nRequirement already satisfied: pandas>=1.5.3 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from semantic-link-sempy>=0.12.0->semantic-link-labs) (2.1.4)\nRequirement already satisfied: pyspark>=3.4.1 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from semantic-link-sempy>=0.12.0->semantic-link-labs) (3.5.1.5.4.20240407)\nRequirement already satisfied: requests>=2.31.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from semantic-link-sempy>=0.12.0->semantic-link-labs) (2.31.0)\nRequirement already satisfied: aiohttp>=3.8.6 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from semantic-link-sempy>=0.12.0->semantic-link-labs) (3.9.3)\nRequirement already satisfied: IPython>=8.14.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from semantic-link-sempy>=0.12.0->semantic-link-labs) (8.20.0)\nRequirement already satisfied: tenacity>=8.2.3 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from semantic-link-sempy>=0.12.0->semantic-link-labs) (8.2.3)\nRequirement already satisfied: azure-core in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from fabric-analytics-sdk==0.0.1->fabric-analytics-sdk[online-notebook]==0.0.1->semantic-link-sempy>=0.12.0->semantic-link-labs) (1.30.2)\nRequirement already satisfied: azure-identity in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from fabric-analytics-sdk==0.0.1->fabric-analytics-sdk[online-notebook]==0.0.1->semantic-link-sempy>=0.12.0->semantic-link-labs) (1.15.0)\nCollecting fabric-analytics-notebook-plugin (from fabric-analytics-sdk[online-notebook]==0.0.1->semantic-link-sempy>=0.12.0->semantic-link-labs)\n  Downloading fabric_analytics_notebook_plugin-0.0.1-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: ply in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from jsonpath_ng->semantic-link-labs) (3.11)\nRequirement already satisfied: aiosignal>=1.1.2 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from aiohttp>=3.8.6->semantic-link-sempy>=0.12.0->semantic-link-labs) (1.2.0)\nRequirement already satisfied: attrs>=17.3.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from aiohttp>=3.8.6->semantic-link-sempy>=0.12.0->semantic-link-labs) (23.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from aiohttp>=3.8.6->semantic-link-sempy>=0.12.0->semantic-link-labs) (1.4.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from aiohttp>=3.8.6->semantic-link-sempy>=0.12.0->semantic-link-labs) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from aiohttp>=3.8.6->semantic-link-sempy>=0.12.0->semantic-link-labs) (1.9.3)\nRequirement already satisfied: isodate>=0.6.1 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from azure-keyvault-secrets>=4.7.0->semantic-link-sempy>=0.12.0->semantic-link-labs) (0.6.1)\nCollecting azure-core (from fabric-analytics-sdk==0.0.1->fabric-analytics-sdk[online-notebook]==0.0.1->semantic-link-sempy>=0.12.0->semantic-link-labs)\n  Downloading azure_core-1.35.1-py3-none-any.whl.metadata (46 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.5/46.5 kB\u001b[0m \u001b[31m749.8 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:--:--\u001b[0m\n\u001b[?25hRequirement already satisfied: typing-extensions>=4.6.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from azure-keyvault-secrets>=4.7.0->semantic-link-sempy>=0.12.0->semantic-link-labs) (4.9.0)\nRequirement already satisfied: cffi>=1.13 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from clr-loader>=0.2.5->semantic-link-sempy>=0.12.0->semantic-link-labs) (1.16.0)\nRequirement already satisfied: decorator in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from IPython>=8.14.0->semantic-link-sempy>=0.12.0->semantic-link-labs) (5.1.1)\nRequirement already satisfied: jedi>=0.16 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from IPython>=8.14.0->semantic-link-sempy>=0.12.0->semantic-link-labs) (0.18.1)\nRequirement already satisfied: matplotlib-inline in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from IPython>=8.14.0->semantic-link-sempy>=0.12.0->semantic-link-labs) (0.1.6)\nRequirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from IPython>=8.14.0->semantic-link-sempy>=0.12.0->semantic-link-labs) (3.0.43)\nRequirement already satisfied: pygments>=2.4.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from IPython>=8.14.0->semantic-link-sempy>=0.12.0->semantic-link-labs) (2.15.1)\nRequirement already satisfied: stack-data in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from IPython>=8.14.0->semantic-link-sempy>=0.12.0->semantic-link-labs) (0.2.0)\nRequirement already satisfied: traitlets>=5 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from IPython>=8.14.0->semantic-link-sempy>=0.12.0->semantic-link-labs) (5.7.1)\nRequirement already satisfied: pexpect>4.3 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from IPython>=8.14.0->semantic-link-sempy>=0.12.0->semantic-link-labs) (4.8.0)\nRequirement already satisfied: numpy<2,>=1.23.2 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from pandas>=1.5.3->semantic-link-sempy>=0.12.0->semantic-link-labs) (1.26.4)\nRequirement already satisfied: python-dateutil>=2.8.2 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from pandas>=1.5.3->semantic-link-sempy>=0.12.0->semantic-link-labs) (2.8.2)\nRequirement already satisfied: pytz>=2020.1 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from pandas>=1.5.3->semantic-link-sempy>=0.12.0->semantic-link-labs) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.1 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from pandas>=1.5.3->semantic-link-sempy>=0.12.0->semantic-link-labs) (2023.3)\nRequirement already satisfied: py4j==0.10.9.7 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from pyspark>=3.4.1->semantic-link-sempy>=0.12.0->semantic-link-labs) (0.10.9.7)\nRequirement already satisfied: charset-normalizer<4,>=2 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from requests>=2.31.0->semantic-link-sempy>=0.12.0->semantic-link-labs) (2.0.4)\nRequirement already satisfied: idna<4,>=2.5 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from requests>=2.31.0->semantic-link-sempy>=0.12.0->semantic-link-labs) (3.4)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from requests>=2.31.0->semantic-link-sempy>=0.12.0->semantic-link-labs) (2.1.0)\nRequirement already satisfied: certifi>=2017.4.17 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from requests>=2.31.0->semantic-link-sempy>=0.12.0->semantic-link-labs) (2024.2.2)\nRequirement already satisfied: markdown-it-py<3.0.0,>=2.2.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from rich>=13.3.5->semantic-link-sempy>=0.12.0->semantic-link-labs) (2.2.0)\nRequirement already satisfied: scipy>=1.3.2 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from scikit-learn>=1.2.2->semantic-link-sempy>=0.12.0->semantic-link-labs) (1.11.4)\nRequirement already satisfied: joblib>=1.1.1 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from scikit-learn>=1.2.2->semantic-link-sempy>=0.12.0->semantic-link-labs) (1.2.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from scikit-learn>=1.2.2->semantic-link-sempy>=0.12.0->semantic-link-labs) (2.2.0)\nRequirement already satisfied: six>=1.11.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from azure-core->fabric-analytics-sdk==0.0.1->fabric-analytics-sdk[online-notebook]==0.0.1->semantic-link-sempy>=0.12.0->semantic-link-labs) (1.16.0)\nRequirement already satisfied: pycparser in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from cffi>=1.13->clr-loader>=0.2.5->semantic-link-sempy>=0.12.0->semantic-link-labs) (2.21)\nRequirement already satisfied: parso<0.9.0,>=0.8.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from jedi>=0.16->IPython>=8.14.0->semantic-link-sempy>=0.12.0->semantic-link-labs) (0.8.3)\nRequirement already satisfied: mdurl~=0.1 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from markdown-it-py<3.0.0,>=2.2.0->rich>=13.3.5->semantic-link-sempy>=0.12.0->semantic-link-labs) (0.1.0)\nRequirement already satisfied: ptyprocess>=0.5 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from pexpect>4.3->IPython>=8.14.0->semantic-link-sempy>=0.12.0->semantic-link-labs) (0.7.0)\nRequirement already satisfied: wcwidth in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->IPython>=8.14.0->semantic-link-sempy>=0.12.0->semantic-link-labs) (0.2.5)\nRequirement already satisfied: cryptography>=2.5 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from azure-identity->fabric-analytics-sdk==0.0.1->fabric-analytics-sdk[online-notebook]==0.0.1->semantic-link-sempy>=0.12.0->semantic-link-labs) (42.0.2)\nRequirement already satisfied: msal<2.0.0,>=1.24.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from azure-identity->fabric-analytics-sdk==0.0.1->fabric-analytics-sdk[online-notebook]==0.0.1->semantic-link-sempy>=0.12.0->semantic-link-labs) (1.25.0)\nRequirement already satisfied: msal-extensions<2.0.0,>=0.3.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from azure-identity->fabric-analytics-sdk==0.0.1->fabric-analytics-sdk[online-notebook]==0.0.1->semantic-link-sempy>=0.12.0->semantic-link-labs) (1.0.0)\nRequirement already satisfied: psutil in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from fabric-analytics-notebook-plugin->fabric-analytics-sdk[online-notebook]==0.0.1->semantic-link-sempy>=0.12.0->semantic-link-labs) (5.9.0)\nRequirement already satisfied: executing in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from stack-data->IPython>=8.14.0->semantic-link-sempy>=0.12.0->semantic-link-labs) (0.8.3)\nRequirement already satisfied: asttokens in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from stack-data->IPython>=8.14.0->semantic-link-sempy>=0.12.0->semantic-link-labs) (2.0.5)\nRequirement already satisfied: pure-eval in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from stack-data->IPython>=8.14.0->semantic-link-sempy>=0.12.0->semantic-link-labs) (0.2.2)\nRequirement already satisfied: PyJWT<3,>=1.0.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from PyJWT[crypto]<3,>=1.0.0->msal<2.0.0,>=1.24.0->azure-identity->fabric-analytics-sdk==0.0.1->fabric-analytics-sdk[online-notebook]==0.0.1->semantic-link-sempy>=0.12.0->semantic-link-labs) (2.4.0)\nRequirement already satisfied: portalocker<3,>=1.0 in /home/trusted-service-user/cluster-env/trident_env/lib/python3.11/site-packages (from msal-extensions<2.0.0,>=0.3.0->azure-identity->fabric-analytics-sdk==0.0.1->fabric-analytics-sdk[online-notebook]==0.0.1->semantic-link-sempy>=0.12.0->semantic-link-labs) (2.3.0)\nDownloading semantic_link_labs-0.12.3-py3-none-any.whl (764 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m764.7/764.7 kB\u001b[0m \u001b[31m1.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n\u001b[?25hDownloading semantic_link_sempy-0.12.1-py3-none-any.whl (3.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0mm\n\u001b[?25hDownloading fabric_analytics_sdk-0.0.1-py3-none-any.whl (34 kB)\nDownloading anytree-2.13.0-py3-none-any.whl (45 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.1/45.1 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading jsonpath_ng-1.7.0-py3-none-any.whl (30 kB)\nDownloading polib-1.2.0-py2.py3-none-any.whl (20 kB)\nDownloading azure_keyvault_secrets-4.10.0-py3-none-any.whl (125 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.2/125.2 kB\u001b[0m \u001b[31m38.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading azure_core-1.35.1-py3-none-any.whl (211 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.8/211.8 kB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading fabric_analytics_notebook_plugin-0.0.1-py3-none-any.whl (20 kB)\nInstalling collected packages: polib, jsonpath_ng, anytree, azure-core, azure-keyvault-secrets, fabric-analytics-sdk, fabric-analytics-notebook-plugin, semantic-link-sempy, semantic-link-labs\n  Attempting uninstall: azure-core\n    Found existing installation: azure-core 1.30.2\n    Uninstalling azure-core-1.30.2:\n      Successfully uninstalled azure-core-1.30.2\n  Attempting uninstall: semantic-link-sempy\n    Found existing installation: semantic-link-sempy 0.11.0\n    Uninstalling semantic-link-sempy-0.11.0:\n      Successfully uninstalled semantic-link-sempy-0.11.0\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nfsspec-wrapper 0.1.15 requires PyJWT>=2.6.0, but you have pyjwt 2.4.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed anytree-2.13.0 azure-core-2024.9.1 azure-keyvault-secrets-4.10.0 fabric-analytics-notebook-plugin-0.0.1 fabric-analytics-sdk-0.0.1 jsonpath_ng-1.7.0 polib-1.2.0 semantic-link-labs-0.12.3 semantic-link-sempy-0.12.1\n"]}],"execution_count":1,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"},"jupyter":{"outputs_hidden":true}},"id":"631d6649-ebd3-4644-a9c7-1d6ef29757d0"},{"cell_type":"markdown","source":["## Library Imports & Configuration\n","\n","Import essential libraries for Fabric model analysis:\n","- **`sempy.fabric`**: Primary interface for Fabric operations\n","- **`sempy_labs.report`**: Report analysis and object extraction\n","- **`pandas`**: Data manipulation framework\n","- **`matplotlib.pyplot`**: Data visualization and charting"],"metadata":{},"id":"270ac315-54a5-46de-9035-31ce8a0756fb"},{"cell_type":"code","source":["# Import core libraries for enhanced Fabric model analysis\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import sempy.fabric as fabric\n","import sempy_labs\n","from sempy_labs.report import ReportWrapper\n","from pyspark.sql import SparkSession, functions as F\n","from pyspark.sql.types import ArrayType, StringType, StructType, LongType, StructField, FloatType\n","from pyspark.sql.functions import col\n","import re\n","from datetime import datetime\n","import time\n","import logging\n","from IPython.display import clear_output\n","from functools import wraps\n","import sys\n","import traceback"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":4,"statement_ids":[4],"state":"finished","livy_statement_state":"available","session_id":"7030184d-2af6-454b-a1a9-716434176701","normalized_state":"finished","queued_time":"2025-10-13T21:53:04.3584134Z","session_start_time":null,"execution_start_time":"2025-10-13T21:53:44.0063198Z","execution_finish_time":"2025-10-13T21:53:47.4819054Z","parent_msg_id":"078a7d58-7580-49f0-9f95-d5bae10dc5f2"},"text/plain":"StatementMeta(, 7030184d-2af6-454b-a1a9-716434176701, 4, Finished, Available, Finished)"},"metadata":{}}],"execution_count":2,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"1f4c6b01-9879-4662-bcac-66f2ccdf96e8"},{"cell_type":"code","source":["# Initialize Spark session and logging\n","spark = SparkSession.builder.getOrCreate()\n","logging.basicConfig(\n","    level=logging.WARNING,\n","    format='%(asctime)s - %(levelname)s - %(message)s',\n","    handlers=[\n","        logging.FileHandler('fabric_scanning.log', mode='w'),\n","        logging.StreamHandler(sys.stdout)\n","    ]\n",")\n","logger = logging.getLogger(__name__)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":5,"statement_ids":[5],"state":"finished","livy_statement_state":"available","session_id":"7030184d-2af6-454b-a1a9-716434176701","normalized_state":"finished","queued_time":"2025-10-13T21:53:04.360909Z","session_start_time":null,"execution_start_time":"2025-10-13T21:53:47.4839411Z","execution_finish_time":"2025-10-13T21:53:47.8342533Z","parent_msg_id":"79fa72e3-3f7c-4665-9552-d1577cc234b7"},"text/plain":"StatementMeta(, 7030184d-2af6-454b-a1a9-716434176701, 5, Finished, Available, Finished)"},"metadata":{}}],"execution_count":3,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"c1246ef0-69cd-42d1-b4f6-f0460a7992b9"},{"cell_type":"code","source":["dir(fabric)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":6,"statement_ids":[6],"state":"finished","livy_statement_state":"available","session_id":"7030184d-2af6-454b-a1a9-716434176701","normalized_state":"finished","queued_time":"2025-10-13T21:53:04.363246Z","session_start_time":null,"execution_start_time":"2025-10-13T21:53:47.8361753Z","execution_finish_time":"2025-10-13T21:53:48.1650047Z","parent_msg_id":"5f1d5b20-7206-45be-a027-2d101211037a"},"text/plain":"StatementMeta(, 7030184d-2af6-454b-a1a9-716434176701, 6, Finished, Available, Finished)"},"metadata":{}},{"output_type":"execute_result","execution_count":17,"data":{"text/plain":"['CognitiveServiceRestClient',\n 'DataCategory',\n 'FabricDataFrame',\n 'FabricRestClient',\n 'FabricSeries',\n 'MetadataKeys',\n 'ModelCalcDependencies',\n 'PowerBIRestClient',\n 'RefreshExecutionDetails',\n 'TOMWrapper',\n 'Trace',\n 'TraceConnection',\n '__all__',\n '__builtins__',\n '__cached__',\n '__doc__',\n '__file__',\n '__getattr__',\n '__loader__',\n '__name__',\n '__package__',\n '__path__',\n '_bpa',\n '_cache',\n '_client',\n '_credentials',\n '_datacategory',\n '_dataframe',\n '_environment',\n '_flat',\n '_flat_list_annotations',\n '_flat_list_apps',\n '_flat_list_calculation_items',\n '_flat_list_columns',\n '_flat_list_dataflows',\n '_flat_list_datasources',\n '_flat_list_gateways',\n '_flat_list_hierarchies',\n '_flat_list_partitions',\n '_flat_list_perspectives',\n '_flat_list_relationships',\n '_keyvault',\n '_metadatakeys',\n '_trace',\n '_trace_evaluate_dax',\n '_utils',\n '_vertipaq',\n 'connect_semantic_model',\n 'create_folder',\n 'create_lakehouse',\n 'create_notebook',\n 'create_tom_server',\n 'create_trace_connection',\n 'create_workspace',\n 'delete_folder',\n 'delete_item',\n 'evaluate_dax',\n 'evaluate_measure',\n 'exceptions',\n 'execute_tmsl',\n 'execute_xmla',\n 'get_artifact_id',\n 'get_lakehouse_id',\n 'get_model_calc_dependencies',\n 'get_notebook_workspace_id',\n 'get_refresh_execution_details',\n 'get_roles',\n 'get_row_level_security_permissions',\n 'get_tmsl',\n 'get_workspace_id',\n 'list_annotations',\n 'list_apps',\n 'list_calculation_items',\n 'list_capacities',\n 'list_columns',\n 'list_dataflow_storage_accounts',\n 'list_dataflows',\n 'list_datasets',\n 'list_datasources',\n 'list_expressions',\n 'list_folders',\n 'list_gateways',\n 'list_hierarchies',\n 'list_items',\n 'list_measures',\n 'list_partitions',\n 'list_perspectives',\n 'list_refresh_requests',\n 'list_relationship_violations',\n 'list_relationships',\n 'list_reports',\n 'list_tables',\n 'list_translations',\n 'list_workspaces',\n 'log_telemetry',\n 'model_memory_analyzer',\n 'move_folder',\n 'plot_relationships',\n 'read_parquet',\n 'read_table',\n 'refresh_dataset',\n 'refresh_tom_cache',\n 'rename_folder',\n 'resolve_dataset_id',\n 'resolve_dataset_name',\n 'resolve_dataset_name_and_id',\n 'resolve_folder_id',\n 'resolve_folder_path',\n 'resolve_item_id',\n 'resolve_item_name',\n 'resolve_workspace_id',\n 'resolve_workspace_name',\n 'resolve_workspace_name_and_id',\n 'run_model_bpa',\n 'run_notebook_job',\n 'set_service_principal',\n 'translate_semantic_model']"},"metadata":{}}],"execution_count":4,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"42a2106b-ade9-4880-b3df-0616c040f76d"},{"cell_type":"markdown","source":["## Progress Tracker\n"],"metadata":{},"id":"86694667-9fb0-440b-a48e-f16597db431d"},{"cell_type":"code","source":["class EnhancedTracker():\n","    def __init__(self):\n","        self.error_count = 0\n","        self.last_error = None\n","        self.stats = {\n","            'start_time': datetime.now(),\n","            'last_update': 0,\n","\n","            # ALL TOTAL COUNTERS\n","            'total_workspaces': 0,\n","            'total_datasets': 0,\n","            'total_reports': 0,\n","            'total_dataflows': 0,\n","            'total_measures': 0,\n","            'total_tables': 0,\n","            'total_columns': 0,\n","            'total_relationships': 0,\n","\n","            # TRACKING TOTAL COUNTERS\n","            # workspace\n","            'workspace_datasets': 0,\n","            'workspace_reports': 0,\n","            'workspace_dataflows': 0,\n","\n","            # dataset\n","            'dataset_tables': 0,\n","            'dataset_measures': 0,\n","            'dataset_relationships': 0,\n","            'dataset_columns': 0,\n","\n","            # PROGRESS COUNTERS\n","            'processed_workspaces': 0,\n","            'processed_datasets': 0,\n","            'processed_tables': 0,\n","\n","            'ws_processed_datasets': 0,\n","\n","            # STAGES\n","            'processing_stage': '',\n","            'current_operation': '',\n","            'current_object': {},\n","            'current_workspace': '',\n","            'current_dataset': '',\n","            'current_table': '',\n","\n","            # ERRORS\n","            'skipped_objects': {},\n","            'errors': [],\n","            'error_types': {},\n","\n","            # SUCCESS/FAILURE COUNTERS\n","            'success_count': 0,\n","            'failure_count': 0,\n","            'skipped_count': 0,\n","\n","            # UNUSED OBJECTS ANALYSIS\n","            'unused_measures': 0,\n","            'unused_columns': 0,\n","            'unused_tables': 0,\n","            'analysis_complete': False,\n","        }\n","        self.spinner_frames = ['⠋', '⠙', '⠹', '⠸', '⠼', '⠴', '⠦', '⠧', '⠇', '⠏']\n","        self.spinner_idx = 0\n","        self.last_update = 0\n","\n","    def log_error(self, message: str, error: Exception = None):\n","        self.error_count += 1\n","        error_details = {\n","            'timestamp': datetime.now().isoformat(),\n","            'message': message,\n","            'error_type': type(error).__name__ if error else 'Unknown',\n","            'error_details': str(error) if error else 'No details',\n","            'current_object': self.stats['current_object'].get('object_name'),\n","            'object_type': self.stats['current_object'].get('object_type')\n","        }\n","\n","        self.stats['errors'].append(error_details)\n","        logger.error(f\"{message}: {str(error) if error else 'No details'}\")\n","        self.last_error = error_details\n","\n","    def show_progress(self):\n","        clear_output(wait=True)\n","        runtime = datetime.now() - self.stats['start_time']\n","        workspaces_progress = self.stats['processed_workspaces']\n","        total_workspaces = self.stats['total_workspaces']\n","\n","        if total_workspaces > 0:\n","            ws_percent = workspaces_progress / total_workspaces\n","        else:\n","            ws_percent = 0\n","        \n","        ws_bar = '█' * int(ws_percent * 50) + '-' * (50 - int(ws_percent * 50))\n","\n","        # Update animation indices\n","        self.spinner_idx = (self.spinner_idx + 1) % len(self.spinner_frames)\n","\n","        status = [\n","            '╔════════════════ 🚀 Enhanced Scan Status ════════════════╗',\n","            f\"║Runtime: {str(runtime).split('.')[0]}\",\n","            f\"║Processing Stage: {self.stats['processing_stage']}\",\n","            f\"║Current Operation: {self.stats['current_operation']}\",\n","            '',\n","            f\"Scan Progress: {self.spinner_frames[self.spinner_idx]} 🔄\",\n","            f\"║Processing Workspace: {workspaces_progress}/{total_workspaces} {self.stats['current_workspace']}\",\n","            f\"╢{ws_bar}╟ {ws_percent*100:.0f}%\",\n","            '',\n","            \"📊 All Objects Summary\",\n","            f\"║Workspaces: {self.stats['total_workspaces']} | Reports: {self.stats['total_reports']} | Datasets: {self.stats['total_datasets']}\",\n","            f\"║Measures: {self.stats['total_measures']} | Tables: {self.stats['total_tables']} | Columns: {self.stats['total_columns']}\",\n","        ]\n","        \n","        # Add unused objects analysis if complete\n","        if self.stats['analysis_complete']:\n","            status.extend([\n","                \"\",\n","                \"🎯 Unused Objects Analysis Results\",\n","                f\"║Unused Measures: {self.stats['unused_measures']} | Unused Columns: {self.stats['unused_columns']} | Unused Tables: {self.stats['unused_tables']}\",\n","            ])\n","        \n","        sys.stdout.write('\\n'.join(status))\n","        sys.stdout.flush()\n","        self.last_update = time.time()\n","    \n","    def update(self, **kwargs):\n","        for key, value in kwargs.items():\n","            if key in self.stats:\n","                self.stats[key] = value\n","            else:\n","                print(f\"Warning: {key} is not a valid stats key.\")\n","        current_time = time.time()\n","        if current_time - self.last_update >= 1:  # Update display every second\n","            self.show_progress()\n","\n","tracker = EnhancedTracker()\n","\n","def track_function(func):\n","    @wraps(func)\n","    def wrapper(*args, **kwargs):\n","        tracker.stats['current_operation'] = func.__name__\n","        start_time = time.time()\n","        try:\n","            result = func(*args, **kwargs)\n","            return result\n","        except Exception as e:\n","            object_name = tracker.stats['current_object'].get('object_name', 'Unknown')\n","            tracker.log_error(f\"Error in {func.__name__} for {object_name}\", e)\n","            raise\n","        finally:\n","            duration = time.time() - start_time\n","            logger.debug(f\"{func.__name__} took {duration:.2f} seconds\")\n","            tracker.stats['current_operation'] = None\n","    return wrapper"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":30,"statement_ids":[30],"state":"finished","livy_statement_state":"available","session_id":"7030184d-2af6-454b-a1a9-716434176701","normalized_state":"finished","queued_time":"2025-10-13T22:35:42.8524418Z","session_start_time":null,"execution_start_time":"2025-10-13T22:35:42.8541936Z","execution_finish_time":"2025-10-13T22:35:43.1931301Z","parent_msg_id":"6ed57b96-03e0-4568-b33f-4a4a17c358f6"},"text/plain":"StatementMeta(, 7030184d-2af6-454b-a1a9-716434176701, 30, Finished, Available, Finished)"},"metadata":{}}],"execution_count":28,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d94e7451-9172-48e1-8bed-e28a929e34c5"},{"cell_type":"markdown","source":["## Utility Functions"],"metadata":{},"id":"de63c7aa-07bf-488f-9b03-d6e805e6200e"},{"cell_type":"code","source":["def sanitize_df_columns(df, extra_columns=False, ws_id=None, ds_id=None):\n","    \"\"\"\n","    Replaces spaces in column names with underscore to prevent errors during Spark Dataframe Creation\n","    \"\"\"\n","    df.columns = [\n","        re.sub(r'\\W+', \"_\", col.strip().lower())\n","        for col in df.columns\n","    ]\n","\n","    if extra_columns:\n","        df['workspace_id'] = ws_id\n","        df['dataset_id'] = ds_id\n","        \n","    return df"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":8,"statement_ids":[8],"state":"finished","livy_statement_state":"available","session_id":"7030184d-2af6-454b-a1a9-716434176701","normalized_state":"finished","queued_time":"2025-10-13T21:53:04.3680034Z","session_start_time":null,"execution_start_time":"2025-10-13T21:53:48.505799Z","execution_finish_time":"2025-10-13T21:53:48.8187052Z","parent_msg_id":"b5686dbf-95d0-4808-9e13-d6608b4497c8"},"text/plain":"StatementMeta(, 7030184d-2af6-454b-a1a9-716434176701, 8, Finished, Available, Finished)"},"metadata":{}}],"execution_count":6,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e910c739-a06c-499a-aacd-efc52a662611"},{"cell_type":"markdown","source":["## Data Collection Functions\n","\n","These functions collect workspace objects (datasets, reports, dataflows, measures, relationships, tables, columns) from Fabric."],"metadata":{},"id":"66ac5666-0b1e-4bf2-85b5-090a6456707c"},{"cell_type":"code","source":["@track_function\n","def get_datasets(ws_id):\n","    \"\"\"Gets all Datasets from the specified workspace\"\"\"\n","    datasets = fabric.list_datasets(workspace=ws_id)\n","    \n","    if not datasets.empty:\n","        datasets = sanitize_df_columns(datasets)\n","        datasets['workspace_id'] = ws_id\n","        tracker.stats['total_datasets'] += len(datasets)\n","        return datasets\n","    else:\n","        return []\n","\n","@track_function\n","def get_reports(ws_id):\n","    \"\"\"Gets all Reports from the specified workspace\"\"\"\n","    reports = fabric.list_reports(workspace=ws_id)\n","    \n","    if not reports.empty:\n","        reports = sanitize_df_columns(reports)\n","        reports.rename(columns={\n","            \"dataset_workspace_id\": \"workspace_id\",\n","            \"id\": \"report_id\"\n","        }, inplace=True)\n","        tracker.stats['total_reports'] += len(reports)\n","        return reports\n","    else:\n","        return []\n","\n","@track_function\n","def get_dataflows(ws_id):\n","    \"\"\"Gets all Dataflows from the specified workspace\"\"\"\n","    dataflows = fabric.list_items(type='Dataflow', workspace=ws_id)\n","    if not dataflows.empty:\n","        dataflows = sanitize_df_columns(dataflows)    \n","        tracker.stats['total_dataflows'] += len(dataflows)\n","        return dataflows\n","    else:\n","        return []\n","\n","@track_function\n","def get_tables(ws_id, ds_id):\n","    \"\"\"Gets all Tables from the specified dataset\"\"\"\n","    tables = fabric.list_tables(dataset=ds_id, workspace=ws_id, extended=True)\n","    if not tables.empty:\n","        tables['table_id'] = (\n","            ds_id + \"|\" + ws_id + \"n:\" + tables['Name'].astype(str)\n","        )\n","        tables = sanitize_df_columns(tables, True, ws_id, ds_id)\n","        tracker.stats['total_tables'] += len(tables) \n","        return tables\n","    else:\n","        return []\n","\n","@track_function\n","def get_measures(ws_id, ds_id):\n","    \"\"\"Gets all Measures from the specified dataset\"\"\"\n","    measures = fabric.list_measures(dataset=ds_id, workspace=ws_id)\n","    if not measures.empty:\n","        measures = sanitize_df_columns(measures, True, ws_id, ds_id)\n","        tracker.stats['total_measures'] += len(measures) \n","        return measures\n","    else:\n","        return []\n","\n","@track_function\n","def get_relationships(ws_id, ds_id):\n","    \"\"\"Gets all Relationships from the specified dataset\"\"\"\n","    relationships = fabric.list_relationships(dataset=ds_id, workspace=ws_id, extended=True)\n","    if not relationships.empty:\n","        relationships = sanitize_df_columns(relationships, True, ws_id, ds_id)\n","        tracker.stats['total_relationships'] += len(relationships) \n","        return relationships\n","    else:\n","        return []\n","\n","@track_function\n","def get_table_columns(ws_id, ds_id):\n","    \"\"\"Gets all Columns from the specified dataset\"\"\"\n","    dataset_cols = fabric.list_columns(dataset=ds_id, workspace=ws_id)\n","\n","    if not dataset_cols.empty:\n","        dataset_cols['table_id'] = (\n","            ds_id + \"|\" + ws_id + \"n:\" + dataset_cols['Table Name'].astype(str)\n","        )\n","        dataset_cols = sanitize_df_columns(dataset_cols, True, ws_id, ds_id)\n","        tracker.stats['total_columns'] += len(dataset_cols)\n","        return dataset_cols\n","    else:\n","        return []"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":9,"statement_ids":[9],"state":"finished","livy_statement_state":"available","session_id":"7030184d-2af6-454b-a1a9-716434176701","normalized_state":"finished","queued_time":"2025-10-13T21:53:04.3703463Z","session_start_time":null,"execution_start_time":"2025-10-13T21:53:48.8208344Z","execution_finish_time":"2025-10-13T21:53:49.2191732Z","parent_msg_id":"67998709-5205-462d-b432-18bb2dd045e2"},"text/plain":"StatementMeta(, 7030184d-2af6-454b-a1a9-716434176701, 9, Finished, Available, Finished)"},"metadata":{}}],"execution_count":7,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"5074eaa2-eb94-47aa-99ab-35a758f15884"},{"cell_type":"markdown","source":["## Unused Objects Detection Functions\n"],"metadata":{},"id":"739aa702-43db-4212-9812-818172caa2dd"},{"cell_type":"code","source":["@track_function\n","def find_model_depenencies(dataset, workspace):\n","    \"\"\"Gets Model Dependencies\"\"\"\n","    try:\n","        dependencies_df = fabric.get_model_calc_dependencies(\n","            dataset=dataset,\n","            workspace=workspace\n","        )\n","        \n","        with dependencies_df as calc_deps:\n","            deps_df = getattr(calc_deps, \"dependencies_df\", None)\n","        \n","        return deps_df\n","    except Exception as e:\n","        tracker.log_error(\"Could not analyze dependencies\", e)\n","        return None\n","\n","\n","@track_function\n","def find_unused_measures(workspace, dataset, all_measures_df, deps_df):\n","    \"\"\"\n","    Identifies measures that are not used in reports or referenced by other objects.\n","    \n","    Args:\n","        workspace (str): The workspace ID\n","        dataset (str): The dataset ID  \n","        all_measures_df (pd.DataFrame): DataFrame containing all measures\n","    \n","    Returns:\n","        tuple: (unused_measures_set, metrics_dict)\n","    \"\"\"\n","    \n","    tracker.update(current_operation = \"📋 Analyzing unused measures...\")\n","    try:\n","        if all_measures_df is None or len(all_measures_df) == 0:\n","            return set(), {'total_measures': 0, 'unused_measures': 0, 'used_measures': 0, 'utilization_rate': 0}\n","            \n","        # all_measures = set(all_measures_df['measure_name'].unique())\n","\n","        # Get model dependencies\n","        tracker.update(current_operation =\"  🔗 Analyzing measure dependencies...\")\n","        referenced_measures = set()\n","\n","        if deps_df is not None and not deps_df.empty:\n","            referenced_measures = set(\n","                deps_df[deps_df['Referenced Object Type'] == 'Measure']['Referenced Object'].unique()\n","            )\n","\n","        # Calculate used measures (can be enhanced with report analysis)\n","        # report_measures = set()  # Placeholder for future report analysis enhancement\n","        used_measures = referenced_measures\n","        # Return unused measures\n","        unused_measures = all_measures_df.difference(used_measures)\n","        \n","        return unused_measures, {\n","            'total_measures': len(all_measures_df),\n","            'unused_measures': len(unused_measures),\n","            'used_measures': len(used_measures),\n","            'utilization_rate': (len(used_measures) / len(all_measures)) * 100 if len(all_measures) > 0 else 0\n","        }\n","        \n","    except Exception as e:\n","        tracker.log_error(f\"Error in find_unused_measures for {dataset}\", e)\n","        return set(), {'total_measures': 0, 'unused_measures': 0, 'used_measures': 0, 'utilization_rate': 0}\n","\n","@track_function\n","def find_unused_columns(workspace, dataset, all_columns_df, deps_df):\n","    \"\"\"\n","    Identifies columns that are not used in reports or referenced by other objects.\n","    \n","    Args:\n","        workspace (str): The workspace ID\n","        dataset (str): The dataset ID\n","        all_columns_df (pd.DataFrame): DataFrame containing all columns\n","    \n","    Returns:\n","        tuple: (unused_columns_set, metrics_dict)\n","    \"\"\"\n","    \n","    tracker.update(current_operation =\"  📋 Analyzing unused columns...\")\n","    try:\n","        if all_columns_df is None or len(all_columns_df) == 0:\n","            return set(), {'total_columns': 0, 'unused_columns': 0, 'used_columns': 0, 'utilization_rate': 0}\n","            \n","        all_columns = set(all_columns_df['column_name'])\n","\n","        # Get model dependencies\n","        tracker.update(current_operation =\"  🔗 Analyzing column dependencies...\")\n","        referenced_columns = set()\n","\n","        if deps_df is not None and not deps_df.empty:\n","            col_object_types = [\"Column\", \"Calc Column\"]\n","            referenced_columns = set(\n","                deps_df[deps_df['Referenced Object Type'].isin(col_object_types)]['Referenced Object'].unique()\n","            )\n","            \n","        # Calculate used columns (can be enhanced with report analysis)\n","        # report_columns = set()  # Placeholder for future report analysis enhancement\n","        used_columns = referenced_columns\n","\n","        # Return unused columns\n","        unused_columns = all_columns.difference(used_columns)\n","        \n","        return unused_columns, {\n","            'total_columns': len(all_columns),\n","            'unused_columns': len(unused_columns),\n","            'used_columns': len(used_columns),\n","            'utilization_rate': (len(used_columns) / len(all_columns)) * 100 if len(all_columns) > 0 else 0\n","        }\n","        \n","    except Exception as e:\n","        tracker.log_error(f\"Error in find_unused_columns for {dataset}\", e)\n","        return set(), {'total_columns': 0, 'unused_columns': 0, 'used_columns': 0, 'utilization_rate': 0}\n","\n","@track_function  \n","def find_unused_tables(workspace, dataset, all_tables_df, all_columns_df, deps_df):\n","    \"\"\"\n","    Identifies tables that are not used in reports or referenced by other objects.\n","    \n","    Args:\n","        workspace (str): The workspace ID\n","        dataset (str): The dataset ID\n","        all_tables_df (pd.DataFrame): DataFrame containing all tables\n","        all_columns_df (pd.DataFrame): DataFrame containing all columns\n","    \n","    Returns:\n","        tuple: (unused_tables_set, metrics_dict)\n","    \"\"\"\n","    \n","    tracker.update(current_operation =\"  📋 Analyzing unused tables...\")\n","    try:\n","        if all_tables_df is None or len(all_tables_df) == 0:\n","            return set(), {'total_tables': 0, 'unused_tables': 0, 'used_tables': 0, 'utilization_rate': 0}\n","            \n","        all_tables = set(all_tables_df['name'])\n","\n","        # Get model dependencies\n","        tracker.update(current_operation =\"  🔗 Analyzing table dependencies...\")\n","        referenced_tables = set()\n","        \n","        if deps_df is not None and not deps_df.empty:\n","            referenced_tables = set(\n","                deps_df['Referenced Table'].dropna()\n","            )\n","\n","        # Calculate used tables (can be enhanced with report analysis)\n","        used_tables = referenced_tables\n","        # Return unused tables\n","        unused_tables = all_tables.difference(used_tables)\n","        \n","        return unused_tables, {\n","            'total_tables': len(all_tables),\n","            'unused_tables': len(unused_tables), \n","            'used_tables': len(used_tables),\n","            'utilization_rate': (len(used_tables) / len(all_tables)) * 100 if len(all_tables) > 0 else 0\n","        }\n","        \n","    except Exception as e:\n","        tracker.log_error(f\"Error in find_unused_tables for {dataset}\", e)\n","        return set(), {'total_tables': 0, 'unused_tables': 0, 'used_tables': 0, 'utilization_rate': 0}"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":29,"statement_ids":[29],"state":"finished","livy_statement_state":"available","session_id":"7030184d-2af6-454b-a1a9-716434176701","normalized_state":"finished","queued_time":"2025-10-13T22:35:35.3226Z","session_start_time":null,"execution_start_time":"2025-10-13T22:35:35.3241177Z","execution_finish_time":"2025-10-13T22:35:35.6476618Z","parent_msg_id":"bf36bf4c-9437-4d85-b228-9e335d4a22d8"},"text/plain":"StatementMeta(, 7030184d-2af6-454b-a1a9-716434176701, 29, Finished, Available, Finished)"},"metadata":{}}],"execution_count":27,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"02255033-6914-4257-899b-3d6753b00f0e"},{"cell_type":"markdown","source":["## 📊 Enhanced Visualization Functions\n"],"metadata":{},"id":"962db2f9-f303-4d3e-bdd0-8c13592dc5d7"},{"cell_type":"code","source":["def create_unused_objects_visualization(measures_metrics, columns_metrics, tables_metrics):\n","    \"\"\"\n","    Creates a comprehensive visualization showing unused objects analysis\n","    \"\"\"\n","    tracker.update(current_operation =\"\\n📊 Creating unused objects visualization...\")\n","    \n","    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n","    fig.suptitle('🎯 Fabric Model - Unused Objects Analysis', fontsize=16, fontweight='bold')\n","    \n","    # Measures Chart (Top Left)\n","    if measures_metrics['total_measures'] > 0:\n","        measures_labels = ['Used', 'Unused']\n","        measures_values = [measures_metrics['used_measures'], measures_metrics['unused_measures']]\n","        measures_colors = ['#28a745', '#dc3545']\n","        \n","        bars1 = axes[0,0].bar(measures_labels, measures_values, color=measures_colors, alpha=0.8)\n","        axes[0,0].set_title(f'📏 Measures Analysis\\nTotal: {measures_metrics[\"total_measures\"]} | Utilization: {measures_metrics[\"utilization_rate\"]:.1f}%')\n","        axes[0,0].set_ylabel('Number of Measures')\n","        \n","        # Add value labels\n","        for i, bar in enumerate(bars1):\n","            height = bar.get_height()\n","            percentage = (measures_values[i] / measures_metrics['total_measures']) * 100\n","            axes[0,0].annotate(f'{int(height)}\\n({percentage:.1f}%)',\n","                             xy=(bar.get_x() + bar.get_width()/2, height),\n","                             xytext=(0, 5), textcoords=\"offset points\",\n","                             ha='center', va='bottom', fontweight='bold')\n","    else:\n","        axes[0,0].text(0.5, 0.5, 'No Measures Found', ha='center', va='center', transform=axes[0,0].transAxes)\n","        axes[0,0].set_title('📏 Measures Analysis')\n","    \n","    # Columns Chart (Top Right)\n","    if columns_metrics['total_columns'] > 0:\n","        columns_labels = ['Used', 'Unused']\n","        columns_values = [columns_metrics['used_columns'], columns_metrics['unused_columns']]\n","        columns_colors = ['#17a2b8', '#fd7e14']\n","        \n","        bars2 = axes[0,1].bar(columns_labels, columns_values, color=columns_colors, alpha=0.8)\n","        axes[0,1].set_title(f'📊 Columns Analysis\\nTotal: {columns_metrics[\"total_columns\"]} | Utilization: {columns_metrics[\"utilization_rate\"]:.1f}%')\n","        axes[0,1].set_ylabel('Number of Columns')\n","        \n","        # Add value labels\n","        for i, bar in enumerate(bars2):\n","            height = bar.get_height()\n","            percentage = (columns_values[i] / columns_metrics['total_columns']) * 100\n","            axes[0,1].annotate(f'{int(height)}\\n({percentage:.1f}%)',\n","                             xy=(bar.get_x() + bar.get_width()/2, height),\n","                             xytext=(0, 5), textcoords=\"offset points\",\n","                             ha='center', va='bottom', fontweight='bold')\n","    else:\n","        axes[0,1].text(0.5, 0.5, 'No Columns Found', ha='center', va='center', transform=axes[0,1].transAxes)\n","        axes[0,1].set_title('📊 Columns Analysis')\n","    \n","    # Tables Chart (Bottom Left)\n","    if tables_metrics['total_tables'] > 0:\n","        tables_labels = ['Used', 'Unused']\n","        tables_values = [tables_metrics['used_tables'], tables_metrics['unused_tables']]\n","        tables_colors = ['#6f42c1', '#e83e8c']\n","        \n","        bars3 = axes[1,0].bar(tables_labels, tables_values, color=tables_colors, alpha=0.8)\n","        axes[1,0].set_title(f'🗃️ Tables Analysis\\nTotal: {tables_metrics[\"total_tables\"]} | Utilization: {tables_metrics[\"utilization_rate\"]:.1f}%')\n","        axes[1,0].set_ylabel('Number of Tables')\n","        \n","        # Add value labels\n","        for i, bar in enumerate(bars3):\n","            height = bar.get_height()\n","            percentage = (tables_values[i] / tables_metrics['total_tables']) * 100\n","            axes[1,0].annotate(f'{int(height)}\\n({percentage:.1f}%)',\n","                             xy=(bar.get_x() + bar.get_width()/2, height),\n","                             xytext=(0, 5), textcoords=\"offset points\",\n","                             ha='center', va='bottom', fontweight='bold')\n","    else:\n","        axes[1,0].text(0.5, 0.5, 'No Tables Found', ha='center', va='center', transform=axes[1,0].transAxes)\n","        axes[1,0].set_title('🗃️ Tables Analysis')\n","    \n","    # Summary Chart (Bottom Right)\n","    total_objects = measures_metrics['total_measures'] + columns_metrics['total_columns'] + tables_metrics['total_tables']\n","    total_unused = measures_metrics['unused_measures'] + columns_metrics['unused_columns'] + tables_metrics['unused_tables']\n","    \n","    if total_objects > 0:\n","        summary_labels = ['Measures', 'Columns', 'Tables']\n","        unused_counts = [measures_metrics['unused_measures'], columns_metrics['unused_columns'], tables_metrics['unused_tables']]\n","        summary_colors = ['#dc3545', '#fd7e14', '#e83e8c']\n","        \n","        bars4 = axes[1,1].bar(summary_labels, unused_counts, color=summary_colors, alpha=0.8)\n","        axes[1,1].set_title(f'📈 Unused Objects Summary\\nTotal Unused: {total_unused}/{total_objects} ({(total_unused/total_objects)*100:.1f}%)')\n","        axes[1,1].set_ylabel('Number of Unused Objects')\n","        \n","        # Add value labels\n","        for i, bar in enumerate(bars4):\n","            height = bar.get_height()\n","            axes[1,1].annotate(f'{int(height)}',\n","                             xy=(bar.get_x() + bar.get_width()/2, height),\n","                             xytext=(0, 5), textcoords=\"offset points\",\n","                             ha='center', va='bottom', fontweight='bold')\n","    else:\n","        axes[1,1].text(0.5, 0.5, 'No Objects Found', ha='center', va='center', transform=axes[1,1].transAxes)\n","        axes[1,1].set_title('📈 Unused Objects Summary')\n","    \n","    plt.tight_layout()\n","    plt.show()"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":11,"statement_ids":[11],"state":"finished","livy_statement_state":"available","session_id":"7030184d-2af6-454b-a1a9-716434176701","normalized_state":"finished","queued_time":"2025-10-13T21:53:04.3997851Z","session_start_time":null,"execution_start_time":"2025-10-13T21:53:49.6096013Z","execution_finish_time":"2025-10-13T21:53:49.9553886Z","parent_msg_id":"0aec08e9-30d0-4889-b0fe-b924aa698f7b"},"text/plain":"StatementMeta(, 7030184d-2af6-454b-a1a9-716434176701, 11, Finished, Available, Finished)"},"metadata":{}}],"execution_count":9,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"bc71a831-cbb8-4dff-987b-1306f3530d3b"},{"cell_type":"code","source":["set([1,1,2,2,3,4,56,7,32,23])"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":28,"statement_ids":[28],"state":"finished","livy_statement_state":"available","session_id":"7030184d-2af6-454b-a1a9-716434176701","normalized_state":"finished","queued_time":"2025-10-13T22:33:27.4393186Z","session_start_time":null,"execution_start_time":"2025-10-13T22:33:27.4404873Z","execution_finish_time":"2025-10-13T22:33:27.7357972Z","parent_msg_id":"1d4af478-38dd-452d-b6ff-373b554564bd"},"text/plain":"StatementMeta(, 7030184d-2af6-454b-a1a9-716434176701, 28, Finished, Available, Finished)"},"metadata":{}},{"output_type":"execute_result","execution_count":83,"data":{"text/plain":"{1, 2, 3, 4, 7, 23, 32, 56}"},"metadata":{}}],"execution_count":26,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"98438ca4-fbfd-4060-a3de-e4091457148a"},{"cell_type":"markdown","source":["## Workspace Objects Collection\n"],"metadata":{},"id":"50f8e31a-65e6-4120-b8d0-044537d1b24e"},{"cell_type":"code","source":["def get_workspace_objects(workspaces):\n","    \"\"\"\n","    Enhanced version of the original get_workspace_objects function\n","    Gets workspace level objects like Semantic Models/Datasets, Reports, and Dataflows\n","    Now also captures dataset info for unused objects analysis\n","    \"\"\"\n","\n","    dataset_list = []\n","    report_list = []\n","    dataflows_list = []\n","    tables_list = []\n","    measures_list = []\n","    relationships_list = []\n","    dataset_cols_list = []\n","    \n","    # 🆕 Store all datasets info for unused object analysis\n","    all_datasets_info = []\n","    \n","    for _, ws in workspaces.iterrows():\n","        ws_name = ws['name']\n","        ws_id = ws['id']\n","        ws_type = ws['type']\n","                            \n","        tracker.stats['processed_workspaces'] += 1\n","        if ws_type == 'AdminInsights':\n","            continue\n","\n","        tracker.update(\n","            current_workspace=ws_name,\n","            current_operation='Scanning for Reports...',\n","            current_object={\n","                'object_type': 'Workspace',\n","                'object_name': ws_name\n","            },\n","            processed_datasets=0,\n","            workspace_datasets=0\n","        )\n","        \n","        try:\n","            reports = get_reports(ws_id)\n","            if len(reports) > 0:\n","                report_list.append(reports)\n","                tracker.update(\n","                    workspace_reports=len(reports),\n","                    current_operation='Scanning for Dataflows...',\n","                    dataset_tables=0,\n","                    dataset_measures=0,\n","                    dataset_relationships=0\n","                )\n","\n","            dataflows = get_dataflows(ws_id)\n","            if len(dataflows) > 0:\n","                dataflows_list.append(dataflows)\n","                tracker.update(\n","                    workspace_dataflows=len(dataflows),\n","                    current_operation='Scanning for Datasets...'\n","                )\n","\n","            datasets = get_datasets(ws_id)\n","            if len(datasets) > 0:\n","                dataset_list.append(datasets)\n","                tracker.update(\n","                    workspace_datasets=len(datasets),\n","                    current_operation='Scanning For Measures...'\n","                )\n","            \n","            for ds_name, ds_id in zip(datasets['dataset_name'], datasets['dataset_id']):\n","                tracker.stats['processed_datasets'] += 1\n","                tracker.update(\n","                    current_object={\n","                        'object_name': ds_name,\n","                        'object_type': 'Dataset'\n","                    },\n","                    dataset_tables=0,\n","                    dataset_measures=0,\n","                    dataset_relationships=0,\n","                    current_dataset=ds_name\n","                )\n","\n","                measures = get_measures(ws_id, ds_id)\n","                if len(measures) > 0:\n","                    measures_list.append(measures)\n","                    tracker.update(\n","                        dataset_measures=len(measures),\n","                        current_operation='Scanning for Relationships...'\n","                    )\n","                \n","                relationships = get_relationships(ws_id, ds_id)\n","                if len(relationships) > 0:\n","                    relationships_list.append(relationships)\n","                    tracker.update(\n","                        dataset_relationships=len(relationships),\n","                        current_operation='Scanning for Tables...'\n","                    )\n","\n","                tables = get_tables(ws_id, ds_id)\n","                if len(tables) > 0:\n","                    tables_list.append(tables)\n","                    tracker.update(\n","                        dataset_tables=len(tables),\n","                        current_operation='Getting Table Columns...'\n","                    )\n","                \n","                dataset_cols = get_table_columns(ws_id, ds_id)\n","                if len(dataset_cols) > 0:\n","                    dataset_cols_list.append(dataset_cols)\n","                    tracker.update(\n","                        dataset_columns=len(dataset_cols),\n","                        current_operation='Saving data...'\n","                    )\n","                \n","                # 🆕 Store dataset info for unused object analysis\n","                all_datasets_info.append({\n","                    'workspace_id': ws_id,\n","                    'dataset_id': ds_id,\n","                    'dataset_name': ds_name,\n","                    'measures': measures,\n","                    'tables': tables,\n","                    'columns': dataset_cols\n","                })\n","                \n","        except Exception as e:\n","            tb = traceback.format_exc().splitlines()\n","            tracker.log_error(f\"Error while finding objects.\", e)\n","            raise\n","\n","    # Combine all data\n","    all_datasets = pd.concat(dataset_list, ignore_index=True) if dataset_list else pd.DataFrame()\n","    all_reports = pd.concat(report_list, ignore_index=True) if report_list else pd.DataFrame()\n","    all_dataflows = pd.concat(dataflows_list, ignore_index=True) if dataflows_list else pd.DataFrame()\n","    all_measures = pd.concat(measures_list, ignore_index=True) if measures_list else pd.DataFrame()\n","    all_relationships = pd.concat(relationships_list, ignore_index=True) if relationships_list else pd.DataFrame()\n","    all_tables = pd.concat(tables_list, ignore_index=True) if tables_list else pd.DataFrame()\n","    all_columns = pd.concat(dataset_cols_list, ignore_index=True) if dataset_cols_list else pd.DataFrame()\n","    \n","    # 🆕 Return datasets info for unused objects analysis\n","    return all_datasets, all_reports, all_dataflows, all_measures, all_relationships, all_tables, all_columns, all_datasets_info"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":12,"statement_ids":[12],"state":"finished","livy_statement_state":"available","session_id":"7030184d-2af6-454b-a1a9-716434176701","normalized_state":"finished","queued_time":"2025-10-13T21:53:04.4362865Z","session_start_time":null,"execution_start_time":"2025-10-13T21:53:49.9577646Z","execution_finish_time":"2025-10-13T21:53:50.8557703Z","parent_msg_id":"1d7d8f44-df4b-4826-901e-07d1662fd6f2"},"text/plain":"StatementMeta(, 7030184d-2af6-454b-a1a9-716434176701, 12, Finished, Available, Finished)"},"metadata":{}}],"execution_count":10,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"59701d46-ca61-44c9-9883-e18e5f4c06d8"},{"cell_type":"markdown","source":["## Cross-Dataset Unused Objects Analysis"],"metadata":{},"id":"db0ce02d-5439-4101-b98b-6da8a60aabd1"},{"cell_type":"code","source":["def analyze_unused_objects_across_datasets(all_datasets_info):\n","    \"\"\"\n","    Analyzes unused objects across all datasets\n","    \"\"\"\n","    tracker.update(current_operation =\"🎯 UNUSED OBJECTS ANALYSIS\")\n","    \n","    total_unused_measures = 0\n","    total_unused_columns = 0  \n","    total_unused_tables = 0\n","    \n","    all_unused_measures = []\n","    all_unused_columns = []\n","    all_unused_tables = []\n","    \n","    overall_measures_metrics = {'total_measures': 0, 'unused_measures': 0, 'used_measures': 0}\n","    overall_columns_metrics = {'total_columns': 0, 'unused_columns': 0, 'used_columns': 0}\n","    overall_tables_metrics = {'total_tables': 0, 'unused_tables': 0, 'used_tables': 0}\n","    \n","    for dataset_info in all_datasets_info:\n","        ws_id = dataset_info['workspace_id']\n","        ds_id = dataset_info['dataset_id']\n","        ds_name = dataset_info['dataset_name']\n","        \n","        tracker.update(current_operation =f\"\\n🔍 Analyzing dataset: {ds_name}\")\n","        \n","        model_dependencies = find_model_depenencies(ds_id,ws_id)\n","        # Analyze unused measures\n","        unused_measures, measures_metrics = find_unused_measures(\n","            ws_id, ds_id, dataset_info['measures'],model_dependencies\n","        )\n","        if unused_measures:\n","            all_unused_measures.extend([(ds_name, measure) for measure in unused_measures])\n","            total_unused_measures += len(unused_measures)\n","        \n","        # Analyze unused columns\n","        unused_columns, columns_metrics = find_unused_columns(\n","            ws_id, ds_id, dataset_info['columns'], model_dependencies\n","        )\n","        if unused_columns:\n","            all_unused_columns.extend([(ds_name, column) for column in unused_columns])\n","            total_unused_columns += len(unused_columns)\n","            \n","        # Analyze unused tables\n","        unused_tables, tables_metrics = find_unused_tables(\n","            ws_id, ds_id, dataset_info['tables'], dataset_info['columns'], model_dependencies\n","        )\n","        if unused_tables:\n","            all_unused_tables.extend([(ds_name, table) for table in unused_tables])\n","            total_unused_tables += len(unused_tables)\n","        \n","        # Aggregate metrics\n","        overall_measures_metrics['total_measures'] += measures_metrics['total_measures']\n","        overall_measures_metrics['unused_measures'] += measures_metrics['unused_measures']\n","        overall_measures_metrics['used_measures'] += measures_metrics['used_measures']\n","        \n","        overall_columns_metrics['total_columns'] += columns_metrics['total_columns']\n","        overall_columns_metrics['unused_columns'] += columns_metrics['unused_columns']\n","        overall_columns_metrics['used_columns'] += columns_metrics['used_columns']\n","        \n","        overall_tables_metrics['total_tables'] += tables_metrics['total_tables']\n","        overall_tables_metrics['unused_tables'] += tables_metrics['unused_tables']\n","        overall_tables_metrics['used_tables'] += tables_metrics['used_tables']\n","    \n","    # Calculate utilization rates\n","    overall_measures_metrics['utilization_rate'] = (\n","        (overall_measures_metrics['used_measures'] / overall_measures_metrics['total_measures']) * 100 \n","        if overall_measures_metrics['total_measures'] > 0 else 0\n","    )\n","    overall_columns_metrics['utilization_rate'] = (\n","        (overall_columns_metrics['used_columns'] / overall_columns_metrics['total_columns']) * 100 \n","        if overall_columns_metrics['total_columns'] > 0 else 0\n","    )\n","    overall_tables_metrics['utilization_rate'] = (\n","        (overall_tables_metrics['used_tables'] / overall_tables_metrics['total_tables']) * 100 \n","        if overall_tables_metrics['total_tables'] > 0 else 0\n","    )\n","    \n","    # Update tracker stats\n","    tracker.update(\n","        unused_measures=total_unused_measures,\n","        unused_columns=total_unused_columns,\n","        unused_tables=total_unused_tables,\n","        analysis_complete=True\n","    )\n","    \n","    # # Print summary\n","    # print(f\"\\n📊 OVERALL UNUSED OBJECTS SUMMARY\")\n","    # print(\"=\"*60)\n","    # print(f\"📏 Measures: {overall_measures_metrics['unused_measures']}/{overall_measures_metrics['total_measures']} unused ({100-overall_measures_metrics['utilization_rate']:.1f}%)\")\n","    # print(f\"📊 Columns:  {overall_columns_metrics['unused_columns']}/{overall_columns_metrics['total_columns']} unused ({100-overall_columns_metrics['utilization_rate']:.1f}%)\")\n","    # print(f\"🗃️  Tables:   {overall_tables_metrics['unused_tables']}/{overall_tables_metrics['total_tables']} unused ({100-overall_tables_metrics['utilization_rate']:.1f}%)\")\n","    \n","    # Create visualization\n","    create_unused_objects_visualization(overall_measures_metrics, overall_columns_metrics, overall_tables_metrics)\n","    \n","    return {\n","        'unused_measures': all_unused_measures,\n","        'unused_columns': all_unused_columns, \n","        'unused_tables': all_unused_tables,\n","        'measures_metrics': overall_measures_metrics,\n","        'columns_metrics': overall_columns_metrics,\n","        'tables_metrics': overall_tables_metrics\n","    }"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":13,"statement_ids":[13],"state":"finished","livy_statement_state":"available","session_id":"7030184d-2af6-454b-a1a9-716434176701","normalized_state":"finished","queued_time":"2025-10-13T21:53:04.4387273Z","session_start_time":null,"execution_start_time":"2025-10-13T21:53:50.8582493Z","execution_finish_time":"2025-10-13T21:53:51.2425812Z","parent_msg_id":"50f005cb-6c7e-4877-9e09-cac8e11f7b91"},"text/plain":"StatementMeta(, 7030184d-2af6-454b-a1a9-716434176701, 13, Finished, Available, Finished)"},"metadata":{}}],"execution_count":11,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d4d0394d-48aa-4bbd-964b-5b5c982c41da"},{"cell_type":"code","source":["my_data = []\n","my_data.append({\n","    'names': ['John','James'],\n","    'ages':[20,30]\n","})\n","my_data.append({\n","    'names':['Lydia', 'Mary'],\n","    'ages':[25,40]\n","})"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":22,"statement_ids":[22],"state":"finished","livy_statement_state":"available","session_id":"7030184d-2af6-454b-a1a9-716434176701","normalized_state":"finished","queued_time":"2025-10-13T22:21:25.5154883Z","session_start_time":null,"execution_start_time":"2025-10-13T22:21:25.5166782Z","execution_finish_time":"2025-10-13T22:21:25.8304992Z","parent_msg_id":"5e79cc38-98f8-4b72-90c8-03ec31ea76b3"},"text/plain":"StatementMeta(, 7030184d-2af6-454b-a1a9-716434176701, 22, Finished, Available, Finished)"},"metadata":{}}],"execution_count":20,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"6e881837-a590-4856-8e12-c4e65736629b"},{"cell_type":"code","source":["for dat in my_data:\n","    names = dat['names']\n","    print(names)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":24,"statement_ids":[24],"state":"finished","livy_statement_state":"available","session_id":"7030184d-2af6-454b-a1a9-716434176701","normalized_state":"finished","queued_time":"2025-10-13T22:22:50.4237473Z","session_start_time":null,"execution_start_time":"2025-10-13T22:22:50.4249171Z","execution_finish_time":"2025-10-13T22:22:50.7328444Z","parent_msg_id":"380cd1d2-1464-4fbf-9f4d-21ec823ecfc5"},"text/plain":"StatementMeta(, 7030184d-2af6-454b-a1a9-716434176701, 24, Finished, Available, Finished)"},"metadata":{}},{"output_type":"stream","name":"stdout","text":["['John', 'James']\n['Lydia', 'Mary']\n"]}],"execution_count":22,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"d34cfe8f-f12a-4b99-9e35-40377d6742e9"},{"cell_type":"markdown","source":["## Lakehouse Storage for Unused Objects\n"],"metadata":{},"id":"48c28805-683f-4573-a374-458a1e9085ef"},{"cell_type":"code","source":["def save_unused_objects_to_lakehouse(unused_objects_results):\n","    \"\"\"\n","    Save unused objects analysis results to lakehouse tables\n","    \"\"\"\n","    tracker.update(current_operation =\"\\n💾 Saving unused objects analysis to lakehouse...\")\n","    \n","    try:\n","        # Create DataFrames for unused objects\n","        if unused_objects_results['unused_measures']:\n","            unused_measures_df = pd.DataFrame(\n","                unused_objects_results['unused_measures'],\n","                columns=['dataset_name', 'measure_name']\n","            )\n","            unused_measures_df['analysis_date'] = datetime.now()\n","            spark.createDataFrame(unused_measures_df).write.mode(\"overwrite\").saveAsTable(\"unused_measures_analysis\")\n","            # print(f\"  ✓ Saved {len(unused_measures_df)} unused measures to 'unused_measures_analysis' table\")\n","        \n","        if unused_objects_results['unused_columns']:\n","            unused_columns_df = pd.DataFrame(\n","                unused_objects_results['unused_columns'],\n","                columns=['dataset_name', 'column_name']\n","            )\n","            unused_columns_df['analysis_date'] = datetime.now()\n","            spark.createDataFrame(unused_columns_df).write.mode(\"overwrite\").saveAsTable(\"unused_columns_analysis\")\n","            # print(f\"  ✓ Saved {len(unused_columns_df)} unused columns to 'unused_columns_analysis' table\")\n","        \n","        if unused_objects_results['unused_tables']:\n","            unused_tables_df = pd.DataFrame(\n","                unused_objects_results['unused_tables'],\n","                columns=['dataset_name', 'table_name']\n","            )\n","            unused_tables_df['analysis_date'] = datetime.now()\n","            spark.createDataFrame(unused_tables_df).write.mode(\"overwrite\").saveAsTable(\"unused_tables_analysis\")\n","            # print(f\"  ✓ Saved {len(unused_tables_df)} unused tables to 'unused_tables_analysis' table\")\n","        \n","        # Create summary metrics table\n","        summary_metrics = []\n","        for metric_type, metrics in [\n","            ('measures', unused_objects_results['measures_metrics']),\n","            ('columns', unused_objects_results['columns_metrics']),\n","            ('tables', unused_objects_results['tables_metrics'])\n","        ]:\n","            summary_metrics.append({\n","                'object_type': metric_type,\n","                'total_objects': metrics[f'total_{metric_type}'],\n","                'unused_objects': metrics[f'unused_{metric_type}'],\n","                'used_objects': metrics[f'used_{metric_type}'],\n","                'utilization_rate': metrics['utilization_rate'],\n","                'analysis_date': datetime.now()\n","            })\n","        \n","        summary_df = pd.DataFrame(summary_metrics)\n","        spark.createDataFrame(summary_df).write.mode(\"overwrite\").saveAsTable(\"unused_objects_summary\")\n","        # print(f\"  ✓ Saved summary metrics to 'unused_objects_summary' table\")\n","        \n","    except Exception as e:\n","        # print(f\"  ❌ Error saving to lakehouse: {str(e)}\")\n","        tracker.log_error(\"Error saving unused objects analysis to lakehouse\", e)"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":14,"statement_ids":[14],"state":"finished","livy_statement_state":"available","session_id":"7030184d-2af6-454b-a1a9-716434176701","normalized_state":"finished","queued_time":"2025-10-13T21:53:04.4410103Z","session_start_time":null,"execution_start_time":"2025-10-13T21:53:51.2449233Z","execution_finish_time":"2025-10-13T21:53:51.6682474Z","parent_msg_id":"c92e9a9d-77d4-4c2c-83f2-7137a08832a6"},"text/plain":"StatementMeta(, 7030184d-2af6-454b-a1a9-716434176701, 14, Finished, Available, Finished)"},"metadata":{}}],"execution_count":12,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"993767cf-aecd-4fbe-910f-48b7c36ba7bf"},{"cell_type":"markdown","source":["## Main Function\n"],"metadata":{},"id":"440d8e38-6baa-4154-a81f-36e7bc49b04a"},{"cell_type":"code","source":["def main():\n","    \"\"\"\n","    🆕 Enhanced main function with integrated unused objects analysis\n","    \"\"\"\n","    print(\"🚀\" + \"=\"*80)\n","    print(\"🎯 FABRIC MODEL ANALYSIS WITH UNUSED OBJECTS DETECTION\")\n","    print(\"=\"*82)\n","    \n","    # Get all workspaces\n","    workspaces = fabric.list_workspaces()\n","    workspaces = sanitize_df_columns(workspaces)    \n","    tracker.update(\n","        processed_workspaces=0,\n","        total_workspaces=len(workspaces),\n","        processing_stage='Data Collection'\n","    )\n","\n","    # Collect all workspace objects\n","    datasets, reports, dataflows, measures, relationships, tables, columns, datasets_info = get_workspace_objects(workspaces)\n","\n","    # Save original data to Lakehouse\n","    tracker.update(\n","        current_operation='Saving to Lakehouse...',\n","        processing_stage='Data Storage'\n","    )\n","    \n","    if not workspaces.empty:\n","        spark.createDataFrame(workspaces).write.mode(\"overwrite\").saveAsTable(\"fabric_workspaces\")\n","    if not datasets.empty:\n","        spark.createDataFrame(datasets).write.mode(\"overwrite\").saveAsTable(\"workspace_datasets\")\n","    if not columns.empty:\n","        spark.createDataFrame(columns).write.mode(\"overwrite\").saveAsTable(\"dataset_columns\")\n","    if not tables.empty:\n","        spark.createDataFrame(tables).write.mode(\"overwrite\").saveAsTable(\"dataset_tables\")\n","    if not dataflows.empty:\n","        spark.createDataFrame(dataflows).write.mode(\"overwrite\").saveAsTable(\"fabric_dataflows\")\n","    if not measures.empty:\n","        spark.createDataFrame(measures).write.mode(\"overwrite\").saveAsTable(\"dataset_measures\")\n","    \n","    if not reports.empty:\n","        reports_spark = spark.createDataFrame(reports)\n","        columns_to_clean = ['users', 'subscriptions']\n","        for col_name in columns_to_clean:\n","            if col_name in reports.columns:\n","                reports_spark = reports_spark.withColumn(\n","                    col_name, \n","                    F.col(col_name).cast(ArrayType(StringType()))\n","                )\n","        reports_spark.write.mode(\"overwrite\").saveAsTable(\"workspace_reports\")\n","\n","    if not relationships.empty:\n","        for col in relationships.select_dtypes(include=['uint64']).columns:\n","            relationships[col] = relationships[col].astype('int64')\n","        spark.createDataFrame(relationships).write.mode(\"overwrite\").saveAsTable(\"dataset_relationships\")\n","    \n","    # 🆕 NEW: Analyze unused objects across all datasets\n","    tracker.update(\n","        current_operation='Analyzing Unused Objects...',\n","        processing_stage='Unused Objects Analysis'\n","    )\n","    \n","    unused_objects_results = analyze_unused_objects_across_datasets(datasets_info)\n","    \n","    # 🆕 NEW: Save unused objects analysis to lakehouse\n","    save_unused_objects_to_lakehouse(unused_objects_results)\n","    \n","    # Final summary\n","    print(\"\\n\" + \"=\"*82)\n","    print(\"🎉 ENHANCED ANALYSIS COMPLETE\")\n","    print(\"=\"*82)\n","    print(f\"📊 Workspaces: {len(workspaces)}\")\n","    print(f\"📊 Datasets: {len(datasets)}\")\n","    print(f\"📊 Reports: {len(reports)}\")\n","    print(f\"📊 Measures: {len(measures)} (Unused: {unused_objects_results['measures_metrics']['unused_measures']})\")\n","    print(f\"📊 Columns: {len(columns)} (Unused: {unused_objects_results['columns_metrics']['unused_columns']})\")\n","    print(f\"📊 Tables: {len(tables)} (Unused: {unused_objects_results['tables_metrics']['unused_tables']})\")\n","    print(f\"📊 Relationships: {len(relationships)}\")\n","    print(\"=\"*82)\n","    \n","    return unused_objects_results"],"outputs":[{"output_type":"display_data","data":{"application/vnd.livy.statement-meta+json":{"spark_pool":null,"statement_id":17,"statement_ids":[17],"state":"finished","livy_statement_state":"available","session_id":"7030184d-2af6-454b-a1a9-716434176701","normalized_state":"finished","queued_time":"2025-10-13T22:03:14.9477383Z","session_start_time":null,"execution_start_time":"2025-10-13T22:03:14.948924Z","execution_finish_time":"2025-10-13T22:03:15.273547Z","parent_msg_id":"1a97e0d2-9019-4c0b-9bf0-61ac8cc57f7a"},"text/plain":"StatementMeta(, 7030184d-2af6-454b-a1a9-716434176701, 17, Finished, Available, Finished)"},"metadata":{}}],"execution_count":15,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"e8e0d4fc-ace9-493e-9eb3-21ac5e4c8918"},{"cell_type":"markdown","source":["\n","### Run the enhanced main function!\n","\n","- ✅ Original workspace objects collection\n","- ✅ Unused measures detection\n","- ✅ Unused columns detection  \n","- ✅ Unused tables detection\n","- ✅ Comprehensive 4-panel visualization\n","- ✅ Lakehouse storage for analysis results\n","- ✅ Real-time progress tracking"],"metadata":{},"id":"8c2d05e6-e1e6-4e15-9825-f629a4154661"},{"cell_type":"code","source":["# 🚀 Execute the enhanced main function\n","# This will now automatically detect unused measures, columns, and tables!\n","results = main()"],"outputs":[],"execution_count":null,"metadata":{"microsoft":{"language":"python","language_group":"synapse_pyspark"}},"id":"8a05fdbd-2dbe-4dcc-b5b4-b388ca1832c1"}],"metadata":{"kernelspec":{"name":"synapse_pyspark","display_name":"Synapse PySpark"},"language_info":{"name":"python"},"microsoft":{"language":"python","language_group":"synapse_pyspark","ms_spell_check":{"ms_spell_check_language":"en"}},"nteract":{"version":"nteract-front-end@1.0.0"},"kernel_info":{"name":"synapse_pyspark"},"spark_compute":{"compute_id":"/trident/default","session_options":{"conf":{"spark.synapse.nbs.session.timeout":"1200000"}}},"dependencies":{"lakehouse":{"known_lakehouses":[{"id":"b688d372-651f-4ab4-8a7a-9c8b0587b8a1"}],"default_lakehouse":"b688d372-651f-4ab4-8a7a-9c8b0587b8a1","default_lakehouse_name":"Scan_Results","default_lakehouse_workspace_id":"32bcf9c2-9d7d-4857-9188-ef29efac3a3b"}}},"nbformat":4,"nbformat_minor":5}