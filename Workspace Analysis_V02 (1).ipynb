{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4571a258-2b09-4733-9702-00d87acafdda",
   "metadata": {
    "jupyter": {
     "outputs_hidden": false
    },
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-10-28T05:11:48.9441057Z",
       "execution_start_time": "2025-10-28T05:11:26.3538963Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "999f9e15-e30f-44a0-9013-f06aabe2deac",
       "queued_time": "2025-10-28T05:11:13.0204081Z",
       "session_id": "c6750e83-47d2-4299-9124-248033c7d2f1",
       "session_start_time": "2025-10-28T05:11:13.0218024Z",
       "spark_pool": null,
       "state": "finished",
       "statement_id": 8,
       "statement_ids": [
        3,
        4,
        5,
        6,
        7,
        8
       ]
      },
      "text/plain": [
       "StatementMeta(, c6750e83-47d2-4299-9124-248033c7d2f1, 8, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "fsspec-wrapper 0.1.15 requires PyJWT>=2.6.0, but you have pyjwt 2.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.0\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m25.3\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpython -m pip install --upgrade pip\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Warning: PySpark kernel has been restarted to use updated packages.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Install semantic-link-labs for extended Fabric analytics\n",
    "%pip install -q -U semantic-link-labs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98c3693c-3b25-45e7-83e0-a66de5d88f3e",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-10-28T05:11:55.5517715Z",
       "execution_start_time": "2025-10-28T05:11:52.1617189Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "50af5961-dbfa-4b90-8e4b-ab8da0f182c2",
       "queued_time": "2025-10-28T05:11:13.022939Z",
       "session_id": "c6750e83-47d2-4299-9124-248033c7d2f1",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 10,
       "statement_ids": [
        10
       ]
      },
      "text/plain": [
       "StatementMeta(, c6750e83-47d2-4299-9124-248033c7d2f1, 10, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… All imports successful. Spark session and Claude AI client initialized\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sempy_labs\n",
    "import sempy.fabric as fabric\n",
    "from sempy_labs.report import ReportWrapper\n",
    "from pyspark.sql import SparkSession, functions as F\n",
    "from pyspark.sql.functions import col\n",
    "from sempy.fabric import FabricRestClient\n",
    "import re\n",
    "import json\n",
    "from typing import Dict, List, Set, Any, Optional\n",
    "import os\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "# Initialize Spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "print(\"âœ… All imports successful. Spark session and Claude AI client initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "857440c4-f5f1-4f3a-a600-a961f08a3e65",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-10-28T05:11:55.9234375Z",
       "execution_start_time": "2025-10-28T05:11:55.5539087Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "a18d3ccc-a872-47d1-92ce-fbca4cd39e2d",
       "queued_time": "2025-10-28T05:11:13.0248362Z",
       "session_id": "c6750e83-47d2-4299-9124-248033c7d2f1",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 11,
       "statement_ids": [
        11
       ]
      },
      "text/plain": [
       "StatementMeta(, c6750e83-47d2-4299-9124-248033c7d2f1, 11, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Report metadata extraction function defined\n"
     ]
    }
   ],
   "source": [
    "# ============================================================\n",
    "# UTILITY FUNCTIONS AND DATA STRUCTURES\n",
    "# ============================================================\n",
    "@dataclass\n",
    "class DatasetInfo:\n",
    "    \"\"\"Data structure to hold comprehensive dataset information\"\"\"\n",
    "    ds_id: str\n",
    "    ds_name: str\n",
    "    ws_id: str\n",
    "    ws_name: str\n",
    "    dependencies_df: Optional[pd.DataFrame] = None\n",
    "    tables_df: Optional[pd.DataFrame] = None\n",
    "    relationships_df: Optional[pd.DataFrame] = None\n",
    "    measures_df: Optional[pd.DataFrame] = None\n",
    "    columns_df: Optional[pd.DataFrame] = None\n",
    "\n",
    "@dataclass\n",
    "class ReportMetadata:\n",
    "    \"\"\"Data structure to hold Power BI report metadata analysis\"\"\"\n",
    "    report_id: str\n",
    "    report_name: str\n",
    "    workspace_id: str\n",
    "    workspace_name: str\n",
    "    dataset_id: str\n",
    "    report_format: str\n",
    "    extraction_method: str\n",
    "    tables: List[str]\n",
    "    columns: List[str]\n",
    "    measures: List[str]\n",
    "    visuals_count: int\n",
    "    filters_count: int\n",
    "    extraction_success: bool\n",
    "    error_message: str = \"\"\n",
    "\n",
    "print(\"âœ… Report metadata extraction function defined\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9f998280-b03c-428f-affa-6f4f02c3d74a",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-10-28T05:11:56.2281296Z",
       "execution_start_time": "2025-10-28T05:11:55.9261049Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "e7482782-a94b-47b9-9221-f8fc7ecd592a",
       "queued_time": "2025-10-28T05:11:13.0294011Z",
       "session_id": "c6750e83-47d2-4299-9124-248033c7d2f1",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 12,
       "statement_ids": [
        12
       ]
      },
      "text/plain": [
       "StatementMeta(, c6750e83-47d2-4299-9124-248033c7d2f1, 12, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "class PowerBIMetadataExtractor:\n",
    "    \"\"\"Extracts columns, tables, and measures from Power BI report metadata\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.tables = set()\n",
    "        self.columns = set()\n",
    "        self.measures = set()\n",
    "        self.visual_details = []\n",
    "        self.filter_details = []\n",
    "        \n",
    "    def extract_from_json_data(self, data: Dict[str, Any]) -> Dict[str, Any]:\n",
    "        \"\"\"Extract metadata from JSON data\"\"\"\n",
    "        self._reset()\n",
    "        \n",
    "        # Extract from sections\n",
    "        sections = data.get('sections', [])\n",
    "        \n",
    "        for section_idx, section in enumerate(sections):\n",
    "            section_name = section.get('displayName', f'Section_{section_idx}')\n",
    "            \n",
    "            # Extract from section-level filters\n",
    "            filters = section.get('filters', [])\n",
    "            if isinstance(filters, str):\n",
    "                filters = json.loads(filters)\n",
    "            self._extract_from_filters(filters, 'section', section_name)\n",
    "            \n",
    "            # Extract from visual containers\n",
    "            visual_containers = section.get('visualContainers', [])\n",
    "            self._extract_from_visual_containers(visual_containers, section_name)\n",
    "        \n",
    "        # Compile results\n",
    "        results = {\n",
    "            'tables': sorted(list(self.tables)),\n",
    "            'columns': sorted(list(self.columns)),\n",
    "            'measures': sorted(list(self.measures)),\n",
    "            'summary': {\n",
    "                'total_tables': len(self.tables),\n",
    "                'total_columns': len(self.columns),\n",
    "                'total_measures': len(self.measures)\n",
    "            },\n",
    "            'visual_details': self.visual_details,\n",
    "            'filter_details': self.filter_details\n",
    "        }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _reset(self):\n",
    "        \"\"\"Reset all collections for new extraction\"\"\"\n",
    "        self.tables.clear()\n",
    "        self.columns.clear()\n",
    "        self.measures.clear()\n",
    "        self.visual_details.clear()\n",
    "        self.filter_details.clear()\n",
    "    \n",
    "    def _extract_from_visual_containers(self, visual_containers: List[Dict], section_name: str):\n",
    "        \"\"\"Extract from visualContainers array\"\"\"\n",
    "        for visual_idx, visual_container in enumerate(visual_containers):\n",
    "            visual_config = visual_container.get('config', {})\n",
    "            if isinstance(visual_config, str):\n",
    "                visual_config = json.loads(visual_config)\n",
    "            visual_name = visual_config.get('name', f'Visual_{visual_idx}')\n",
    "            \n",
    "            # Extract from visual-level filters\n",
    "            filters = visual_container.get('filters', [])\n",
    "            if isinstance(filters, str):\n",
    "                filters = json.loads(filters)\n",
    "            self._extract_from_filters(\n",
    "                filters, \n",
    "                'visual', \n",
    "                f\"{section_name}->{visual_name}\"\n",
    "            )\n",
    "            \n",
    "            # Extract from singleVisual\n",
    "            single_visual = visual_config.get('singleVisual', {})\n",
    "            if single_visual:\n",
    "                self._extract_from_single_visual(single_visual, section_name, visual_name)\n",
    "    \n",
    "    def _extract_from_single_visual(self, single_visual: Dict, section_name: str, visual_name: str):\n",
    "        \"\"\"Extract from singleVisual object\"\"\"\n",
    "        visual_type = single_visual.get('visualType', 'unknown')\n",
    "        \n",
    "        # Extract from projections\n",
    "        projections = single_visual.get('projections', {})\n",
    "        projection_refs = []\n",
    "        \n",
    "        for projection_type, projection_list in projections.items():\n",
    "            for proj in projection_list:\n",
    "                query_ref = proj.get('queryRef', '')\n",
    "                if query_ref:\n",
    "                    projection_refs.append(query_ref)\n",
    "                    self._parse_query_ref(query_ref)\n",
    "        \n",
    "        # Extract from prototypeQuery\n",
    "        prototype_query = single_visual.get('prototypeQuery', {})\n",
    "        self._extract_from_prototype_query(prototype_query)\n",
    "        \n",
    "        # Extract from objects (labels and other formatting with field references)\n",
    "        objects = single_visual.get('objects', {})\n",
    "        if objects:\n",
    "            self._extract_field_reference(objects)\n",
    "        \n",
    "        # Store visual details\n",
    "        self.visual_details.append({\n",
    "            'section': section_name,\n",
    "            'visual_name': visual_name,\n",
    "            'visual_type': visual_type,\n",
    "            'projection_refs': projection_refs,\n",
    "            'has_prototype_query': bool(prototype_query)\n",
    "        })\n",
    "    \n",
    "    def _extract_from_prototype_query(self, prototype_query: Dict):\n",
    "        \"\"\"Extract from prototypeQuery object\"\"\"\n",
    "        # Extract tables from 'From' clause\n",
    "        from_clause = prototype_query.get('From', [])\n",
    "        for from_item in from_clause:\n",
    "            entity = from_item.get('Entity', '')\n",
    "            if entity and self._is_actual_table_name(entity):\n",
    "                self.tables.add(entity)\n",
    "        \n",
    "        # Extract columns and measures from 'Select' clause using unified extractor\n",
    "        select_clause = prototype_query.get('Select', [])\n",
    "        for select_item in select_clause:\n",
    "            self._extract_field_reference(select_item)\n",
    "    \n",
    "    def _extract_from_filters(self, filters: List[Dict], filter_type: str, context: str):\n",
    "        \"\"\"Extract from filters array\"\"\"\n",
    "\n",
    "        for filter_idx, filter_obj in enumerate(filters):\n",
    "            filter_name = filter_obj.get('name', f'Filter_{filter_idx}')\n",
    "            \n",
    "            # Extract from expression\n",
    "            expression = filter_obj.get('expression', {})\n",
    "            self._extract_from_expression(expression)\n",
    "            \n",
    "            # Extract from filter object (nested structure)\n",
    "            filter_def = filter_obj.get('filter', {})\n",
    "            if filter_def:\n",
    "                # Extract tables from 'From' clause in filter\n",
    "                from_clause = filter_def.get('From', [])\n",
    "                for from_item in from_clause:\n",
    "                    entity = from_item.get('Entity', '')\n",
    "                    if entity:\n",
    "                        self.tables.add(entity)\n",
    "                \n",
    "                # Extract from 'Where' clause - might contain column references\n",
    "                where_clause = filter_def.get('Where', [])\n",
    "                for where_item in where_clause:\n",
    "                    self._extract_from_where_condition(where_item)\n",
    "            \n",
    "            # Store filter details\n",
    "            self.filter_details.append({\n",
    "                'filter_type': filter_type,\n",
    "                'context': context,\n",
    "                'filter_name': filter_name,\n",
    "                'has_expression': bool(expression),\n",
    "                'has_filter_def': bool(filter_def)\n",
    "            })\n",
    "    \n",
    "    def _extract_field_reference(self, item: Dict):\n",
    "        \"\"\"Unified field reference extractor for Columns and Measures.\n",
    "        \n",
    "        Works for both:\n",
    "        - prototypeQuery.Select[] items\n",
    "        - objects.labels[].properties nested structures\n",
    "        - Any nested structure with Column/Measure patterns\n",
    "        \"\"\"\n",
    "        if not isinstance(item, dict):\n",
    "            return\n",
    "        \n",
    "        # Get Name if available (from Select clause)\n",
    "        name = item.get('Name', '')\n",
    "        \n",
    "        # Extract Column reference\n",
    "        if 'Column' in item:\n",
    "            column_def = item['Column']\n",
    "            if isinstance(column_def, dict):\n",
    "                entity = self._get_entity_from_expression(column_def)\n",
    "                property_name = column_def.get('Property', '')\n",
    "                \n",
    "                # If we have Name, parse it for the table name\n",
    "                if name and '.' in name:\n",
    "                    table_name, field_name = name.split('.', 1)\n",
    "                    if self._is_actual_table_name(table_name):\n",
    "                        self.tables.add(table_name)\n",
    "                        self.columns.add(f\"'{table_name}'[{field_name}]\")\n",
    "                # Otherwise use Entity from SourceRef\n",
    "                elif entity and property_name and self._is_actual_table_name(entity):\n",
    "                    self.tables.add(entity)\n",
    "                    self.columns.add(f\"'{entity}'[{property_name}]\")\n",
    "        \n",
    "        # Extract Measure reference\n",
    "        elif 'Measure' in item:\n",
    "            measure_def = item['Measure']\n",
    "            if isinstance(measure_def, dict):\n",
    "                entity = self._get_entity_from_expression(measure_def)\n",
    "                property_name = measure_def.get('Property', '')\n",
    "                \n",
    "                # If we have Name, parse it for the table name\n",
    "                if name and '.' in name:\n",
    "                    table_name, field_name = name.split('.', 1)\n",
    "                    if self._is_actual_table_name(table_name):\n",
    "                        self.tables.add(table_name)\n",
    "                        self.measures.add(f\"'{table_name}'[{field_name}]\")\n",
    "                # Otherwise use Entity from SourceRef\n",
    "                elif entity and property_name and self._is_actual_table_name(entity):\n",
    "                    self.tables.add(entity)\n",
    "                    self.measures.add(f\"'{entity}'[{property_name}]\")\n",
    "        \n",
    "        # Recursively check nested structures (for objects.labels, etc.)\n",
    "        for value in item.values():\n",
    "            if isinstance(value, dict):\n",
    "                self._extract_field_reference(value)\n",
    "            elif isinstance(value, list):\n",
    "                for list_item in value:\n",
    "                    if isinstance(list_item, dict):\n",
    "                        self._extract_field_reference(list_item)\n",
    "    \n",
    "    def _get_entity_from_expression(self, field_def: Dict) -> str:\n",
    "        \"\"\"Extract entity/table name from Expression.SourceRef.\n",
    "        \n",
    "        Handles both:\n",
    "        - {\"Expression\": {\"SourceRef\": {\"Entity\": \"TableName\"}}}  # Actual table\n",
    "        - {\"Expression\": {\"SourceRef\": {\"Source\": \"t\"}}}          # Alias\n",
    "        \"\"\"\n",
    "        expression = field_def.get('Expression', {})\n",
    "        if isinstance(expression, dict):\n",
    "            source_ref = expression.get('SourceRef', {})\n",
    "            if isinstance(source_ref, dict):\n",
    "                # Prefer Entity over Source (Entity is actual table name)\n",
    "                return source_ref.get('Entity', source_ref.get('Source', ''))\n",
    "        return ''\n",
    "    \n",
    "    def _extract_from_expression(self, expression: Dict):\n",
    "        \"\"\"Extract from expression object\"\"\"\n",
    "        if 'Column' in expression:\n",
    "            # Extract table from SourceRef\n",
    "            column_expr = expression['Column']\n",
    "            source_ref = column_expr.get('Expression', {}).get('SourceRef', {})\n",
    "            entity = source_ref.get('Entity', '')\n",
    "            if entity:\n",
    "                self.tables.add(entity)\n",
    "            \n",
    "            # Extract column property\n",
    "            property_name = column_expr.get('Property', '')\n",
    "            if property_name and entity:\n",
    "                self.columns.add(f\"'{entity}'[{property_name}]\")\n",
    "    \n",
    "    def _extract_from_where_condition(self, where_item: Dict):\n",
    "        \"\"\"Extract from WHERE condition\"\"\"\n",
    "        condition = where_item.get('Condition', {})\n",
    "        if 'In' in condition:\n",
    "            expressions = condition['In'].get('Expressions', [])\n",
    "            for expr in expressions:\n",
    "                self._extract_from_expression(expr)\n",
    "    \n",
    "    def _is_actual_table_name(self, table_name: str) -> bool:\n",
    "        \"\"\"Check if table name is an actual table, not a query alias/prefix.\"\"\"\n",
    "        if not table_name or not isinstance(table_name, str):\n",
    "            return False\n",
    "        \n",
    "        # Filter out single character aliases (d, s, c, _, etc.)\n",
    "        if len(table_name) <= 1:\n",
    "            return False\n",
    "        \n",
    "        # Filter out common query aliases\n",
    "        query_aliases = {'subquery', 'temp', 'alias', 'src', 'tgt'}\n",
    "        if table_name.lower() in query_aliases:\n",
    "            return False\n",
    "            \n",
    "        return True\n",
    "    \n",
    "    def _parse_query_ref(self, query_ref: str):\n",
    "        \"\"\"Parse queryRef format (e.g., 'table.column' or 'table.measure')\"\"\"\n",
    "        if '.' in query_ref:\n",
    "            table_name, field_name = query_ref.split('.', 1)\n",
    "            if self._is_actual_table_name(table_name):\n",
    "                self.tables.add(table_name)\n",
    "            # We'll determine if it's a column or measure from prototype query\n",
    "            # For now, just store the full reference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "933fb886-82cd-4c73-aa67-0728e148ac08",
   "metadata": {
    "collapsed": false,
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-10-28T05:11:57.1282391Z",
       "execution_start_time": "2025-10-28T05:11:56.2330628Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "40dd55b6-7fd9-4e9a-84e7-d7359a5f3ec9",
       "queued_time": "2025-10-28T05:11:13.2973036Z",
       "session_id": "c6750e83-47d2-4299-9124-248033c7d2f1",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 13,
       "statement_ids": [
        13
       ]
      },
      "text/plain": [
       "StatementMeta(, c6750e83-47d2-4299-9124-248033c7d2f1, 13, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "class FabricWorkspaceAnalyzer:\n",
    "    \"\"\"Main analyzer class implementing the complete workflow\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.workspaces_df = pd.DataFrame()\n",
    "        self.datasets_df = pd.DataFrame()\n",
    "        self.reports_df = pd.DataFrame()\n",
    "        self.pbi_reports_df = pd.DataFrame()\n",
    "        self.all_dataset_info = {}\n",
    "        self.report_metadata_list = []\n",
    "        self.report_objects_used = []\n",
    "        self.error_log = []  # Store detailed errors for later display\n",
    "        \n",
    "    def sanitize_df_columns(self, df, extra_columns=False, ws_id=None, ds_id=None, ws_name=None, ds_name=None):\n",
    "        \"\"\"Replaces spaces in column names with underscore to prevent errors during Spark Dataframe Creation\"\"\"\n",
    "        if df.empty:\n",
    "            return df\n",
    "            \n",
    "        df.columns = [\n",
    "            re.sub(r'\\W+', \"_\", col.strip().lower())\n",
    "            for col in df.columns\n",
    "        ]\n",
    "\n",
    "        if extra_columns:\n",
    "            df['workspace_id'] = ws_id\n",
    "            df['dataset_id'] = ds_id\n",
    "            df['workspace_name'] = ws_name\n",
    "            df['dataset_name'] = ds_name\n",
    "            \n",
    "        return df\n",
    "\n",
    "    def save_to_lakehouse(self, df, table_name, description=\"\"):\n",
    "        \"\"\"Save DataFrame to lakehouse using Spark\"\"\"\n",
    "        try:\n",
    "            if df.empty:\n",
    "                print(f\"  âš ï¸ Skipping empty DataFrame for table: {table_name}\")\n",
    "                return\n",
    "                \n",
    "            # Add analysis timestamp\n",
    "            df_with_timestamp = df.copy()\n",
    "            df_with_timestamp['analysis_date'] = datetime.now()\n",
    "            \n",
    "            # Convert to Spark DataFrame and save\n",
    "            spark_df = spark.createDataFrame(df_with_timestamp)\n",
    "            spark_df.write.mode(\"overwrite\").saveAsTable(table_name)\n",
    "            \n",
    "            print(f\"  âœ… Saved {len(df)} records to '{table_name}' table\")\n",
    "            if description:\n",
    "                print(f\"     ðŸ“ {description}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ Error saving to {table_name}: {str(e)}\")\n",
    "    def _extract_meaningful_error(self, error_msg):\n",
    "        \"\"\"\n",
    "        Extract the meaningful error message from exceptions.\n",
    "        Removes technical details like stack traces, activity IDs, and timestamps.\n",
    "        \"\"\"\n",
    "        lines = error_msg.split('\\n')\n",
    "        \n",
    "        # First line is usually the initial error\n",
    "        brief_error = lines[0].strip()\n",
    "        \n",
    "        # Look for the actual error message after \"Caused by\" or similar patterns\n",
    "        for i, line in enumerate(lines):\n",
    "            line_stripped = line.strip()\n",
    "            \n",
    "            # Look for patterns that indicate the meaningful error\n",
    "            if 'Caused by' in line_stripped and i + 1 < len(lines):\n",
    "                next_line = lines[i + 1].strip()\n",
    "                # Skip if next line is empty or starts with technical details\n",
    "                if next_line and not next_line.startswith('Technical Details:'):\n",
    "                    return next_line\n",
    "            \n",
    "            # Stop at technical details section\n",
    "            if 'Technical Details:' in line_stripped:\n",
    "                break\n",
    "            \n",
    "            # Stop at stack traces\n",
    "            if line_stripped.startswith('at '):\n",
    "                break\n",
    "        \n",
    "        # Return the first line if nothing better found\n",
    "        return brief_error\n",
    "\n",
    "    \n",
    "    def get_workspaces(self):\n",
    "        \"\"\"Step 1: Get Workspaces\"\"\"\n",
    "        print(\"ðŸ” STEP 1: Discovering workspaces...\")\n",
    "        \n",
    "        self.workspaces_df = fabric.list_workspaces()\n",
    "        self.workspaces_df = self.sanitize_df_columns(self.workspaces_df)\n",
    "        self.workspaces_df = self.workspaces_df[['id', 'name', 'type']]\n",
    "        \n",
    "        print(f\"  âœ… Found {len(self.workspaces_df)} workspaces\")\n",
    "        return self.workspaces_df\n",
    "    \n",
    "    def get_datasets_and_reports(self):\n",
    "        \"\"\"Step 2: Get Datasets and Reports in parallel\"\"\"\n",
    "        print(\"\\nðŸ” STEP 2: Getting datasets and reports...\")\n",
    "        \n",
    "        datasets_all, reports_all = [], []\n",
    "        \n",
    "        for _, ws in self.workspaces_df.iterrows():\n",
    "            ws_id = ws['id']\n",
    "            ws_name = ws['name']\n",
    "            ws_type = ws['type']\n",
    "            \n",
    "            if ws_type == \"AdminInsights\":\n",
    "                continue\n",
    "                \n",
    "            print(f\"  ðŸ“¦ Scanning workspace: {ws_name}\")\n",
    "            \n",
    "            # Get Datasets\n",
    "            try:\n",
    "                ds = fabric.list_datasets(workspace=ws_id)\n",
    "                if not ds.empty:\n",
    "                    ds['workspace_id'] = ws_id\n",
    "                    ds['workspace_name'] = ws_name\n",
    "                    datasets_all.append(ds)\n",
    "            except Exception as e:\n",
    "                print(f\"    âš ï¸ Datasets error in {ws_name}: {e}\")\n",
    "            \n",
    "            # Get Reports\n",
    "            try:\n",
    "                rep = fabric.list_reports(workspace=ws_id)\n",
    "                if not rep.empty:\n",
    "                    rep['workspace_id'] = ws_id\n",
    "                    rep['workspace_name'] = ws_name\n",
    "                    reports_all.append(rep)\n",
    "            except Exception as e:\n",
    "                print(f\"    âš ï¸ Reports error in {ws_name}: {e}\")\n",
    "        \n",
    "        # Combine results\n",
    "        self.datasets_df = self.sanitize_df_columns(pd.concat(datasets_all, ignore_index=True) if datasets_all else pd.DataFrame())\n",
    "        self.reports_df = self.sanitize_df_columns(pd.concat(reports_all, ignore_index=True) if reports_all else pd.DataFrame())\n",
    "        \n",
    "        # Filter PowerBI reports\n",
    "        if not self.reports_df.empty and \"report_type\" in self.reports_df.columns:\n",
    "            self.pbi_reports_df = self.reports_df[self.reports_df[\"report_type\"] == \"PowerBIReport\"].copy()\n",
    "        else:\n",
    "            self.pbi_reports_df = self.reports_df\n",
    "        \n",
    "        print(f\"  âœ… Found {len(self.datasets_df)} datasets and {len(self.reports_df)} reports ({len(self.pbi_reports_df)} PowerBI reports)\")\n",
    "        return self.datasets_df, self.reports_df\n",
    "    \n",
    "    def process_all_datasets(self):\n",
    "        \"\"\"Step 3: Process all datasets and aggregate all objects (tables, columns, measures, dependencies)\"\"\"\n",
    "        print(\"\\nðŸ” STEP 3: Processing all datasets and aggregating objects...\")\n",
    "        \n",
    "        all_columns_list = []\n",
    "        all_tables_list = []\n",
    "        all_measures_list = []\n",
    "        all_dependencies_list = []\n",
    "        all_relationships_list = []\n",
    "        \n",
    "        for _, ds_row in self.datasets_df.iterrows():\n",
    "            ds_id = ds_row['dataset_id']\n",
    "            ds_name = ds_row['dataset_name']\n",
    "            ws_id = ds_row['workspace_id']\n",
    "            ws_name = ds_row['workspace_name']\n",
    "            \n",
    "            print(f\"  ðŸ“Š Processing dataset: {ds_name}\")\n",
    "            \n",
    "            # Collect comprehensive dataset info\n",
    "            dataset_info = self.collect_dataset_info(ds_id, ds_name, ws_id, ws_name)\n",
    "            self.all_dataset_info[ds_id] = dataset_info\n",
    "            \n",
    "            # Aggregate columns\n",
    "            if dataset_info.columns_df is not None and not dataset_info.columns_df.empty:\n",
    "                all_columns_list.append(dataset_info.columns_df)\n",
    "            \n",
    "            # Aggregate tables\n",
    "            if dataset_info.tables_df is not None and not dataset_info.tables_df.empty:\n",
    "                all_tables_list.append(dataset_info.tables_df)\n",
    "            \n",
    "            # Aggregate measures\n",
    "            if dataset_info.measures_df is not None and not dataset_info.measures_df.empty:\n",
    "                # Add additional context that might not be in the measures_df\n",
    "                measures_with_context = dataset_info.measures_df.copy()\n",
    "                if 'dataset_id' not in measures_with_context.columns:\n",
    "                    measures_with_context['dataset_id'] = ds_id\n",
    "                if 'dataset_name' not in measures_with_context.columns:\n",
    "                    measures_with_context['dataset_name'] = dataset_info.ds_name\n",
    "                if 'workspace_id' not in measures_with_context.columns:\n",
    "                    measures_with_context['workspace_id'] = dataset_info.ws_id\n",
    "                if 'workspace_name' not in measures_with_context.columns:\n",
    "                    measures_with_context['workspace_name'] = dataset_info.ws_name\n",
    "                all_measures_list.append(measures_with_context)\n",
    "            \n",
    "            # Aggregate dependencies\n",
    "            if dataset_info.dependencies_df is not None and not dataset_info.dependencies_df.empty:\n",
    "                all_dependencies_list.append(dataset_info.dependencies_df)\n",
    "            \n",
    "            # Aggregate relationships\n",
    "            if dataset_info.relationships_df is not None and not dataset_info.relationships_df.empty:\n",
    "                relationships_with_context = dataset_info.relationships_df.copy()\n",
    "                relationships_with_context['dataset_id'] = ds_id\n",
    "                relationships_with_context['dataset_name'] = dataset_info.ds_name\n",
    "                relationships_with_context['workspace_id'] = dataset_info.ws_id\n",
    "                relationships_with_context['workspace_name'] = dataset_info.ws_name\n",
    "                all_relationships_list.append(relationships_with_context)\n",
    "        \n",
    "        # Combine all aggregated data\n",
    "        all_columns_df = pd.concat(all_columns_list, ignore_index=True) if all_columns_list else pd.DataFrame()\n",
    "        all_tables_df = pd.concat(all_tables_list, ignore_index=True) if all_tables_list else pd.DataFrame()\n",
    "        all_measures_df = pd.concat(all_measures_list, ignore_index=True) if all_measures_list else pd.DataFrame()\n",
    "        all_dependencies_df = pd.concat(all_dependencies_list, ignore_index=True) if all_dependencies_list else pd.DataFrame()\n",
    "        all_relationships_df = pd.concat(all_relationships_list, ignore_index=True) if all_relationships_list else pd.DataFrame()\n",
    "        \n",
    "        print(f\"  âœ… Processed {len(self.all_dataset_info)} datasets\")\n",
    "        print(f\"    ðŸ“‹ Aggregated: {len(all_columns_df)} columns, {len(all_tables_df)} tables, {len(all_measures_df)} measures\")\n",
    "        print(f\"    ðŸ”— Aggregated: {len(all_dependencies_df)} dependencies, {len(all_relationships_df)} relationships\")\n",
    "        \n",
    "        return all_columns_df, all_tables_df, all_measures_df, all_dependencies_df, all_relationships_df\n",
    "    \n",
    "    def get_reports_metadata(self):\n",
    "        \"\"\"Step 4: Get Reports metadata (what objects they use)\"\"\"\n",
    "        print(\"\\nðŸ” STEP 4: Extracting report metadata...\")\n",
    "        \n",
    "        if self.pbi_reports_df.empty:\n",
    "            print(\"  âš ï¸ No PowerBI reports found\")\n",
    "            return []\n",
    "        \n",
    "        for idx, report_row in self.pbi_reports_df.iterrows():\n",
    "            report_id = report_row.get('id', '')\n",
    "            report_name = report_row.get('name', f'Report_{idx}')\n",
    "            workspace_id = report_row.get('workspace_id', '')\n",
    "            workspace_name = report_row.get('workspace_name', '')\n",
    "            dataset_id = report_row.get('dataset_id', '')\n",
    "            \n",
    "            print(f\"  ðŸ“Š Processing report {idx+1}/{len(self.pbi_reports_df)+1}: {report_name}\")\n",
    "            \n",
    "            # Extract metadata\n",
    "            report_metadata = self.extract_report_metadata(\n",
    "                report_id, report_name, workspace_id, workspace_name, dataset_id\n",
    "            )\n",
    "            \n",
    "            self.report_metadata_list.append(report_metadata)\n",
    "            \n",
    "            # Create detailed records for each object used by this report\n",
    "            if report_metadata.extraction_success:\n",
    "                # Add table records\n",
    "                for table in report_metadata.tables:\n",
    "                    self.report_objects_used.append({\n",
    "                        'report_id': report_id,\n",
    "                        'report_name': report_name,\n",
    "                        'workspace_id': workspace_id,\n",
    "                        'workspace_name': workspace_name,\n",
    "                        'dataset_id': dataset_id,\n",
    "                        'object_type': 'Table',\n",
    "                        'object_name': table,\n",
    "                        'full_reference': table,\n",
    "                        'extraction_method': report_metadata.extraction_method\n",
    "                    })\n",
    "                \n",
    "                # Add column records\n",
    "                for column in report_metadata.columns:\n",
    "                    table_name = column.split(\"'\")[1]\n",
    "                    column_name = column.split(\"'\")[2].strip(\"[]\")\n",
    "                    self.report_objects_used.append({\n",
    "                        'report_id': report_id,\n",
    "                        'report_name': report_name,\n",
    "                        'workspace_id': workspace_id,\n",
    "                        'workspace_name': workspace_name,\n",
    "                        'dataset_id': dataset_id,\n",
    "                        'object_type': 'Column',\n",
    "                        'object_name': column_name,\n",
    "                        'full_reference': column,\n",
    "                        'table_name': table_name,\n",
    "                        'extraction_method': report_metadata.extraction_method\n",
    "                    })\n",
    "                \n",
    "                # Add measure records\n",
    "                for measure in report_metadata.measures:\n",
    "                    table_name = measure.split(\"'\")[1]\n",
    "                    measure_name = measure.split(\"'\")[2].strip(\"[]\")\n",
    "                    self.report_objects_used.append({\n",
    "                        'report_id': report_id,\n",
    "                        'report_name': report_name,\n",
    "                        'workspace_id': workspace_id,\n",
    "                        'workspace_name': workspace_name,\n",
    "                        'dataset_id': dataset_id,\n",
    "                        'object_type': 'Measure',\n",
    "                        'object_name': measure_name,\n",
    "                        'full_reference': measure,\n",
    "                        'table_name': table_name,\n",
    "                        'extraction_method': report_metadata.extraction_method\n",
    "                    })\n",
    "        \n",
    "        print(f\"  âœ… Processed {len(self.report_metadata_list)+1} reports, extracted {len(self.report_objects_used)} object references\")\n",
    "        return self.report_metadata_list\n",
    "    \n",
    "    def check_dependencies(self, all_columns_df, all_tables_df, all_measures_df):\n",
    "        \"\"\"Step 5: Check for dependencies between objects\"\"\"\n",
    "        print(\"\\nðŸ” STEP 5: Checking for dependencies...\")\n",
    "        \n",
    "        # Convert report objects to DataFrame for easier analysis\n",
    "        report_objects_df = pd.DataFrame(self.report_objects_used) if self.report_objects_used else pd.DataFrame()\n",
    "        \n",
    "        # Get all used objects from reports\n",
    "        used_tables = set()\n",
    "        used_columns = set()\n",
    "        used_measures = set()\n",
    "        \n",
    "        if not report_objects_df.empty:\n",
    "            used_tables.update(report_objects_df[report_objects_df['object_type'] == 'Table']['full_reference'].tolist())\n",
    "            used_columns.update(report_objects_df[report_objects_df['object_type'] == 'Column']['full_reference'].tolist())\n",
    "            used_measures.update(report_objects_df[report_objects_df['object_type'] == 'Measure']['full_reference'].tolist())\n",
    "        \n",
    "        print(f\"  ðŸ“‹ Initial objects from reports: {len(used_tables)} tables, {len(used_columns)} columns, {len(used_measures)} measures\")\n",
    "        \n",
    "        # Check for dependencies within datasets (relationships and transitive dependencies)\n",
    "        for ds_id, dataset_info in self.all_dataset_info.items():\n",
    "            # Check relationships - columns used in relationships are required\n",
    "            if dataset_info.relationships_df is not None and not dataset_info.relationships_df.empty:\n",
    "                for _, rel in dataset_info.relationships_df.iterrows():\n",
    "                    if 'qualified_from' in rel:\n",
    "                        used_columns.add(rel['qualified_from'])\n",
    "                    if 'qualified_to' in rel:\n",
    "                        used_columns.add(rel['qualified_to'])\n",
    "        \n",
    "        print(f\"  ðŸ”— After adding relationship columns: {len(used_columns)} columns\")\n",
    "        \n",
    "        # Transitive dependency resolution: find what the used objects depend on\n",
    "        # Keep iterating until no new dependencies are found\n",
    "        iteration = 0\n",
    "        max_iterations = 10  # Prevent infinite loops\n",
    "        \n",
    "        while iteration < max_iterations:\n",
    "            iteration += 1\n",
    "            initial_tables_count = len(used_tables)\n",
    "            initial_columns_count = len(used_columns)\n",
    "            initial_measures_count = len(used_measures)\n",
    "            \n",
    "            print(f\"  ðŸ”„ Dependency resolution iteration {iteration}...\")\n",
    "            \n",
    "            # Check dependencies for all used objects\n",
    "            for ds_id, dataset_info in self.all_dataset_info.items():\n",
    "                if dataset_info.dependencies_df is None or dataset_info.dependencies_df.empty:\n",
    "                    continue\n",
    "                \n",
    "                # Iterate through each dependency row\n",
    "                for _, dep in dataset_info.dependencies_df.iterrows():\n",
    "                    # Get the full_object_name (the object that has the dependency)\n",
    "                    full_object_name = dep.get('full_object_name', '')\n",
    "                    \n",
    "                    # Check if this object is in our used sets\n",
    "                    if full_object_name in used_columns or full_object_name in used_measures:\n",
    "                        # This object is used, so we need to mark its dependencies as used too\n",
    "                        ref_object_type = dep.get('referenced_object_type', '')\n",
    "                        referenced_full_object_name = dep.get('referenced_full_object_name', '')\n",
    "                        \n",
    "                        if ref_object_type == 'Table':\n",
    "                            # The used object depends on a table\n",
    "                            table_name = dep.get('referenced_table', '')\n",
    "                            if table_name:\n",
    "                                used_tables.add(table_name)\n",
    "                        \n",
    "                        elif ref_object_type == 'Column':\n",
    "                            # The used object depends on a column\n",
    "                            if referenced_full_object_name:\n",
    "                                used_columns.add(referenced_full_object_name)\n",
    "                        \n",
    "                        elif ref_object_type == 'Measure':\n",
    "                            # The used object depends on a measure\n",
    "                            if referenced_full_object_name:\n",
    "                                used_measures.add(referenced_full_object_name)\n",
    "            \n",
    "            # Check if we found any new dependencies\n",
    "            new_tables = len(used_tables) - initial_tables_count\n",
    "            new_columns = len(used_columns) - initial_columns_count\n",
    "            new_measures = len(used_measures) - initial_measures_count\n",
    "            \n",
    "            print(f\"    âž• Added: {new_tables} tables, {new_columns} columns, {new_measures} measures\")\n",
    "            \n",
    "            # If no new dependencies were found, we're done\n",
    "            if new_tables == 0 and new_columns == 0 and new_measures == 0:\n",
    "                print(f\"  âœ… Dependency resolution converged after {iteration} iteration(s)\")\n",
    "                break\n",
    "        \n",
    "        print(f\"  âœ… Final dependencies: {len(used_tables)} tables, {len(used_columns)} columns, {len(used_measures)} measures\")\n",
    "        \n",
    "        # Display detailed errors if any\n",
    "        if self.error_log:\n",
    "            print(f\"\\nâš ï¸ Detailed Error Log ({len(self.error_log)} errors):\")\n",
    "            for idx, error_entry in enumerate(self.error_log, 1):\n",
    "                print(f\"\\nError #{idx}:\")\n",
    "                print(f\"  Dataset: {error_entry['dataset']}\")\n",
    "                print(f\"  Operation: {error_entry['operation']}\")\n",
    "                print(f\"  Details: {error_entry['error']}\")\n",
    "        \n",
    "        return {\n",
    "            'used_tables': used_tables,\n",
    "            'used_columns': used_columns,\n",
    "            'used_measures': used_measures,\n",
    "            'report_objects_df': report_objects_df\n",
    "        }\n",
    "    \n",
    "    def filter_results(self, all_columns_df, all_tables_df, all_measures_df, dependencies):\n",
    "        \"\"\"Step 6: Filter results to identify used vs unused objects\"\"\"\n",
    "        print(\"\\nðŸ” STEP 6: Filtering results to identify used vs unused objects...\")\n",
    "        \n",
    "        used_tables = dependencies['used_tables']\n",
    "        used_columns = dependencies['used_columns']\n",
    "        used_measures = dependencies['used_measures']\n",
    "        \n",
    "        # Filter columns\n",
    "        if not all_columns_df.empty:\n",
    "            if 'qualified_name' in all_columns_df.columns:\n",
    "                all_columns_df['is_used'] = all_columns_df['qualified_name'].isin(used_columns)\n",
    "            else:\n",
    "                # Create qualified name if it doesn't exist\n",
    "                all_columns_df['qualified_name'] = \"'\" + all_columns_df['table_name'] + \"'[\" + all_columns_df['column_name'] + ']'\n",
    "                all_columns_df['is_used'] = all_columns_df['qualified_name'].isin(used_columns)\n",
    "            \n",
    "            used_columns_df = all_columns_df[all_columns_df['is_used'] == True].copy()\n",
    "            unused_columns_df = all_columns_df[all_columns_df['is_used'] == False].copy()\n",
    "        else:\n",
    "            used_columns_df = pd.DataFrame()\n",
    "            unused_columns_df = pd.DataFrame()\n",
    "        \n",
    "        # Filter tables\n",
    "        if not all_tables_df.empty:\n",
    "            all_tables_df['is_used'] = all_tables_df['name'].isin(used_tables)\n",
    "            used_tables_df = all_tables_df[all_tables_df['is_used'] == True].copy()\n",
    "            unused_tables_df = all_tables_df[all_tables_df['is_used'] == False].copy()\n",
    "        else:\n",
    "            used_tables_df = pd.DataFrame()\n",
    "            unused_tables_df = pd.DataFrame()\n",
    "        \n",
    "        # Filter measures\n",
    "        if not all_measures_df.empty:\n",
    "            # Create qualified measure name for comparison\n",
    "            all_measures_df['qualified_name'] = \"'\" + all_measures_df['table_name'] + \"'[\" + all_measures_df['measure_name'] + \"]\"\n",
    "            all_measures_df['is_used'] = all_measures_df['qualified_name'].isin(used_measures)\n",
    "            used_measures_df = all_measures_df[all_measures_df['is_used'] == True].copy()\n",
    "            unused_measures_df = all_measures_df[all_measures_df['is_used'] == False].copy()\n",
    "        else:\n",
    "            used_measures_df = pd.DataFrame()\n",
    "            unused_measures_df = pd.DataFrame()\n",
    "        \n",
    "        print(f\"  âœ… Results filtered:\")\n",
    "        print(f\"    Used: {len(used_tables_df)} tables, {len(used_columns_df)} columns, {len(used_measures_df)} measures\")\n",
    "        print(f\"    Unused: {len(unused_tables_df)} tables, {len(unused_columns_df)} columns, {len(unused_measures_df)} measures\")\n",
    "        \n",
    "        return {\n",
    "            'used_tables': used_tables_df,\n",
    "            'used_columns': used_columns_df,\n",
    "            'used_measures': used_measures_df,\n",
    "            'unused_tables': unused_tables_df,\n",
    "            'unused_columns': unused_columns_df,\n",
    "            'unused_measures': unused_measures_df\n",
    "        }\n",
    "    \n",
    "    def collect_dataset_info(self, ds_id: str, ds_name: str, ws_id: str, ws_name: str) -> DatasetInfo:\n",
    "        \"\"\"Centralized function to collect all dataset-related information\"\"\"\n",
    "        dataset_info = DatasetInfo(ds_id, ds_name, ws_id, ws_name)\n",
    "        \n",
    "        # Get model dependencies\n",
    "        try:\n",
    "            deps = fabric.get_model_calc_dependencies(dataset=ds_id, workspace=ws_id)\n",
    "            with deps as calc_deps:\n",
    "                dependencies_df = getattr(calc_deps, \"dependencies_df\", None)\n",
    "            \n",
    "            if dependencies_df is not None and not dependencies_df.empty:\n",
    "                dependencies_df = self.sanitize_df_columns(\n",
    "                    df=dependencies_df, \n",
    "                    extra_columns=True,\n",
    "                    ws_id=ws_id, \n",
    "                    ds_id=ds_id,\n",
    "                    ws_name=ws_name,\n",
    "                    ds_name=ds_name\n",
    "                )\n",
    "                dataset_info.dependencies_df = dependencies_df\n",
    "            else:\n",
    "                dataset_info.dependencies_df = pd.DataFrame()\n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            # Extract only the first line of the error message\n",
    "            brief_error = self._extract_meaningful_error(error_msg)\n",
    "            print(f\"    âš ï¸ Dependencies unavailable for {ds_name}: {brief_error}\")\n",
    "            # Store full error in error log\n",
    "            self.error_log.append({\n",
    "                'dataset': ds_name,\n",
    "                'operation': 'get_model_calc_dependencies',\n",
    "                'error': error_msg\n",
    "            })\n",
    "            dataset_info.dependencies_df = pd.DataFrame()\n",
    "\n",
    "        # Get tables\n",
    "        try:\n",
    "            tables = fabric.list_tables(dataset=ds_id, workspace=ws_id)\n",
    "            if not tables.empty:\n",
    "                tables = self.sanitize_df_columns(\n",
    "                    df=tables, \n",
    "                    extra_columns=True,\n",
    "                    ws_id=ws_id, \n",
    "                    ds_id=ds_id,\n",
    "                    ws_name=ws_name,\n",
    "                    ds_name=ds_name\n",
    "                )\n",
    "                dataset_info.tables_df = tables\n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            brief_error = self._extract_meaningful_error(error_msg)\n",
    "            print(f\"    âš ï¸ Tables unavailable for {ds_name}: {brief_error}\")\n",
    "            self.error_log.append({\n",
    "                'dataset': ds_name,\n",
    "                'operation': 'list_tables',\n",
    "                'error': error_msg\n",
    "            })\n",
    "            \n",
    "        # Get relationships\n",
    "        try:\n",
    "            relationships = fabric.list_relationships(dataset=ds_id, workspace=ws_id, extended=True)\n",
    "            if not relationships.empty:\n",
    "                relationships = self.sanitize_df_columns(df=relationships)\n",
    "                relationships['qualified_from'] = \"'\" + relationships['from_table'] + \"'[\" + relationships['from_column'] + \"]\"\n",
    "                relationships['qualified_to'] = \"'\" + relationships['to_table'] + \"'[\" + relationships['to_column'] + \"]\"\n",
    "                dataset_info.relationships_df = relationships\n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            brief_error = self._extract_meaningful_error(error_msg)\n",
    "            print(f\"    âš ï¸ Relationships unavailable for {ds_name}: {brief_error}\")\n",
    "            self.error_log.append({\n",
    "                'dataset': ds_name,\n",
    "                'operation': 'list_relationships',\n",
    "                'error': error_msg\n",
    "            })\n",
    "\n",
    "        # Get measures\n",
    "        try:\n",
    "            measures = fabric.list_measures(dataset=ds_id, workspace=ws_id)\n",
    "            if not measures.empty:\n",
    "                measures = self.sanitize_df_columns(df=measures)\n",
    "                dataset_info.measures_df = measures\n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            brief_error = self._extract_meaningful_error(error_msg)\n",
    "            print(f\"    âš ï¸ Measures unavailable for {ds_name}: {brief_error}\")\n",
    "            self.error_log.append({\n",
    "                'dataset': ds_name,\n",
    "                'operation': 'list_measures',\n",
    "                'error': error_msg\n",
    "            })\n",
    "\n",
    "        # Get columns\n",
    "        try:\n",
    "            columns = fabric.list_columns(dataset=ds_id, workspace=ws_id, extended=True)\n",
    "            if not columns.empty:\n",
    "                columns = self.sanitize_df_columns(\n",
    "                    df=columns,\n",
    "                    extra_columns=True,\n",
    "                    ws_id=ws_id, \n",
    "                    ds_id=ds_id,\n",
    "                    ws_name=ws_name,\n",
    "                    ds_name=ds_name\n",
    "                )\n",
    "                columns['qualified_name'] = \"'\" + columns['table_name'] + \"'[\" + columns['column_name'] + ']'\n",
    "                dataset_info.columns_df = columns\n",
    "        except Exception as e:\n",
    "            error_msg = str(e)\n",
    "            brief_error = self._extract_meaningful_error(error_msg)\n",
    "            print(f\"    âš ï¸ Columns unavailable for {ds_name}: {brief_error}\")\n",
    "            self.error_log.append({\n",
    "                'dataset': ds_name,\n",
    "                'operation': 'list_columns',\n",
    "                'error': error_msg\n",
    "            })\n",
    "        \n",
    "        return dataset_info\n",
    "    \n",
    "    def extract_report_metadata(self, report_id: str, report_name: str, workspace_id: str, workspace_name: str, dataset_id: str) -> ReportMetadata:\n",
    "        \"\"\"Extract metadata from Power BI reports using dual approach\"\"\"\n",
    "        \n",
    "        # Initialize result object\n",
    "        result = ReportMetadata(\n",
    "            report_id=report_id,\n",
    "            report_name=report_name,\n",
    "            workspace_id=workspace_id,\n",
    "            workspace_name=workspace_name,\n",
    "            dataset_id=dataset_id,\n",
    "            report_format=\"Unknown\",\n",
    "            extraction_method=\"None\",\n",
    "            tables=[],\n",
    "            columns=[],\n",
    "            measures=[],\n",
    "            visuals_count=0,\n",
    "            filters_count=0,\n",
    "            extraction_success=False\n",
    "        )\n",
    "        \n",
    "        try:\n",
    "            # Step 1: Try to determine report format\n",
    "            report = ReportWrapper(report=report_id, workspace=workspace_id)\n",
    "            rep_format = report.format\n",
    "            result.report_format = rep_format\n",
    "            print(f\"  ðŸ“‘ Report Type: {rep_format}\")\n",
    "            if rep_format == \"PBIR\":\n",
    "                # Method 1: Use sempy_labs.report.list_all_semantic_model_objects() for PBIR format\n",
    "                try:\n",
    "                    objects = report.list_semantic_model_objects()\n",
    "                    \n",
    "                    if objects is not None and not objects.empty:\n",
    "                        # Process the objects DataFrame\n",
    "                        tables = objects['Table Name'].unique().tolist()\n",
    "                        columns = (\n",
    "                            objects[objects['Object Type'] == 'Column']\n",
    "                            .assign(qualified=lambda df: \"'\" + df['Table Name'].fillna('') + \"'[\" + df['Object Name'] + \"]\")['qualified'] #build 'table'[column]\n",
    "                            .unique().tolist()\n",
    "                        )\n",
    "                        measures = (\n",
    "                            objects[objects['Object Type'] == 'Measure']\n",
    "                            .assign(qualified = lambda df: \"'\" + df['Table Name'].fillna('') + \"'[\" + df['Object Name'] + \"]\")[\"qualified\"] #build 'table'[measure]\n",
    "                            .unique().tolist()\n",
    "                        )\n",
    "                        \n",
    "                        result.tables = tables\n",
    "                        result.columns = columns\n",
    "                        result.measures = measures\n",
    "                        result.extraction_method = \"sempy_labs_objects\"\n",
    "                        result.extraction_success = True\n",
    "                        \n",
    "                        print(f\"    âœ… Extracted via sempy_labs: {len(tables)} tables, {len(columns)} columns, {len(measures)} measures\")\n",
    "                        return result\n",
    "                        \n",
    "                except NotImplementedError as e:\n",
    "                    print(f\"    âš ï¸ sempy_labs method not supported: {str(e)}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"    âš ï¸ sempy_labs method failed: {str(e)}\")\n",
    "            \n",
    "            # Method 2: Fall back to JSON parsing\n",
    "            report_json = sempy_labs.report.get_report_json(report=report_id, workspace=workspace_id)\n",
    "            \n",
    "            if report_json:\n",
    "                # Use our custom extractor\n",
    "                extractor = PowerBIMetadataExtractor()\n",
    "                extraction_results = extractor.extract_from_json_data(report_json)\n",
    "                \n",
    "                result.tables = extraction_results.get('tables', [])\n",
    "                result.columns = extraction_results.get('columns', [])\n",
    "                result.measures = extraction_results.get('measures', [])\n",
    "                result.visuals_count = len(extraction_results.get('visual_details', []))\n",
    "                result.filters_count = len(extraction_results.get('filter_details', []))\n",
    "                result.extraction_method = \"json_parsing\"\n",
    "                result.extraction_success = True\n",
    "                \n",
    "                print(f\"    âœ… Extracted via JSON: {len(result.tables)} tables, {len(result.columns)} columns, {len(result.measures)} measures\")\n",
    "                return result\n",
    "            else:\n",
    "                result.error_message = \"Could not retrieve report JSON\"\n",
    "                \n",
    "        except Exception as e:\n",
    "            result.error_message = f\"Extraction failed: {str(e)}\"\n",
    "            print(f\"    âŒ Error extracting metadata: {str(e)}\")\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def generate_ai_dataset_context(self, all_tables_df, all_columns_df, all_measures_df, all_relationships_df, filtered_results):\n",
    "        \"\"\"Generate AI-optimized dataset context table with health scores\"\"\"\n",
    "        print(\"\\nðŸ¤– Generating AI dataset context table...\")\n",
    "        \n",
    "        ai_dataset_records = []\n",
    "        \n",
    "        for ds_id, dataset_info in self.all_dataset_info.items():\n",
    "            # Get basic dataset info\n",
    "            ds_row = self.datasets_df[self.datasets_df['dataset_id'] == ds_id].iloc[0] if not self.datasets_df[self.datasets_df['dataset_id'] == ds_id].empty else None\n",
    "            if ds_row is None:\n",
    "                continue\n",
    "            \n",
    "            # Count objects for this dataset\n",
    "            total_tables = len(all_tables_df[all_tables_df['dataset_id'] == ds_id]) if not all_tables_df.empty else 0\n",
    "            total_columns = len(all_columns_df[all_columns_df['dataset_id'] == ds_id]) if not all_columns_df.empty else 0\n",
    "            total_measures = len(all_measures_df[all_measures_df['dataset_id'] == ds_id]) if not all_measures_df.empty else 0\n",
    "            total_relationships = len(all_relationships_df[all_relationships_df['dataset_id'] == ds_id]) if not all_relationships_df.empty else 0\n",
    "            \n",
    "            # Count report usage\n",
    "            report_count = len(self.reports_df[self.reports_df['dataset_id'] == ds_id]) if not self.reports_df.empty else 0\n",
    "            \n",
    "            # Count connected vs isolated tables\n",
    "            dataset_tables = all_tables_df[all_tables_df['dataset_id'] == ds_id]['name'].tolist() if not all_tables_df.empty else []\n",
    "            connected_tables_set = set()\n",
    "            if dataset_info.relationships_df is not None and not dataset_info.relationships_df.empty:\n",
    "                connected_tables_set.update(dataset_info.relationships_df['from_table'].tolist())\n",
    "                connected_tables_set.update(dataset_info.relationships_df['to_table'].tolist())\n",
    "            connected_tables = len(connected_tables_set)\n",
    "            isolated_tables = total_tables - connected_tables\n",
    "            \n",
    "            # Count unused objects\n",
    "            unused_tables = len(filtered_results['unused_tables'][filtered_results['unused_tables']['dataset_id'] == ds_id]) if not filtered_results['unused_tables'].empty else 0\n",
    "            unused_columns = len(filtered_results['unused_columns'][filtered_results['unused_columns']['dataset_id'] == ds_id]) if not filtered_results['unused_columns'].empty else 0\n",
    "            unused_measures = len(filtered_results['unused_measures'][filtered_results['unused_measures']['dataset_id'] == ds_id]) if not filtered_results['unused_measures'].empty else 0\n",
    "            \n",
    "            # Detect circular relationships (simplified - count self-referencing)\n",
    "            circular_chains = 0\n",
    "            if dataset_info.relationships_df is not None and not dataset_info.relationships_df.empty:\n",
    "                # Simple circular detection: tables that reference themselves\n",
    "                circular_chains = len(dataset_info.relationships_df[\n",
    "                    dataset_info.relationships_df['from_table'] == dataset_info.relationships_df['to_table']\n",
    "                ])\n",
    "            \n",
    "            # Calculate health scores (0-1 scale)\n",
    "            relationship_health = (connected_tables / total_tables) if total_tables > 0 else 0.0\n",
    "            usage_efficiency = 1 - (unused_columns / total_columns) if total_columns > 0 else 1.0\n",
    "            \n",
    "            # Model complexity score (normalized)\n",
    "            # Higher complexity = more relationships + measures relative to tables\n",
    "            complexity_raw = (total_relationships + total_measures) / max(total_tables, 1)\n",
    "            model_complexity = min(complexity_raw / 10, 1.0)  # Normalize to 0-1, cap at 1\n",
    "            \n",
    "            # Overall optimization score (0-100)\n",
    "            # Weighted combination: relationships (30%), usage (40%), no isolated tables (20%), no unused (10%)\n",
    "            optimization_score = (\n",
    "                relationship_health * 30 +\n",
    "                usage_efficiency * 40 +\n",
    "                ((total_tables - isolated_tables) / max(total_tables, 1)) * 20 +\n",
    "                (1 - (unused_tables / max(total_tables, 1))) * 10\n",
    "            )\n",
    "            \n",
    "            ai_dataset_records.append({\n",
    "                'workspace_id': dataset_info.ws_id,\n",
    "                'workspace_name': dataset_info.ws_name,\n",
    "                'dataset_id': ds_id,\n",
    "                'dataset_name': dataset_info.ds_name,\n",
    "                # Size metrics\n",
    "                'total_tables': total_tables,\n",
    "                'total_columns': total_columns,\n",
    "                'total_measures': total_measures,\n",
    "                'total_relationships': total_relationships,\n",
    "                # Usage metrics\n",
    "                'report_count': report_count,\n",
    "                'dataflow_count': 0,  # Placeholder - can be added if needed\n",
    "                'connected_tables': connected_tables,\n",
    "                'isolated_tables': isolated_tables,\n",
    "                # Quality metrics\n",
    "                'unused_tables': unused_tables,\n",
    "                'unused_columns': unused_columns,\n",
    "                'unused_measures': unused_measures,\n",
    "                'circular_chains': circular_chains,\n",
    "                # Calculated health scores\n",
    "                'relationship_health': round(relationship_health, 3),\n",
    "                'usage_efficiency': round(usage_efficiency, 3),\n",
    "                'model_complexity': round(model_complexity, 3),\n",
    "                'optimization_score': round(optimization_score, 2)\n",
    "            })\n",
    "        \n",
    "        ai_dataset_context_df = pd.DataFrame(ai_dataset_records)\n",
    "        print(f\"  âœ… Generated {len(ai_dataset_records)} dataset context records\")\n",
    "        return ai_dataset_context_df\n",
    "    \n",
    "    def generate_ai_object_features(self, all_columns_df, all_measures_df, all_tables_df, all_relationships_df, filtered_results, ai_dataset_context_df):\n",
    "        \"\"\"Generate AI-optimized object features table with rich context\"\"\"\n",
    "        print(\"\\nðŸ¤– Generating AI object features table...\")\n",
    "        \n",
    "        ai_object_records = []\n",
    "        \n",
    "        # Process columns\n",
    "        if not all_columns_df.empty:\n",
    "            for _, col_row in all_columns_df.iterrows():\n",
    "                ds_id = col_row.get('dataset_id', '')\n",
    "                table_name = col_row.get('table_name', '')\n",
    "                column_name = col_row.get('column_name', '')\n",
    "                qualified_name = col_row.get('qualified_name', '')\n",
    "                \n",
    "                # Get dataset context\n",
    "                dataset_context = ai_dataset_context_df[ai_dataset_context_df['dataset_id'] == ds_id].iloc[0] if not ai_dataset_context_df[ai_dataset_context_df['dataset_id'] == ds_id].empty else None\n",
    "                \n",
    "                # Get table context\n",
    "                table_measures = len(all_measures_df[\n",
    "                    (all_measures_df['dataset_id'] == ds_id) & \n",
    "                    (all_measures_df['table_name'] == table_name)\n",
    "                ]) if not all_measures_df.empty else 0\n",
    "                \n",
    "                table_columns = len(all_columns_df[\n",
    "                    (all_columns_df['dataset_id'] == ds_id) & \n",
    "                    (all_columns_df['table_name'] == table_name)\n",
    "                ]) if not all_columns_df.empty else 0\n",
    "                \n",
    "                table_relationships = len(all_relationships_df[\n",
    "                    (all_relationships_df['dataset_id'] == ds_id) & \n",
    "                    ((all_relationships_df['from_table'] == table_name) | \n",
    "                     (all_relationships_df['to_table'] == table_name))\n",
    "                ]) if not all_relationships_df.empty else 0\n",
    "                \n",
    "                # Check if table is isolated\n",
    "                table_is_isolated = table_relationships == 0\n",
    "                \n",
    "                # Check column usage\n",
    "                is_used = col_row.get('is_used', False)\n",
    "                \n",
    "                # Count usage types from dependencies\n",
    "                dataset_info = self.all_dataset_info.get(ds_id)\n",
    "                used_by_measures = 0\n",
    "                used_by_relationships = 0\n",
    "                used_by_dependencies = 0\n",
    "                referenced_by_list = []\n",
    "                \n",
    "                if dataset_info and dataset_info.dependencies_df is not None and not dataset_info.dependencies_df.empty:\n",
    "                    deps = dataset_info.dependencies_df[\n",
    "                        dataset_info.dependencies_df['referenced_full_object_name'] == qualified_name\n",
    "                    ]\n",
    "                    used_by_dependencies = len(deps)\n",
    "                    \n",
    "                    # Get specific usage types\n",
    "                    if 'object_type' in deps.columns:\n",
    "                        used_by_measures = len(deps[deps['object_type'] == 'Measure'])\n",
    "                    \n",
    "                    # Get referenced by list\n",
    "                    if 'object_name' in deps.columns:\n",
    "                        referenced_by_list = deps['object_name'].unique().tolist()\n",
    "                \n",
    "                # Check relationship usage\n",
    "                if not all_relationships_df.empty:\n",
    "                    rel_usage = all_relationships_df[\n",
    "                        (all_relationships_df['dataset_id'] == ds_id) & \n",
    "                        ((all_relationships_df['qualified_from'] == qualified_name) | \n",
    "                         (all_relationships_df['qualified_to'] == qualified_name))\n",
    "                    ]\n",
    "                    used_by_relationships = len(rel_usage)\n",
    "                \n",
    "                # Calculate usage score (0-1)\n",
    "                usage_score = min((used_by_measures * 0.4 + used_by_relationships * 0.4 + used_by_dependencies * 0.2) / 5, 1.0)\n",
    "                \n",
    "                ai_object_records.append({\n",
    "                    'workspace_id': col_row.get('workspace_id', ''),\n",
    "                    'workspace_name': col_row.get('workspace_name', ''),\n",
    "                    'dataset_id': ds_id,\n",
    "                    'dataset_name': col_row.get('dataset_name', ''),\n",
    "                    'table_name': table_name,\n",
    "                    'object_name': column_name,\n",
    "                    'object_type': 'calculated_column' if col_row.get('type', '').lower() == 'calculated' else 'column',\n",
    "                    # Object properties\n",
    "                    'data_type': col_row.get('data_type', col_row.get('type', 'Unknown')),\n",
    "                    'is_hidden': col_row.get('is_hidden', False),\n",
    "                    'is_calculated': col_row.get('type', '').lower() == 'calculated',\n",
    "                    'has_dax': col_row.get('expression', '') != '',\n",
    "                    # Table context\n",
    "                    'table_measure_count': table_measures,\n",
    "                    'table_column_count': table_columns,\n",
    "                    'table_relationship_count': table_relationships,\n",
    "                    'table_is_isolated': table_is_isolated,\n",
    "                    # Dataset context (denormalized)\n",
    "                    'dataset_total_tables': dataset_context['total_tables'] if dataset_context is not None else 0,\n",
    "                    'dataset_relationship_health': dataset_context['relationship_health'] if dataset_context is not None else 0.0,\n",
    "                    'dataset_usage_efficiency': dataset_context['usage_efficiency'] if dataset_context is not None else 0.0,\n",
    "                    # Usage features\n",
    "                    'used_by_measures': used_by_measures,\n",
    "                    'used_by_relationships': used_by_relationships,\n",
    "                    'used_by_dependencies': used_by_dependencies,\n",
    "                    'is_used': is_used,\n",
    "                    'usage_score': round(usage_score, 3),\n",
    "                    # Referenced by (as JSON string)\n",
    "                    'referenced_by_list': json.dumps(referenced_by_list) if referenced_by_list else ''\n",
    "                })\n",
    "        \n",
    "        # Process measures\n",
    "        if not all_measures_df.empty:\n",
    "            for _, meas_row in all_measures_df.iterrows():\n",
    "                ds_id = meas_row.get('dataset_id', '')\n",
    "                table_name = meas_row.get('table_name', '')\n",
    "                measure_name = meas_row.get('measure_name', '')\n",
    "                qualified_name = meas_row.get('qualified_name', '')\n",
    "                \n",
    "                # Get dataset context\n",
    "                dataset_context = ai_dataset_context_df[ai_dataset_context_df['dataset_id'] == ds_id].iloc[0] if not ai_dataset_context_df[ai_dataset_context_df['dataset_id'] == ds_id].empty else None\n",
    "                \n",
    "                # Get table context\n",
    "                table_measures = len(all_measures_df[\n",
    "                    (all_measures_df['dataset_id'] == ds_id) & \n",
    "                    (all_measures_df['table_name'] == table_name)\n",
    "                ]) if not all_measures_df.empty else 0\n",
    "                \n",
    "                table_columns = len(all_columns_df[\n",
    "                    (all_columns_df['dataset_id'] == ds_id) & \n",
    "                    (all_columns_df['table_name'] == table_name)\n",
    "                ]) if not all_columns_df.empty else 0\n",
    "                \n",
    "                table_relationships = len(all_relationships_df[\n",
    "                    (all_relationships_df['dataset_id'] == ds_id) & \n",
    "                    ((all_relationships_df['from_table'] == table_name) | \n",
    "                     (all_relationships_df['to_table'] == table_name))\n",
    "                ]) if not all_relationships_df.empty else 0\n",
    "                \n",
    "                table_is_isolated = table_relationships == 0\n",
    "                \n",
    "                # Check measure usage\n",
    "                is_used = meas_row.get('is_used', False)\n",
    "                \n",
    "                # Count usage from dependencies\n",
    "                dataset_info = self.all_dataset_info.get(ds_id)\n",
    "                used_by_measures = 0\n",
    "                used_by_dependencies = 0\n",
    "                referenced_by_list = []\n",
    "                \n",
    "                if dataset_info and dataset_info.dependencies_df is not None and not dataset_info.dependencies_df.empty:\n",
    "                    deps = dataset_info.dependencies_df[\n",
    "                        dataset_info.dependencies_df['referenced_full_object_name'] == qualified_name\n",
    "                    ]\n",
    "                    used_by_dependencies = len(deps)\n",
    "                    \n",
    "                    if 'object_type' in deps.columns:\n",
    "                        used_by_measures = len(deps[deps['object_type'] == 'Measure'])\n",
    "                    \n",
    "                    if 'object_name' in deps.columns:\n",
    "                        referenced_by_list = deps['object_name'].unique().tolist()\n",
    "                \n",
    "                # Calculate usage score\n",
    "                usage_score = min((used_by_measures * 0.5 + used_by_dependencies * 0.5) / 3, 1.0)\n",
    "                \n",
    "                ai_object_records.append({\n",
    "                    'workspace_id': meas_row.get('workspace_id', ''),\n",
    "                    'workspace_name': meas_row.get('workspace_name', ''),\n",
    "                    'dataset_id': ds_id,\n",
    "                    'dataset_name': meas_row.get('dataset_name', ''),\n",
    "                    'table_name': table_name,\n",
    "                    'object_name': measure_name,\n",
    "                    'object_type': 'measure',\n",
    "                    # Object properties\n",
    "                    'data_type': 'Measure',\n",
    "                    'is_hidden': meas_row.get('is_hidden', False),\n",
    "                    'is_calculated': True,\n",
    "                    'has_dax': meas_row.get('expression', '') != '',\n",
    "                    # Table context\n",
    "                    'table_measure_count': table_measures,\n",
    "                    'table_column_count': table_columns,\n",
    "                    'table_relationship_count': table_relationships,\n",
    "                    'table_is_isolated': table_is_isolated,\n",
    "                    # Dataset context\n",
    "                    'dataset_total_tables': dataset_context['total_tables'] if dataset_context is not None else 0,\n",
    "                    'dataset_relationship_health': dataset_context['relationship_health'] if dataset_context is not None else 0.0,\n",
    "                    'dataset_usage_efficiency': dataset_context['usage_efficiency'] if dataset_context is not None else 0.0,\n",
    "                    # Usage features\n",
    "                    'used_by_measures': used_by_measures,\n",
    "                    'used_by_relationships': 0,  # Measures aren't used in relationships\n",
    "                    'used_by_dependencies': used_by_dependencies,\n",
    "                    'is_used': is_used,\n",
    "                    'usage_score': round(usage_score, 3),\n",
    "                    # Referenced by\n",
    "                    'referenced_by_list': json.dumps(referenced_by_list) if referenced_by_list else ''\n",
    "                })\n",
    "        \n",
    "        ai_object_features_df = pd.DataFrame(ai_object_records)\n",
    "        print(f\"  âœ… Generated {len(ai_object_records)} object feature records\")\n",
    "        return ai_object_features_df\n",
    "    \n",
    "    def save_all_results(self, all_columns_df, all_tables_df, all_measures_df, all_dependencies_df, all_relationships_df, filtered_results, dependencies):\n",
    "        \"\"\"Save AI-optimized results to lakehouse\"\"\"\n",
    "        print(\"\\nðŸ’¾ STEP 7: Saving AI-optimized results to lakehouse...\")\n",
    "        \n",
    "        # Generate AI-optimized tables\n",
    "        ai_dataset_context_df = self.generate_ai_dataset_context(\n",
    "            all_tables_df, all_columns_df, all_measures_df, all_relationships_df, filtered_results\n",
    "        )\n",
    "        \n",
    "        ai_object_features_df = self.generate_ai_object_features(\n",
    "            all_columns_df, all_measures_df, all_tables_df, all_relationships_df, filtered_results, ai_dataset_context_df\n",
    "        )\n",
    "        \n",
    "        # Save ONLY AI-optimized tables for ML/AI consumption\n",
    "        print(\"\\nðŸ¤– Saving AI-optimized tables for ML training and predictions...\")\n",
    "        self.save_to_lakehouse(ai_dataset_context_df, \"ai_dataset_context\", \n",
    "                              \"AI-ready dataset features with health scores (0-100) for ML training\")\n",
    "        self.save_to_lakehouse(ai_object_features_df, \"ai_object_features\", \n",
    "                              \"AI-ready object-level features with full lineage context for predictions\")\n",
    "        \n",
    "        print(\"\\nðŸ”— Saving relationships table...\")\n",
    "        self.save_to_lakehouse(all_relationships_df, \"dataset_relationships\", \n",
    "                              \"All relationships across datasets with qualified column references\")\n",
    "\n",
    "        print(\"\\nðŸ”— Saving dependencies table...\")\n",
    "        self.save_to_lakehouse(all_dependencies_df, \"dataste_dependencies\",\n",
    "                                \"All dataset dependedncies for reference later\")\n",
    "        print(\"\\nâœ… AI-optimized lakehouse tables created successfully!\")\n",
    "        print(f\"   ðŸ“Š ai_dataset_context: {len(ai_dataset_context_df)} datasets with 16 features\")\n",
    "        print(f\"   ðŸ“Š ai_object_features: {len(ai_object_features_df)} objects (columns + measures) with 23 features\")\n",
    "        print(\"\\nðŸ’¡ Use these tables for:\")\n",
    "        print(\"   - Training schema optimization models\")\n",
    "        print(\"   - Predicting unused objects\")\n",
    "        print(\"   - Generating dataset health scores\")\n",
    "        print(\"   - Recommending model improvements\")\n",
    "    \n",
    "    def run_complete_analysis(self):\n",
    "        \"\"\"Run the complete analysis workflow\"\"\"\n",
    "        print(\"ðŸš€ STARTING COMPLETE FABRIC WORKSPACE ANALYSIS\")\n",
    "        print(\"=\" * 80)\n",
    "        \n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Step 1: Get Workspaces\n",
    "        self.get_workspaces()\n",
    "        \n",
    "        # Step 2: Get Datasets and Reports\n",
    "        self.get_datasets_and_reports()\n",
    "        \n",
    "        # Step 3: Process all datasets and aggregate all objects\n",
    "        all_columns_df, all_tables_df, all_measures_df, all_dependencies_df, all_relationships_df = self.process_all_datasets()\n",
    "        \n",
    "        # Step 4: Get report metadata\n",
    "        self.get_reports_metadata()\n",
    "        \n",
    "        # Step 5: Check dependencies\n",
    "        dependencies = self.check_dependencies(all_columns_df, all_tables_df, all_measures_df)\n",
    "        \n",
    "        # Step 6: Filter results\n",
    "        filtered_results = self.filter_results(all_columns_df, all_tables_df, all_measures_df, dependencies)\n",
    "        \n",
    "        # Step 7: Save all results\n",
    "        self.save_all_results(all_columns_df, all_tables_df, all_measures_df, all_dependencies_df, all_relationships_df, filtered_results, dependencies)\n",
    "        \n",
    "        end_time = time.time()\n",
    "        duration = end_time - start_time\n",
    "        \n",
    "        # Final summary\n",
    "        print(\"\\n\" + \"=\" * 80)\n",
    "        print(\"ðŸŽ‰ FABRIC WORKSPACE ANALYSIS COMPLETE!\")\n",
    "        print(\"=\" * 80)\n",
    "        print(f\"â±ï¸ Total execution time: {duration:.2f} seconds\")\n",
    "        print(f\"\\nðŸ“Š Summary:\")\n",
    "        print(f\"  Workspaces analyzed: {len(self.workspaces_df)}\")\n",
    "        print(f\"  Datasets processed: {len(self.datasets_df)}\")\n",
    "        print(f\"  Reports analyzed: {len(self.report_metadata_list)}\")\n",
    "        print(f\"  Total objects found: {len(all_columns_df)} columns, {len(all_tables_df)} tables, {len(all_measures_df)} measures\")\n",
    "        print(f\"  Used objects: {len(filtered_results['used_columns'])} columns, {len(filtered_results['used_tables'])} tables, {len(filtered_results['used_measures'])} measures\")\n",
    "        print(f\"  Unused objects: {len(filtered_results['unused_columns'])} columns, {len(filtered_results['unused_tables'])} tables, {len(filtered_results['unused_measures'])} measures\")\n",
    "        print(\"\\nðŸ’¾ All results saved to lakehouse tables!\")\n",
    "        print(\"=\" * 80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "97078286-a3aa-42c3-a224-5f90f955b02c",
   "metadata": {
    "microsoft": {
     "language": "python",
     "language_group": "synapse_pyspark"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.livy.statement-meta+json": {
       "execution_finish_time": "2025-10-28T05:13:34.2786707Z",
       "execution_start_time": "2025-10-28T05:11:57.1307281Z",
       "livy_statement_state": "available",
       "normalized_state": "finished",
       "parent_msg_id": "c87780f3-638b-453b-b475-0e4357ac5d12",
       "queued_time": "2025-10-28T05:11:13.2993794Z",
       "session_id": "c6750e83-47d2-4299-9124-248033c7d2f1",
       "session_start_time": null,
       "spark_pool": null,
       "state": "finished",
       "statement_id": 14,
       "statement_ids": [
        14
       ]
      },
      "text/plain": [
       "StatementMeta(, c6750e83-47d2-4299-9124-248033c7d2f1, 14, Finished, Available, Finished)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Initializing Fabric Workspace Analyzer...\n",
      "\n",
      "ðŸš€ STARTING COMPLETE FABRIC WORKSPACE ANALYSIS\n",
      "================================================================================\n",
      "ðŸ” STEP 1: Discovering workspaces...\n",
      "  âœ… Found 4 workspaces\n",
      "\n",
      "ðŸ” STEP 2: Getting datasets and reports...\n",
      "  ðŸ“¦ Scanning workspace: Fabric_Demo\n",
      "  ðŸ“¦ Scanning workspace: BI_Metadata\n",
      "  ðŸ“¦ Scanning workspace: NLQ_Task\n",
      "  ðŸ“¦ Scanning workspace: Auto_DP\n",
      "  âœ… Found 3 datasets and 2 reports (2 PowerBI reports)\n",
      "\n",
      "ðŸ” STEP 3: Processing all datasets and aggregating objects...\n",
      "  ðŸ“Š Processing dataset: LH_D365FNO\n",
      "    âš ï¸ Dependencies unavailable for LH_D365FNO: An error occurred when running AdomdCommand. AdomdCommandActivityId: '7a02ca9d-13ab-4c42-a420-e5b7305f62b2'\n",
      "\n",
      "Caused by AdomdErrorResponseException:\n",
      "The database is empty. The DISCOVER_CALC_DEPENDENCY operation cannot be performed on an empty database.\n",
      "\n",
      "Technical Details:\n",
      "RootActivityId: 18ac6a24-6ec0-40e9-bd5c-79487b0a3002\n",
      "Date (UTC): 10/28/2025 5:12:11 AM\n",
      "   at Microsoft.AnalysisServices.AdomdClient.XmlaClient.CheckForSoapFault(XmlReader reader, XmlaResult xmlaResult, Boolean throwIfError)\n",
      "   at Microsoft.AnalysisServices.AdomdClient.XmlaClient.CheckForError(XmlReader reader, XmlaResult xmlaResult, Boolean throwIfError)\n",
      "   at Microsoft.AnalysisServices.AdomdClient.XmlaClient.SendMessage(Boolean endReceivalIfException, Boolean readSession, Boolean readNamespaceCompatibility)\n",
      "   at Microsoft.AnalysisServices.AdomdClient.XmlaClient.ExecuteStatement(String statement, IDictionary connectionProperties, IDictionary commandProperties, IDataParameterCollection parameters, Boolean isMdx)\n",
      "   at Microsoft.AnalysisServices.AdomdClient.AdomdConnection.XmlaClientProvider.Microsoft.AnalysisServices.AdomdClient.IExecuteProvider.ExecuteTabular(CommandBehavior behavior, ICommandContentProvider contentProvider, AdomdPropertyCollection commandProperties, IDataParameterCollection parameters)\n",
      "   at Microsoft.AnalysisServices.AdomdClient.AdomdCommand.ExecuteReader(CommandBehavior behavior)\n",
      "   at Microsoft.AnalysisServices.AdomdClient.AdomdCommand.ExecuteReader()\n",
      "   at Microsoft.Fabric.SemanticLink.DAXToParquetWriter.Write(String dax, String fileName, Int32 batchSize, AdomdConnection adomdConnection, Int32 verbose, Nullable`1 maxNumRows, Nullable`1 activityId)\n",
      "\n",
      "  ðŸ“Š Processing dataset: NLQ_Model\n",
      "  ðŸ“Š Processing dataset: NLQ_Demo\n",
      "  âœ… Processed 3 datasets\n",
      "    ðŸ“‹ Aggregated: 65 columns, 6 tables, 1 measures\n",
      "    ðŸ”— Aggregated: 10 dependencies, 4 relationships\n",
      "\n",
      "ðŸ” STEP 4: Extracting report metadata...\n",
      "  ðŸ“Š Processing report 1/3: NLQ\n",
      "  ðŸ“‘ Report Type: PBIR-Legacy\n",
      "    âœ… Extracted via JSON: 0 tables, 0 columns, 0 measures\n",
      "  ðŸ“Š Processing report 2/3: NLQ_Demo\n",
      "  ðŸ“‘ Report Type: PBIR-Legacy\n",
      "    âœ… Extracted via JSON: 2 tables, 4 columns, 1 measures\n",
      "  âœ… Processed 3 reports, extracted 7 object references\n",
      "\n",
      "ðŸ” STEP 5: Checking for dependencies...\n",
      "  ðŸ“‹ Initial objects from reports: 2 tables, 4 columns, 1 measures\n",
      "  ðŸ”— After adding relationship columns: 12 columns\n",
      "  ðŸ”„ Dependency resolution iteration 1...\n",
      "    âž• Added: 0 tables, 1 columns, 0 measures\n",
      "  ðŸ”„ Dependency resolution iteration 2...\n",
      "    âž• Added: 0 tables, 0 columns, 0 measures\n",
      "  âœ… Dependency resolution converged after 2 iteration(s)\n",
      "  âœ… Final dependencies: 2 tables, 13 columns, 1 measures\n",
      "\n",
      "ðŸ” STEP 6: Filtering results to identify used vs unused objects...\n",
      "  âœ… Results filtered:\n",
      "    Used: 2 tables, 13 columns, 1 measures\n",
      "    Unused: 4 tables, 52 columns, 0 measures\n",
      "\n",
      "ðŸ’¾ STEP 7: Saving AI-optimized results to lakehouse...\n",
      "\n",
      "ðŸ¤– Generating AI dataset context table...\n",
      "  âœ… Generated 3 dataset context records\n",
      "\n",
      "ðŸ¤– Generating AI object features table...\n",
      "  âœ… Generated 66 object feature records\n",
      "\n",
      "ðŸ¤– Saving AI-optimized tables for ML training and predictions...\n",
      "  âœ… Saved 3 records to 'ai_dataset_context' table\n",
      "     ðŸ“ AI-ready dataset features with health scores (0-100) for ML training\n",
      "  âœ… Saved 66 records to 'ai_object_features' table\n",
      "     ðŸ“ AI-ready object-level features with full lineage context for predictions\n",
      "\n",
      "ðŸ”— Saving relationships table...\n",
      "  âœ… Saved 4 records to 'dataset_relationships' table\n",
      "     ðŸ“ All relationships across datasets with qualified column references\n",
      "\n",
      "ðŸ”— Saving dependencies table...\n",
      "  âœ… Saved 10 records to 'dataste_dependencies' table\n",
      "     ðŸ“ All dataset dependedncies for reference later\n",
      "\n",
      "âœ… AI-optimized lakehouse tables created successfully!\n",
      "   ðŸ“Š ai_dataset_context: 3 datasets with 16 features\n",
      "   ðŸ“Š ai_object_features: 66 objects (columns + measures) with 23 features\n",
      "\n",
      "ðŸ’¡ Use these tables for:\n",
      "   - Training schema optimization models\n",
      "   - Predicting unused objects\n",
      "   - Generating dataset health scores\n",
      "   - Recommending model improvements\n",
      "\n",
      "================================================================================\n",
      "ðŸŽ‰ FABRIC WORKSPACE ANALYSIS COMPLETE!\n",
      "================================================================================\n",
      "â±ï¸ Total execution time: 93.45 seconds\n",
      "\n",
      "ðŸ“Š Summary:\n",
      "  Workspaces analyzed: 4\n",
      "  Datasets processed: 3\n",
      "  Reports analyzed: 2\n",
      "  Total objects found: 65 columns, 6 tables, 1 measures\n",
      "  Used objects: 13 columns, 2 tables, 1 measures\n",
      "  Unused objects: 52 columns, 4 tables, 0 measures\n",
      "\n",
      "ðŸ’¾ All results saved to lakehouse tables!\n",
      "================================================================================\n",
      "\n",
      "ðŸŽ‰ Analysis complete! AI-optimized tables are ready.\n",
      "\n",
      "ðŸ“Š Quick Access:\n",
      "  ai_datasets = spark.table('ai_dataset_context').toPandas()\n",
      "  ai_objects = spark.table('ai_object_features').toPandas()\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ============================================================\n",
    "# RUN THE COMPLETE ANALYSIS\n",
    "# ============================================================\n",
    "\n",
    "print(\"ðŸš€ Initializing Fabric Workspace Analyzer...\\n\")\n",
    "analyzer = FabricWorkspaceAnalyzer()\n",
    "\n",
    "# Run the complete analysis\n",
    "analyzer.run_complete_analysis()\n",
    "\n",
    "print(\"\\nðŸŽ‰ Analysis complete! AI-optimized tables are ready.\")\n",
    "print(\"\\nðŸ“Š Quick Access:\")\n",
    "print(\"  ai_datasets = spark.table('ai_dataset_context').toPandas()\")\n",
    "print(\"  ai_objects = spark.table('ai_object_features').toPandas()\")\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "dependencies": {
   "lakehouse": {
    "default_lakehouse": "6f0b7983-bd9a-43fa-8269-8e327ed1e996",
    "default_lakehouse_name": "Migration_LH",
    "default_lakehouse_workspace_id": "928ee79e-5cc1-44b3-8d63-89320ca539c9",
    "known_lakehouses": [
     {
      "id": "6f0b7983-bd9a-43fa-8269-8e327ed1e996"
     }
    ]
   }
  },
  "kernel_info": {
   "name": "synapse_pyspark"
  },
  "kernelspec": {
   "display_name": "Synapse PySpark",
   "name": "synapse_pyspark"
  },
  "language_info": {
   "name": "python"
  },
  "microsoft": {
   "language": "python",
   "language_group": "synapse_pyspark",
   "ms_spell_check": {
    "ms_spell_check_language": "en"
   }
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "spark_compute": {
   "compute_id": "/trident/default",
   "session_options": {
    "conf": {
     "spark.synapse.nbs.session.timeout": "1200000"
    }
   }
  },
  "synapse_widget": {
   "state": {},
   "version": "0.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
